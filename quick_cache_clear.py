#!/usr/bin/env python3
"""
å¿«é€Ÿç¼“å­˜æ¸…ç†è„šæœ¬
ä¸“é—¨ç”¨äºæ¸…ç†PyTorchå’Œç³»ç»Ÿç¼“å­˜
"""

import torch
import gc
import os
from pathlib import Path

def quick_clear():
    """å¿«é€Ÿæ¸…ç†ç¼“å­˜"""
    print("ğŸ§¹ å¿«é€Ÿæ¸…ç†ç¼“å­˜...")
    
    # 1. æ¸…ç†PyTorch GPUç¼“å­˜
    if torch.cuda.is_available():
        print(f"   GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
        before = torch.cuda.memory_allocated() / 1024**2
        torch.cuda.empty_cache()
        after = torch.cuda.memory_allocated() / 1024**2
        print(f"   GPUå†…å­˜: {before:.1f}MB â†’ {after:.1f}MB")
    else:
        print("   GPUä¸å¯ç”¨ï¼Œè·³è¿‡GPUç¼“å­˜æ¸…ç†")
    
    # 2. Pythonåƒåœ¾å›æ”¶
    collected = gc.collect()
    print(f"   åƒåœ¾å›æ”¶: æ¸…ç†äº† {collected} ä¸ªå¯¹è±¡")
    
    # 3. æ£€æŸ¥ç¼“å­˜ç›®å½•çŠ¶æ€
    cache_dirs = ['cache', 'logs', 'checkpoints']
    for dir_name in cache_dirs:
        dir_path = Path(dir_name)
        if dir_path.exists():
            size = sum(f.stat().st_size for f in dir_path.rglob('*') if f.is_file()) / 1024**2
            print(f"   {dir_name}: {size:.1f}MB")
        else:
            print(f"   {dir_name}: ä¸å­˜åœ¨")
    
    print("âœ… å¿«é€Ÿæ¸…ç†å®Œæˆï¼")

if __name__ == "__main__":
    quick_clear() 