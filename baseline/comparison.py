import pandas as pd
import torch
import gc
from typing import TYPE_CHECKING

from .baseline import set_baseline_seed
from .spectral_clustering import SpectralPartitioner
from .kmeans_clustering import KMeansPartitioner
from .random_partition import RandomPartitioner
from .evaluator import evaluate_partition_method

if TYPE_CHECKING:
    from env import PowerGridPartitionEnv


def compare_methods(env: 'PowerGridPartitionEnv', agent, seed: int = 42) -> pd.DataFrame:
    """
    æ¯”è¾ƒä¸åŒåˆ†åŒºæ–¹æ³•
    """
    # åœ¨å¯¹æ¯”å¼€å§‹å‰è®¾ç½®å…¨å±€éšæœºç§å­
    set_baseline_seed(seed)
    
    results = []
    
    try:
        # 1. RLæ–¹æ³•
        print("\nğŸ¤– è¯„ä¼°RLæ–¹æ³•...")
        obs_dict, _ = env.reset()

        # ä½¿ç”¨è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“è¿›è¡Œåˆ†åŒº
        state = obs_dict
        done = False

        while not done:
            # ä½¿ç”¨æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œï¼ˆæ™ºèƒ½ä½“å†…éƒ¨ä¼šå¤„ç†åŠ¨ä½œæ©ç ï¼‰
            action, _, _ = agent.select_action(state, training=False)
            if action is None:
                break

            next_obs, _, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            state = next_obs
        
        rl_metrics = evaluate_partition_method(env, env.state_manager.current_partition.cpu().numpy())
        rl_metrics['method'] = 'RL (PPO)'
        results.append(rl_metrics)
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

    except Exception as e:
        print(f"âš ï¸ RLæ–¹æ³•è¯„ä¼°å¤±è´¥: {str(e)}")
        results.append({'method': 'RL (PPO)', 'load_cv': 999, 'total_coupling': 999, 
                       'connectivity': 0, 'modularity': 0})

    try:
        # 2. è°±èšç±»
        print("ğŸ“Š è¯„ä¼°è°±èšç±»...")
        spectral = SpectralPartitioner(seed=seed)
        spectral_partition = spectral.partition(env)
        spectral_metrics = evaluate_partition_method(env, spectral_partition)
        spectral_metrics['method'] = 'Spectral Clustering'
        results.append(spectral_metrics)
        gc.collect()

    except Exception as e:
        print(f"âš ï¸ è°±èšç±»è¯„ä¼°å¤±è´¥: {str(e)}")
        results.append({'method': 'Spectral Clustering', 'load_cv': 999, 'total_coupling': 999,
                       'connectivity': 0, 'modularity': 0})

    try:
        # 3. K-means
        print("ğŸ“Š è¯„ä¼°K-means...")
        kmeans = KMeansPartitioner(seed=seed)
        kmeans_partition = kmeans.partition(env)
        kmeans_metrics = evaluate_partition_method(env, kmeans_partition)
        kmeans_metrics['method'] = 'K-means'
        results.append(kmeans_metrics)
        gc.collect()

    except Exception as e:
        print(f"âš ï¸ K-meansè¯„ä¼°å¤±è´¥: {str(e)}")
        results.append({'method': 'K-means', 'load_cv': 999, 'total_coupling': 999,
                       'connectivity': 0, 'modularity': 0})

    try:
        # 4. éšæœºåˆ†åŒº
        print("ğŸ² è¯„ä¼°éšæœºåˆ†åŒº...")
        random_partitioner = RandomPartitioner(seed=seed)
        random_partition = random_partitioner.partition(env)
        random_metrics = evaluate_partition_method(env, random_partition)
        random_metrics['method'] = 'Random'
        results.append(random_metrics)
        gc.collect()

    except Exception as e:
        print(f"âš ï¸ éšæœºåˆ†åŒºè¯„ä¼°å¤±è´¥: {str(e)}")
        results.append({'method': 'Random', 'load_cv': 999, 'total_coupling': 999,
                       'connectivity': 0, 'modularity': 0})
    
    # åˆ›å»ºç»“æœDataFrameï¼Œç¡®ä¿è‡³å°‘æœ‰ä¸€äº›ç»“æœ
    if not results:
        results = [{'method': 'No Method Succeeded', 'load_cv': 999, 'total_coupling': 999,
                   'connectivity': 0, 'modularity': 0}]
    
    df = pd.DataFrame(results)
    df = df.set_index('method')
    
    return df


def run_baseline_comparison(env, agent, seed: int = 42):
    """æ‰§è¡ŒåŸºçº¿æ–¹æ³•å¯¹æ¯”çš„ä¸»å‡½æ•°"""
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
    set_baseline_seed(seed)
    
    # æ‰§è¡Œå¯¹æ¯”
    print(f"\nğŸ“Š æ‰§è¡Œæ–¹æ³•å¯¹æ¯”ï¼ˆéšæœºç§å­ï¼š{seed}ï¼‰...")
    comparison_df = compare_methods(env, agent, seed=seed)

    # æ˜¾ç¤ºç»“æœ
    print("\nğŸ“‹ åˆ†åŒºæ–¹æ³•å¯¹æ¯”ç»“æœ:")
    print(comparison_df.round(4))

    # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆè¶Šå°è¶Šå¥½çš„æŒ‡æ ‡å–è´Ÿå€¼ï¼‰
    weights = {
        'load_cv': -0.3,
        'total_coupling': -0.25,
        'connectivity': 0.2,
        'modularity': 0.15,
        'power_balance': -0.1
    }

    scores = []
    for method in comparison_df.index:
        score = sum(comparison_df.loc[method, metric] * weight 
                   for metric, weight in weights.items() 
                   if metric in comparison_df.columns)
        scores.append(score)

    comparison_df['overall_score'] = scores
    comparison_df = comparison_df.sort_values('overall_score', ascending=False)

    print("\nğŸ† ç»¼åˆè¯„åˆ†æ’å:")
    print(comparison_df[['overall_score']].round(4))
    
    return comparison_df 