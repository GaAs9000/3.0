"""
UnifiedDirectorç»Ÿä¸€å¯¼æ¼”ç³»ç»Ÿå•å…ƒæµ‹è¯•

æµ‹è¯•è¦†ç›–ï¼š
- åŸºç¡€é…ç½®ç±»åŠŸèƒ½
- æ ¸å¿ƒç»„ä»¶ç‹¬ç«‹æµ‹è¯•
- é›†æˆæµ‹è¯•
- å¼‚å¸¸å¤„ç†å’Œè¾¹ç•Œæ¡ä»¶

ä½œè€…ï¼šClaude & User Collaboration
æ—¥æœŸï¼š2025-07-13
"""

import unittest
import numpy as np
import torch
import tempfile
import shutil
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
import json
import time

# å¯¼å…¥å¾…æµ‹è¯•çš„æ¨¡å—
import sys
sys.path.append(str(Path(__file__).parent.parent / 'code' / 'src'))

from rl.unified_director import (
    DataMixConfig, AdaptiveThreshold, PerformanceMonitor, 
    SafetyMonitor, CheckpointManager, TopologyTransitionManager,
    ParameterCoordinator, UnifiedDirector
)


class TestDataMixConfig(unittest.TestCase):
    """æµ‹è¯•æ•°æ®æ··åˆé…ç½®ç±»"""
    
    def setUp(self):
        self.config = DataMixConfig()
    
    def test_default_configuration(self):
        """æµ‹è¯•é»˜è®¤é…ç½®"""
        self.assertEqual(self.config.early_phase_end, 0.2)
        self.assertEqual(self.config.mid_phase_end, 0.5)
        self.assertEqual(self.config.balance_phase_end, 0.8)
        self.assertTrue(self.config.smooth_transition)
        
    def test_early_phase_mix_ratio(self):
        """æµ‹è¯•æ—©æœŸé˜¶æ®µæ•°æ®æ··åˆæ¯”ä¾‹"""
        ratio = self.config.get_current_mix_ratio(0.1)  # 10%è¿›åº¦
        self.assertEqual(ratio['small'], 1.0)
        self.assertEqual(ratio['medium'], 0.0)
        self.assertEqual(ratio['large'], 0.0)
    
    def test_mid_phase_mix_ratio(self):
        """æµ‹è¯•ä¸­æœŸé˜¶æ®µæ•°æ®æ··åˆæ¯”ä¾‹"""
        ratio = self.config.get_current_mix_ratio(0.35)  # 35%è¿›åº¦
        self.assertGreater(ratio['small'], 0.5)
        self.assertGreater(ratio['medium'], 0.0)
        self.assertEqual(ratio['large'], 0.0)
    
    def test_balance_phase_mix_ratio(self):
        """æµ‹è¯•å¹³è¡¡é˜¶æ®µæ•°æ®æ··åˆæ¯”ä¾‹"""
        ratio = self.config.get_current_mix_ratio(0.65)  # 65%è¿›åº¦
        self.assertGreater(ratio['small'], 0.0)
        self.assertGreater(ratio['medium'], 0.0)
        self.assertGreater(ratio['large'], 0.0)
    
    def test_late_phase_mix_ratio(self):
        """æµ‹è¯•åæœŸé˜¶æ®µæ•°æ®æ··åˆæ¯”ä¾‹"""
        ratio = self.config.get_current_mix_ratio(0.9)  # 90%è¿›åº¦
        self.assertLess(ratio['small'], 0.5)
        self.assertGreater(ratio['large'], 0.3)


class TestAdaptiveThreshold(unittest.TestCase):
    """æµ‹è¯•è‡ªé€‚åº”é˜ˆå€¼é…ç½®ç±»"""
    
    def setUp(self):
        self.threshold = AdaptiveThreshold()
    
    def test_default_configuration(self):
        """æµ‹è¯•é»˜è®¤é…ç½®"""
        self.assertEqual(self.threshold.min_episodes_per_stage, 200)
        self.assertEqual(self.threshold.stability_threshold, 0.8)
        self.assertTrue(self.threshold.emergency_fallback_enabled)
    
    def test_should_not_transition_insufficient_episodes(self):
        """æµ‹è¯•episodeæ•°ä¸è¶³æ—¶ä¸åº”åˆ‡æ¢"""
        metrics = {'avg_reward': 0.5, 'stability_confidence': 0.9}
        should_switch, reason = self.threshold.should_transition(metrics, 100)
        self.assertFalse(should_switch)
        self.assertIn("æœ€å°episodeè¦æ±‚", reason)
    
    def test_should_transition_max_episodes(self):
        """æµ‹è¯•è¾¾åˆ°æœ€å¤§episodeæ•°æ—¶å¼ºåˆ¶åˆ‡æ¢"""
        metrics = {'avg_reward': -2.0, 'stability_confidence': 0.3}
        should_switch, reason = self.threshold.should_transition(metrics, 1000)
        self.assertTrue(should_switch)
        self.assertIn("æœ€å¤§episodeé™åˆ¶", reason)
    
    def test_should_transition_good_performance(self):
        """æµ‹è¯•æ€§èƒ½è‰¯å¥½æ—¶åº”è¯¥åˆ‡æ¢"""
        metrics = {
            'avg_reward': 0.5,
            'quality_score': 0.6,
            'stability_confidence': 0.85
        }
        should_switch, reason = self.threshold.should_transition(metrics, 300)
        self.assertTrue(should_switch)
        self.assertIn("ç»¼åˆè¯„åˆ†", reason)


class TestPerformanceMonitor(unittest.TestCase):
    """æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨"""
    
    def setUp(self):
        self.monitor = PerformanceMonitor(window_size=10)
    
    def test_initial_state(self):
        """æµ‹è¯•åˆå§‹çŠ¶æ€"""
        metrics = self.monitor.get_metrics()
        self.assertEqual(metrics, {})
    
    def test_update_and_metrics(self):
        """æµ‹è¯•æ›´æ–°å’ŒæŒ‡æ ‡è®¡ç®—"""
        # æ·»åŠ ä¸€äº›episodeæ•°æ®
        for i in range(5):
            episode_info = {
                'reward': i * 0.1,
                'episode_length': 10 + i,
                'quality_score': 0.5 + i * 0.1,
                'connectivity': 0.8 + i * 0.02
            }
            self.monitor.update(episode_info)
        
        metrics = self.monitor.get_metrics()
        self.assertEqual(metrics['sample_count'], 5)
        self.assertAlmostEqual(metrics['avg_reward'], 0.2, places=2)
        self.assertAlmostEqual(metrics['avg_length'], 12.0, places=1)
    
    def test_stability_calculation(self):
        """æµ‹è¯•ç¨³å®šæ€§è®¡ç®—"""
        # æ·»åŠ ç¨³å®šçš„æ•°æ®
        for i in range(15):
            episode_info = {
                'reward': 0.5 + np.random.normal(0, 0.05),  # ä½å˜å¼‚
                'episode_length': 20,
                'quality_score': 0.6,
                'connectivity': 0.9
            }
            self.monitor.update(episode_info)
        
        metrics = self.monitor.get_metrics()
        self.assertIn('stability_confidence', metrics)
        self.assertGreater(metrics['stability_confidence'], 0.5)


class TestSafetyMonitor(unittest.TestCase):
    """æµ‹è¯•å®‰å…¨ç›‘æ§å™¨"""
    
    def setUp(self):
        self.config = {'max_failed_transitions': 3}
        self.monitor = SafetyMonitor(self.config)
    
    def test_initial_state(self):
        """æµ‹è¯•åˆå§‹çŠ¶æ€"""
        status = self.monitor.get_status()
        self.assertEqual(status['failed_transitions'], 0)
        self.assertEqual(status['emergency_activations'], 0)
    
    def test_safe_conditions(self):
        """æµ‹è¯•å®‰å…¨æ¡ä»¶"""
        metrics = {
            'avg_reward': 0.5,
            'avg_connectivity': 0.9,
            'stability_cv': 0.1
        }
        is_safe, warnings = self.monitor.check_safety(metrics, 'small')
        self.assertTrue(is_safe)
        self.assertEqual(len(warnings), 0)
    
    def test_unsafe_conditions(self):
        """æµ‹è¯•ä¸å®‰å…¨æ¡ä»¶"""
        metrics = {
            'avg_reward': -15.0,  # ä¸¥é‡æ€§èƒ½æ¶åŒ–
            'avg_connectivity': 0.3,  # è¿é€šæ€§ä¸‹é™
            'stability_cv': 3.0  # è®­ç»ƒä¸ç¨³å®š
        }
        is_safe, warnings = self.monitor.check_safety(metrics, 'medium')
        self.assertFalse(is_safe)
        self.assertGreater(len(warnings), 0)
    
    def test_failed_transition_tracking(self):
        """æµ‹è¯•å¤±è´¥åˆ‡æ¢è¿½è¸ª"""
        for i in range(3):
            self.monitor.record_failed_transition()
        
        # ç¬¬4æ¬¡å¤±è´¥åº”è¯¥è§¦å‘ä¸å®‰å…¨çŠ¶æ€
        metrics = {'avg_reward': 0.0}
        is_safe, warnings = self.monitor.check_safety(metrics, 'large')
        self.assertFalse(is_safe)
        self.assertIn("åˆ‡æ¢å¤±è´¥æ¬¡æ•°è¿‡å¤š", str(warnings))


class TestCheckpointManager(unittest.TestCase):
    """æµ‹è¯•æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def setUp(self):
        self.temp_dir = tempfile.mkdtemp()
        self.manager = CheckpointManager(self.temp_dir)
    
    def tearDown(self):
        shutil.rmtree(self.temp_dir)
    
    def test_save_and_load_checkpoint(self):
        """æµ‹è¯•ä¿å­˜å’ŒåŠ è½½æ£€æŸ¥ç‚¹"""
        test_state = {
            'episode': 100,
            'topology_stage': 'medium',
            'performance_data': [1, 2, 3, 4, 5]
        }
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        filepath = self.manager.save_checkpoint(test_state, 'test_checkpoint')
        self.assertIsNotNone(filepath)
        self.assertTrue(Path(filepath).exists())
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        loaded_state = self.manager.load_checkpoint('test_checkpoint')
        self.assertIsNotNone(loaded_state)
        self.assertEqual(loaded_state['episode'], 100)
        self.assertEqual(loaded_state['topology_stage'], 'medium')
    
    def test_load_latest_checkpoint(self):
        """æµ‹è¯•åŠ è½½æœ€æ–°æ£€æŸ¥ç‚¹"""
        # ä¿å­˜å¤šä¸ªæ£€æŸ¥ç‚¹
        states = [
            {'episode': 100, 'timestamp': '20250713_100000'},
            {'episode': 200, 'timestamp': '20250713_110000'},
            {'episode': 300, 'timestamp': '20250713_120000'}
        ]
        
        for i, state in enumerate(states):
            self.manager.save_checkpoint(state, f'checkpoint_{i}')
            time.sleep(0.1)  # ç¡®ä¿æ—¶é—´æˆ³ä¸åŒ
        
        # åŠ è½½æœ€æ–°çš„ï¼ˆåº”è¯¥æ˜¯æœ€åä¸€ä¸ªï¼‰
        latest_state = self.manager.load_checkpoint()
        self.assertIsNotNone(latest_state)
        self.assertEqual(latest_state['episode'], 300)


class TestTopologyTransitionManager(unittest.TestCase):
    """æµ‹è¯•æ‹“æ‰‘åˆ‡æ¢ç®¡ç†å™¨"""
    
    def setUp(self):
        self.data_config = DataMixConfig()
        self.manager = TopologyTransitionManager(self.data_config)
    
    def test_initial_state(self):
        """æµ‹è¯•åˆå§‹çŠ¶æ€"""
        self.assertEqual(self.manager.current_stage, 'small')
        self.assertEqual(self.manager.stage_episodes, 0)
    
    def test_topology_spec_early_phase(self):
        """æµ‹è¯•æ—©æœŸé˜¶æ®µæ‹“æ‰‘è§„æ ¼"""
        spec = self.manager.get_current_topology_spec(0.1)
        self.assertEqual(spec['dominant_scale'], 'small')
        self.assertEqual(spec['mix_ratio']['small'], 1.0)
    
    def test_should_not_transition_insufficient_episodes(self):
        """æµ‹è¯•episodeä¸è¶³æ—¶ä¸åº”åˆ‡æ¢"""
        metrics = {'stability_confidence': 0.8}
        should_switch, reason = self.manager.should_transition_topology(0.6, metrics)
        self.assertFalse(should_switch)
        self.assertIn("episodeä¸è¶³", reason)
    
    def test_should_not_transition_unstable(self):
        """æµ‹è¯•ä¸ç¨³å®šæ—¶ä¸åº”åˆ‡æ¢"""
        self.manager.stage_episodes = 200  # æ»¡è¶³æœ€å°episodeè¦æ±‚
        metrics = {'stability_confidence': 0.3}  # ä½ç¨³å®šæ€§
        should_switch, reason = self.manager.should_transition_topology(0.6, metrics)
        self.assertFalse(should_switch)
        self.assertIn("ä¸å¤Ÿç¨³å®š", reason)
    
    def test_successful_transition(self):
        """æµ‹è¯•æˆåŠŸåˆ‡æ¢"""
        self.manager.stage_episodes = 200
        metrics = {'stability_confidence': 0.8}
        should_switch, reason = self.manager.should_transition_topology(0.6, metrics)
        self.assertTrue(should_switch)
        
        # æ‰§è¡Œåˆ‡æ¢
        transition_info = self.manager.execute_transition('medium')
        self.assertEqual(transition_info['old_stage'], 'small')
        self.assertEqual(transition_info['new_stage'], 'medium')
        self.assertEqual(self.manager.current_stage, 'medium')
        self.assertEqual(self.manager.stage_episodes, 0)  # é‡ç½®è®¡æ•°å™¨


class TestParameterCoordinator(unittest.TestCase):
    """æµ‹è¯•å‚æ•°åè°ƒå™¨"""
    
    def setUp(self):
        self.config = {'adaptive_curriculum': {}}
        self.coordinator = ParameterCoordinator(self.config)
    
    def test_initial_state(self):
        """æµ‹è¯•åˆå§‹çŠ¶æ€"""
        self.assertFalse(self.coordinator.transition_active)
        params = self.coordinator.get_current_params(100)
        self.assertEqual(params, {})
    
    def test_prepare_transition(self):
        """æµ‹è¯•å‡†å¤‡åˆ‡æ¢"""
        target_params = self.coordinator.prepare_transition('small', 'medium', 100)
        self.assertTrue(self.coordinator.transition_active)
        self.assertEqual(self.coordinator.transition_start_episode, 100)
        self.assertIn('connectivity_penalty', target_params)
        self.assertIn('reward_weights', target_params)
    
    def test_warmup_parameter_interpolation(self):
        """æµ‹è¯•é¢„çƒ­å‚æ•°æ’å€¼"""
        # å‡†å¤‡åˆ‡æ¢
        self.coordinator.prepare_transition('small', 'medium', 100)
        self.coordinator.set_original_params({
            'connectivity_penalty': 0.3,
            'learning_rate_factor': 1.5,
            'reward_weights': {'load_b': 0.6, 'decoupling': 0.2}
        })
        
        # æµ‹è¯•é¢„çƒ­ä¸­æœŸå‚æ•°ï¼ˆ50%è¿›åº¦ï¼‰
        mid_episode = 100 + self.coordinator.warmup_episodes // 2
        current_params = self.coordinator.get_current_params(mid_episode)
        
        # åº”è¯¥æ˜¯æ’å€¼ç»“æœ
        self.assertGreater(current_params['connectivity_penalty'], 0.3)
        self.assertLess(current_params['connectivity_penalty'], 0.8)
    
    def test_warmup_completion(self):
        """æµ‹è¯•é¢„çƒ­å®Œæˆ"""
        self.coordinator.prepare_transition('small', 'large', 100)
        
        # é¢„çƒ­å®Œæˆå
        completion_episode = 100 + self.coordinator.warmup_episodes
        final_params = self.coordinator.get_current_params(completion_episode)
        
        self.assertFalse(self.coordinator.transition_active)
        self.assertEqual(final_params, self.coordinator.target_params)


class TestUnifiedDirectorIntegration(unittest.TestCase):
    """æµ‹è¯•ç»Ÿä¸€å¯¼æ¼”é›†æˆåŠŸèƒ½"""
    
    def setUp(self):
        self.config = {
            'unified_director': {
                'enabled': True,
                'data_mix': {},
                'adaptive_threshold': {},
                'safety': {},
                'checkpoint_dir': tempfile.mkdtemp()
            },
            'training': {'num_episodes': 1000},
            'adaptive_curriculum': {}
        }
        self.director = UnifiedDirector(self.config)
        self.director.set_total_episodes(1000)
    
    def tearDown(self):
        shutil.rmtree(self.config['unified_director']['checkpoint_dir'])
    
    def test_initialization(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        self.assertTrue(self.director.enabled)
        self.assertEqual(self.director.total_episodes, 1000)
        self.assertIsNotNone(self.director.topology_manager)
        self.assertIsNotNone(self.director.performance_monitor)
    
    def test_component_integration(self):
        """æµ‹è¯•ç»„ä»¶é›†æˆ"""
        mock_adaptive_director = Mock()
        mock_scale_generator = Mock()
        
        self.director.integrate_components(
            adaptive_director=mock_adaptive_director,
            scale_generator=mock_scale_generator
        )
        
        self.assertEqual(self.director.adaptive_director, mock_adaptive_director)
        self.assertEqual(self.director.scale_generator, mock_scale_generator)
    
    def test_step_decision_making(self):
        """æµ‹è¯•æ­¥éª¤å†³ç­–åˆ¶å®š"""
        episode_info = {
            'episode': 50,
            'reward': 0.3,
            'episode_length': 25,
            'quality_score': 0.4,
            'connectivity': 0.85
        }
        
        decision = self.director.step(50, episode_info)
        
        # éªŒè¯å†³ç­–ç»“æ„
        self.assertIn('timestamp', decision)
        self.assertIn('episode', decision)
        self.assertIn('progress', decision)
        self.assertIn('current_stage', decision)
        self.assertIn('topology_decision', decision)
        self.assertIn('parameter_adjustments', decision)
        self.assertIn('performance_metrics', decision)
        self.assertIn('safety_status', decision)
    
    def test_safety_fallback(self):
        """æµ‹è¯•å®‰å…¨å›é€€æœºåˆ¶"""
        # æ¨¡æ‹Ÿå±é™©æ¡ä»¶
        dangerous_episode_info = {
            'episode': 100,
            'reward': -20.0,  # æä½å¥–åŠ±
            'episode_length': 5,
            'connectivity': 0.2  # æä½è¿é€šæ€§
        }
        
        decision = self.director.step(100, dangerous_episode_info)
        
        # åº”è¯¥è§¦å‘å®‰å…¨æªæ–½
        safety_status = decision['safety_status']
        if not safety_status['is_safe']:
            self.assertIn('emergency_mode', decision)
            self.assertIn('fallback_params', decision)
    
    def test_scale_generator_coordination(self):
        """æµ‹è¯•å¤šå°ºåº¦ç”Ÿæˆå™¨åè°ƒ"""
        mock_scale_generator = Mock()
        mock_scale_generator.update_generation_config = Mock()
        
        self.director.integrate_components(scale_generator=mock_scale_generator)
        
        topology_spec = {
            'dominant_scale': 'medium',
            'mix_ratio': {'small': 0.3, 'medium': 0.7, 'large': 0.0}
        }
        
        result = self.director.coordinate_with_scale_generator(100, topology_spec)
        
        self.assertEqual(result['status'], 'success')
        self.assertIn('config', result)
        mock_scale_generator.update_generation_config.assert_called_once()


class TestUnifiedDirectorEdgeCases(unittest.TestCase):
    """æµ‹è¯•ç»Ÿä¸€å¯¼æ¼”è¾¹ç•Œæ¡ä»¶å’Œå¼‚å¸¸å¤„ç†"""
    
    def setUp(self):
        self.config = {
            'unified_director': {
                'enabled': True,
                'data_mix': {},
                'adaptive_threshold': {},
                'safety': {},
                'checkpoint_dir': tempfile.mkdtemp()
            },
            'training': {'num_episodes': 100},
            'adaptive_curriculum': {}
        }
    
    def test_disabled_director(self):
        """æµ‹è¯•ç¦ç”¨çš„ç»Ÿä¸€å¯¼æ¼”"""
        self.config['unified_director']['enabled'] = False
        director = UnifiedDirector(self.config)
        
        # åº”è¯¥å›é€€åˆ°è‡ªé€‚åº”å¯¼æ¼”
        episode_info = {'episode': 10, 'reward': 0.1}
        decision = director.step(10, episode_info)
        
        self.assertIn('status', decision)
    
    def test_missing_adaptive_director(self):
        """æµ‹è¯•ç¼ºå°‘è‡ªé€‚åº”å¯¼æ¼”çš„æƒ…å†µ"""
        director = UnifiedDirector(self.config)
        # ä¸é›†æˆè‡ªé€‚åº”å¯¼æ¼”
        
        episode_info = {'episode': 10, 'reward': 0.1}
        decision = director.step(10, episode_info)
        
        # åº”è¯¥ä¼˜é›…å¤„ç†
        adaptive_decision = decision['adaptive_decision']
        self.assertEqual(adaptive_decision['status'], 'not_available')
    
    def test_missing_scale_generator(self):
        """æµ‹è¯•ç¼ºå°‘å¤šå°ºåº¦ç”Ÿæˆå™¨çš„æƒ…å†µ"""
        director = UnifiedDirector(self.config)
        
        topology_spec = {'dominant_scale': 'small'}
        result = director.coordinate_with_scale_generator(10, topology_spec)
        
        self.assertEqual(result['status'], 'generator_not_available')
    
    def test_checkpoint_failure_recovery(self):
        """æµ‹è¯•æ£€æŸ¥ç‚¹å¤±è´¥æ¢å¤"""
        # ä½¿ç”¨æ— æ•ˆçš„æ£€æŸ¥ç‚¹ç›®å½•
        self.config['unified_director']['checkpoint_dir'] = '/invalid/path'
        
        # åº”è¯¥ä¸ä¼šå´©æºƒ
        try:
            director = UnifiedDirector(self.config)
            episode_info = {'episode': 200, 'reward': 0.1}
            decision = director.step(200, episode_info)  # è§¦å‘æ£€æŸ¥ç‚¹ä¿å­˜
            # æµ‹è¯•é€šè¿‡å¦‚æœæ²¡æœ‰å¼‚å¸¸
        except Exception as e:
            self.fail(f"ç»Ÿä¸€å¯¼æ¼”åº”è¯¥ä¼˜é›…å¤„ç†æ£€æŸ¥ç‚¹é”™è¯¯ï¼Œä½†æŠ›å‡ºäº†: {e}")


def run_integration_test():
    """è¿è¡Œé›†æˆæµ‹è¯•"""
    print("ğŸ§ª è¿è¡ŒUnifiedDirectoré›†æˆæµ‹è¯•...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®
    config = {
        'unified_director': {
            'enabled': True,
            'data_mix': {},
            'adaptive_threshold': {'min_episodes_per_stage': 50},
            'safety': {},
            'checkpoint_dir': tempfile.mkdtemp()
        },
        'training': {'num_episodes': 300},
        'adaptive_curriculum': {}
    }
    
    try:
        # åˆå§‹åŒ–ç»Ÿä¸€å¯¼æ¼”
        director = UnifiedDirector(config)
        director.set_total_episodes(300)
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        for episode in range(100):
            episode_info = {
                'episode': episode,
                'reward': np.random.normal(0.0, 0.5),
                'episode_length': np.random.randint(10, 30),
                'quality_score': min(1.0, max(0.0, 0.3 + episode * 0.002)),
                'connectivity': min(1.0, max(0.5, 0.7 + episode * 0.001))
            }
            
            decision = director.step(episode, episode_info)
            
            # éªŒè¯å†³ç­–ç»“æ„
            assert 'topology_decision' in decision
            assert 'parameter_adjustments' in decision
            assert 'performance_metrics' in decision
            assert 'safety_status' in decision
        
        # æ£€æŸ¥çŠ¶æ€æ‘˜è¦
        summary = director.get_status_summary()
        assert summary['current_episode'] == 99
        assert 'coordinator_status' in summary
        assert 'performance_summary' in summary
        
        print("âœ… é›†æˆæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # æ¸…ç†
        shutil.rmtree(config['unified_director']['checkpoint_dir'])


if __name__ == '__main__':
    # è¿è¡Œå•å…ƒæµ‹è¯•
    print("ğŸ§ª å¼€å§‹UnifiedDirectorå•å…ƒæµ‹è¯•...")
    unittest.main(argv=[''], exit=False, verbosity=2)
    
    # è¿è¡Œé›†æˆæµ‹è¯•
    run_integration_test()
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")