#!/usr/bin/env python3
"""
åˆ†åŒºå¯¹æ¯”å¯è§†åŒ–å™¨
ç®€æ´é«˜æ•ˆåœ°å¯¹æ¯”å±•ç¤ºbaselineå’Œagentçš„åˆ†åŒºç»“æœ
æ”¯æŒä¸­æ–‡æ§åˆ¶å°è¾“å‡ºå’Œè‹±æ–‡å›¾ç‰‡æ–‡æœ¬
"""

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import matplotlib.font_manager as fm
import networkx as nx
import numpy as np
from typing import Dict, Tuple, Optional, Any
import seaborn as sns


class PartitionComparator:
    """åˆ†åŒºç»“æœå¯¹æ¯”å¯è§†åŒ–å™¨"""
    
    def __init__(self):
        # è®¾ç½®matplotlibæ ·å¼å’Œå­—ä½“
        plt.style.use('seaborn-v0_8-whitegrid')
        
        # æ™ºèƒ½å­—ä½“é…ç½®
        self._setup_fonts()
        
        self.colors = sns.color_palette("husl", 8)  # æ”¯æŒæœ€å¤š8ä¸ªåˆ†åŒº
        
    def _setup_fonts(self):
        """æ™ºèƒ½è®¾ç½®å­—ä½“ï¼Œä¼˜å…ˆä½¿ç”¨ç³»ç»Ÿå¯ç”¨çš„æœ€ä½³å­—ä½“"""
        import matplotlib
        matplotlib.rcParams.update(matplotlib.rcParamsDefault)  # åˆ·æ–°ç¼“å­˜
        
        # è·å–ç³»ç»Ÿå¯ç”¨å­—ä½“
        available_fonts = [f.name for f in fm.fontManager.ttflist]
        
        # å­—ä½“ä¼˜å…ˆçº§åˆ—è¡¨ (æŒ‰è´¨é‡å’Œå…¼å®¹æ€§æ’åº)
        preferred_fonts = [
            'DejaVu Sans',          # ä¼˜ç§€çš„Unicodeæ”¯æŒ
            'Liberation Sans',      # Linuxæ ‡å‡†å­—ä½“
            'Arial',                # Windowsæ ‡å‡†å­—ä½“  
            'Helvetica',            # Macæ ‡å‡†å­—ä½“
            'Bitstream Vera Sans',  # å¤‡ç”¨å­—ä½“
            'sans-serif'            # ç³»ç»Ÿé»˜è®¤
        ]
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„å­—ä½“
        selected_fonts = []
        for font in preferred_fonts:
            if font in available_fonts or font == 'sans-serif':
                selected_fonts.append(font)
        
        # åº”ç”¨å­—ä½“é…ç½®
        plt.rcParams['font.family'] = 'sans-serif'
        plt.rcParams['font.sans-serif'] = selected_fonts
        plt.rcParams['axes.unicode_minus'] = False
        
        print(f"ğŸ“ å¯è§†åŒ–ä½¿ç”¨å­—ä½“: {selected_fonts[0] if selected_fonts else 'default'}")
        
    def _get_safe_text(self, text: str) -> str:
        """è·å–å®‰å…¨çš„æ–‡æœ¬ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦æ¸²æŸ“é—®é¢˜"""
        # æ›¿æ¢å¯èƒ½æœ‰é—®é¢˜çš„ç‰¹æ®Šå­—ç¬¦
        replacements = {
            'âœ“': 'Pass',
            'âœ—': 'Fail', 
            'Ã—': 'x',
            'âˆš': 'OK',
            'Â±': '+/-',
            'â‰¥': '>=',
            'â‰¤': '<=',
            'â†’': '->',
            'â†': '<-',
            'â†‘': '^',
            'â†“': 'v'
        }
        
        safe_text = text
        for old, new in replacements.items():
            safe_text = safe_text.replace(old, new)
        
        return safe_text
        
    def compare_partitions(self,
                          adj_matrix: np.ndarray,
                          baseline_partition: np.ndarray,
                          agent_partition: np.ndarray,
                          baseline_metrics: Dict[str, float],
                          agent_metrics: Dict[str, float],
                          node_positions: Optional[Dict] = None,
                          scenario_name: str = "Normal",
                          save_path: str = None) -> str:
        """
        å¯¹æ¯”ä¸¤ç§åˆ†åŒºæ–¹æ³•çš„ç»“æœ
        
        Args:
            adj_matrix: é‚»æ¥çŸ©é˜µ
            baseline_partition: baselineåˆ†åŒºç»“æœ (1-indexed)
            agent_partition: agentåˆ†åŒºç»“æœ (1-indexed)
            baseline_metrics: baselineæ€§èƒ½æŒ‡æ ‡
            agent_metrics: agentæ€§èƒ½æŒ‡æ ‡
            node_positions: èŠ‚ç‚¹ä½ç½®å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨è®¡ç®—
            scenario_name: åœºæ™¯åç§°
            save_path: ä¿å­˜è·¯å¾„
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # åˆ›å»ºNetworkXå›¾
        G = nx.from_numpy_array(adj_matrix)
        
        # è®¡ç®—èŠ‚ç‚¹ä½ç½®ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if node_positions is None:
            # ä½¿ç”¨spring layoutï¼Œä½†å›ºå®šéšæœºç§å­ä¿è¯ä¸€è‡´æ€§
            node_positions = nx.spring_layout(G, k=2/np.sqrt(len(G)), seed=42)
        
        # åˆ›å»ºå›¾å½¢
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        fig.suptitle(f'Partition Comparison - {scenario_name} Scenario', fontsize=16, fontweight='bold')
        
        # ç»˜åˆ¶baselineç»“æœ
        self._draw_partition(G, baseline_partition, node_positions, ax1, 
                           "Baseline (Spectral)", baseline_metrics)
        
        # ç»˜åˆ¶agentç»“æœ
        self._draw_partition(G, agent_partition, node_positions, ax2, 
                           "RL Agent", agent_metrics)
        
        # æ·»åŠ æ•´ä½“æ€§èƒ½å¯¹æ¯”
        self._add_comparison_text(fig, baseline_metrics, agent_metrics)
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        if save_path is None:
            save_path = f"result/output/partition_comparison_{scenario_name.lower()}.png"
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return save_path
    
    def _draw_partition(self, G: nx.Graph, partition: np.ndarray, 
                       pos: Dict, ax: plt.Axes, title: str, metrics: Dict[str, float]):
        """ç»˜åˆ¶å•ä¸ªåˆ†åŒºç»“æœ"""
        
        # èŠ‚ç‚¹é¢œè‰²æ˜ å°„
        node_colors = []
        for i in range(len(G)):
            if i < len(partition) and partition[i] > 0:
                color_idx = int(partition[i] - 1) % len(self.colors)
                node_colors.append(self.colors[color_idx])
            else:
                node_colors.append('gray')
        
        # è¾¹çš„æ ·å¼ï¼šè·¨åˆ†åŒºè¾¹ç”¨è™šçº¿
        edge_styles = []
        edge_colors = []
        edge_widths = []
        
        for u, v in G.edges():
            if u < len(partition) and v < len(partition):
                if int(partition[u]) == int(partition[v]):
                    edge_styles.append('solid')
                    edge_colors.append('gray')
                    edge_widths.append(1.0)
                else:
                    edge_styles.append('dashed')
                    edge_colors.append('red')
                    edge_widths.append(2.0)
            else:
                edge_styles.append('solid')
                edge_colors.append('gray')
                edge_widths.append(1.0)
        
        # ç»˜åˆ¶ç½‘ç»œ
        nx.draw_networkx_nodes(G, pos, node_color=node_colors, 
                             node_size=500, ax=ax, alpha=0.9)
        
        # åˆ†åˆ«ç»˜åˆ¶ä¸åŒæ ·å¼çš„è¾¹
        solid_edges = [(u, v) for i, (u, v) in enumerate(G.edges()) if edge_styles[i] == 'solid']
        dashed_edges = [(u, v) for i, (u, v) in enumerate(G.edges()) if edge_styles[i] == 'dashed']
        
        nx.draw_networkx_edges(G, pos, edgelist=solid_edges, 
                             edge_color='gray', width=1.0, ax=ax)
        nx.draw_networkx_edges(G, pos, edgelist=dashed_edges, 
                             edge_color='red', width=2.0, style='dashed', ax=ax)
        
        # æ·»åŠ èŠ‚ç‚¹æ ‡ç­¾
        nx.draw_networkx_labels(G, pos, ax=ax, font_size=10)
        
        # è®¾ç½®æ ‡é¢˜å’ŒæŒ‡æ ‡
        ax.set_title(title, fontsize=14, fontweight='bold', pad=10)
        
        # æ·»åŠ å®Œæ•´çš„test.pyæŒ‡æ ‡ï¼Œä½¿ç”¨å®‰å…¨æ–‡æœ¬é¿å…æ¸²æŸ“é—®é¢˜
        metrics_text = "Raw Metrics (lower is better):\n"
        metrics_text += f"Load_B (CV): {metrics.get('cv', 0):.3f}\n"
        metrics_text += f"Coupling: {metrics.get('coupling', 0):.3f}\n"
        metrics_text += f"Power_B: {metrics.get('power_b', 0):.3f}\n\n"
        
        metrics_text += "Converted Scores (higher is better):\n"
        metrics_text += f"Load_B Score: {metrics.get('load_b_score', 0):.3f}\n"
        metrics_text += f"Decoupling Score: {metrics.get('decoupling_score', 0):.3f}\n"
        metrics_text += f"Power_B Score: {metrics.get('power_b_score', 0):.3f}\n\n"
        
        metrics_text += f"Comprehensive Score: {metrics.get('reward', 0):.3f}\n"
        # ä½¿ç”¨å®‰å…¨æ–‡æœ¬å‡½æ•°å¤„ç†ç‰¹æ®Šå­—ç¬¦
        success_text = "Pass" if metrics.get('success', False) else "Fail"
        metrics_text += f"Success: {success_text}"
        
        # åº”ç”¨å®‰å…¨æ–‡æœ¬è½¬æ¢
        safe_metrics_text = self._get_safe_text(metrics_text)
        
        ax.text(0.02, 0.98, safe_metrics_text, transform=ax.transAxes,
                verticalalignment='top', horizontalalignment='left',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8),
                fontsize=8)  # ç¼©å°å­—ä½“ä»¥é€‚åº”æ›´å¤šä¿¡æ¯
        
        # æ·»åŠ åˆ†åŒºå›¾ä¾‹
        unique_partitions = np.unique(partition[partition > 0])
        if len(unique_partitions) > 0:
            patches = []
            for p in unique_partitions:
                color_idx = (p - 1) % len(self.colors)
                patches.append(mpatches.Patch(color=self.colors[color_idx], 
                                            label=f'Partition {p}'))
            ax.legend(handles=patches, loc='upper right', fontsize=9)
        
        ax.axis('off')
    
    def _add_comparison_text(self, fig: plt.figure, 
                           baseline_metrics: Dict[str, float], 
                           agent_metrics: Dict[str, float]):
        """Add complete test.py metrics comparison information"""
        
        # Calculate all metric improvements
        comparison_text = "Agent vs Baseline Improvements:\n"
        
        # Raw metric improvements (lower is better, so positive % means improvement)
        if baseline_metrics.get('cv', 0) > 0:
            cv_improvement = (baseline_metrics['cv'] - agent_metrics['cv']) / baseline_metrics['cv'] * 100
            comparison_text += f"Load_B Improvement: {cv_improvement:+.1f}%\n"
        
        if baseline_metrics.get('coupling', 0) > 0:
            coupling_improvement = (baseline_metrics['coupling'] - agent_metrics['coupling']) / baseline_metrics['coupling'] * 100
            comparison_text += f"Coupling Improvement: {coupling_improvement:+.1f}%\n"
        
        if baseline_metrics.get('power_b', 0) > 0:
            power_improvement = (baseline_metrics['power_b'] - agent_metrics['power_b']) / baseline_metrics['power_b'] * 100
            comparison_text += f"Power_B Improvement: {power_improvement:+.1f}%\n"
        
        # Comprehensive score improvement (higher is better)
        if baseline_metrics.get('reward', 0) > 0:
            reward_improvement = (agent_metrics['reward'] - baseline_metrics['reward']) / baseline_metrics['reward'] * 100
            comparison_text += f"Comprehensive Score Improvement: {reward_improvement:+.1f}%"
        
        # åº”ç”¨å®‰å…¨æ–‡æœ¬è½¬æ¢
        safe_comparison_text = self._get_safe_text(comparison_text)
        
        # Add comparison text at the bottom
        fig.text(0.5, 0.02, safe_comparison_text, ha='center', fontsize=10,
                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))
    
    def compare_all_methods(self,
                           adj_matrix: np.ndarray,
                           partitions: Dict[str, np.ndarray],
                           metrics: Dict[str, Dict[str, float]],
                           node_positions: Optional[Dict] = None,
                           scenario_name: str = "Normal",
                           save_path: str = None) -> str:
        """
        å¯¹æ¯”æ‰€æœ‰æ–¹æ³•çš„åˆ†åŒºç»“æœ - 2x4å¸ƒå±€
        
        Args:
            adj_matrix: é‚»æ¥çŸ©é˜µ
            partitions: æ‰€æœ‰æ–¹æ³•çš„åˆ†åŒºç»“æœ {method_name: partition}
            metrics: æ‰€æœ‰æ–¹æ³•çš„æ€§èƒ½æŒ‡æ ‡ {method_name: metrics_dict}
            node_positions: èŠ‚ç‚¹ä½ç½®å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨è®¡ç®—
            scenario_name: åœºæ™¯åç§°
            save_path: ä¿å­˜è·¯å¾„
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # åˆ›å»ºNetworkXå›¾
        G = nx.from_numpy_array(adj_matrix)
        
        # è®¡ç®—èŠ‚ç‚¹ä½ç½®ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if node_positions is None:
            # ä½¿ç”¨spring layoutï¼Œä½†å›ºå®šéšæœºç§å­ä¿è¯ä¸€è‡´æ€§
            node_positions = nx.spring_layout(G, k=2/np.sqrt(len(G)), seed=42)
        
        # åˆ›å»º2x4å¸ƒå±€çš„å›¾å½¢
        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        fig.suptitle(f'All Methods Comparison - {scenario_name} Scenario', fontsize=16, fontweight='bold')
        
        # æ–¹æ³•åç§°é¡ºåº (ç¡®ä¿Agentåœ¨ç¬¬ä¸€ä½)
        method_order = ['Agent']
        other_methods = [name for name in partitions.keys() if name != 'Agent']
        method_order.extend(other_methods)
        
        # ç»˜åˆ¶æ‰€æœ‰æ–¹æ³•çš„ç»“æœ
        for idx, method_name in enumerate(method_order):
            if idx >= 8:  # åªæ˜¾ç¤ºå‰8ä¸ªæ–¹æ³•
                break
                
            row = idx // 4
            col = idx % 4
            ax = axes[row, col]
            
            if method_name in partitions and method_name in metrics:
                self._draw_partition(G, partitions[method_name], node_positions, ax, 
                                   method_name, metrics[method_name])
            else:
                # å¦‚æœæŸä¸ªæ–¹æ³•ä¸å­˜åœ¨ï¼Œæ˜¾ç¤ºç©ºç™½
                ax.set_title(f'{method_name} (Not Available)', fontsize=14)
                ax.axis('off')
        
        # å¦‚æœæ–¹æ³•æ•°é‡å°‘äº8ä¸ªï¼Œéšè—å¤šä½™çš„å­å›¾
        total_methods = len(method_order)
        for idx in range(total_methods, 8):
            row = idx // 4
            col = idx % 4
            axes[row, col].axis('off')
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        if save_path is None:
            save_path = f"result/output/all_methods_comparison_{scenario_name.lower()}.png"
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return save_path


# ä¾¿æ·å‡½æ•°
def compare_partitions(adj_matrix: np.ndarray,
                      baseline_partition: np.ndarray,
                      agent_partition: np.ndarray,
                      baseline_metrics: Dict[str, float],
                      agent_metrics: Dict[str, float],
                      scenario_name: str = "Normal") -> str:
    """ä¾¿æ·å‡½æ•°ï¼šå¯¹æ¯”ä¸¤ç§åˆ†åŒºæ–¹æ³•"""
    comparator = PartitionComparator()
    return comparator.compare_partitions(
        adj_matrix, baseline_partition, agent_partition,
        baseline_metrics, agent_metrics, 
        scenario_name=scenario_name
    )