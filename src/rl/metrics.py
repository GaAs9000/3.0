import numpy as np
import torch
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from sklearn.metrics import silhouette_score
from torch_geometric.data import Data
from collections import deque, defaultdict
import random
import copy
from torch_scatter import scatter_add, scatter_mean

@dataclass
class PartitionMetrics:
    """åˆ†åŒºè¯„ä¼°æŒ‡æ ‡æ•°æ®ç±»"""
    load_cv: float          # è´Ÿè·å˜å¼‚ç³»æ•°
    load_gini: float        # è´Ÿè·åŸºå°¼ç³»æ•°
    total_coupling: float   # æ€»è€¦åˆåº¦
    inter_region_lines: int # è·¨åŒºåŸŸçº¿è·¯æ•°
    connectivity: float     # è¿é€šæ€§å¾—åˆ†
    power_balance: float    # åŠŸç‡å¹³è¡¡åº¦
    efficiency: float       # åˆ†åŒºæ•ˆç‡
    modularity: float       # æ¨¡å—åº¦


class PowerGridPartitionEnv:
    """
    ç”µç½‘åˆ†åŒºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
    
    å®ç°ç‰¹ç‚¹ï¼š
    1. å¤šç›®æ ‡å¥–åŠ±å‡½æ•°ï¼ˆè´Ÿè·å‡è¡¡ã€è§£è€¦ã€è¿é€šæ€§ã€åŠŸç‡å¹³è¡¡ç­‰ï¼‰
    2. ç‰©ç†çº¦æŸæ£€æŸ¥ï¼ˆåŠŸç‡å¹³è¡¡ã€ç”µå‹é™åˆ¶ã€N-1å®‰å…¨ï¼‰
    3. å¢é‡å¼åˆ†åŒºï¼ˆæ¯æ­¥åˆ†é…ä¸€ä¸ªè¾¹ç•ŒèŠ‚ç‚¹ï¼‰
    4. è‡ªé€‚åº”å¥–åŠ±æ ‡å‡†åŒ–
    5. è¯¦ç»†çš„æŒ‡æ ‡è¿½è¸ª
    """
    
    def __init__(self, data: Data, embeddings: torch.Tensor, K: int = None,
                 device: str = 'cpu', enable_physics_constraints: bool = True,
                 reward_weights: Optional[Dict[str, float]] = None):
        """
        åˆå§‹åŒ–ç¯å¢ƒ
        
        å‚æ•°:
            data: PyGæ•°æ®å¯¹è±¡
            embeddings: GATç¼–ç åçš„èŠ‚ç‚¹åµŒå…¥
            K: ç›®æ ‡åˆ†åŒºæ•°
            device: è®¡ç®—è®¾å¤‡
            enable_physics_constraints: æ˜¯å¦å¯ç”¨ç‰©ç†çº¦æŸ
            reward_weights: å¥–åŠ±æƒé‡å­—å…¸
        """
        # å¯¼å…¥é…ç½®
        try:
            from config import NUM_REGIONS
            if K is None:
                K = NUM_REGIONS
        except ImportError:
            if K is None:
                K = 3  # é»˜è®¤åˆ†åŒºæ•°é‡
        
        self.device = torch.device(device)
        self.data = data.to(self.device)
        self.embeddings = embeddings.to(self.device)
        self.K = K
        self.N = data.num_nodes
        self.enable_physics_constraints = enable_physics_constraints
        
        # æå–èŠ‚ç‚¹ç‰¹å¾
        self.Pd = data.x[:, 0]  # æœ‰åŠŸè´Ÿè·
        self.Qd = data.x[:, 1]  # æ— åŠŸè´Ÿè·
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å‘ç”µæ•°æ®
        if data.x.shape[1] > 8:  # æœ‰å‘ç”µæœºç‰¹å¾
            self.Pg = data.x[:, 8]   # æœ‰åŠŸå‘ç”µ
            self.Qg = data.x[:, 9]   # æ— åŠŸå‘ç”µ
            self.is_gen = data.x[:, 10] > 0.5
        else:
            self.Pg = torch.zeros_like(self.Pd)
            self.Qg = torch.zeros_like(self.Qd)
            self.is_gen = torch.zeros(self.N, dtype=torch.bool, device=self.device)
        
        # æå–è¾¹ç‰¹å¾
        self.edge_index = data.edge_index
        self.edge_attr = data.edge_attr
        
        # è®¡ç®—å¯¼çº³ï¼ˆç”¨äºè€¦åˆåº¦è®¡ç®—ï¼‰ - ç›´æ¥ä½¿ç”¨é¢„å…ˆè®¡ç®—å¥½çš„å¯¼çº³ç‰¹å¾ï¼ˆç¬¬4åˆ—ï¼‰
        self.admittance = self.edge_attr[:, 4]
        
        # å¤„ç†å¯¼çº³ä¸­çš„NaNå’Œinfå€¼
        nan_mask = torch.isnan(self.admittance) | torch.isinf(self.admittance)
        if nan_mask.any():
            print(f"âš ï¸ è­¦å‘Šï¼šå‘ç° {nan_mask.sum().item()} ä¸ªå¼‚å¸¸å¯¼çº³å€¼ï¼Œè¿›è¡Œä¿®å¤...")
            self.admittance = torch.where(nan_mask, torch.tensor(1e-10, device=self.device), self.admittance)
        
        # æ„å»ºé‚»æ¥è¡¨ï¼ˆç”¨äºå¿«é€ŸæŸ¥è¯¢ï¼‰
        self.adj_list = self._build_adjacency_list()
        
        # é¢„è®¡ç®—å¹³å‡è´Ÿè½½å’Œåº¦ï¼Œç”¨äºå¥–åŠ±å‡½æ•°
        self.avg_load = self.Pd.mean().item()
        self.avg_degree = np.mean([len(adj) for adj in self.adj_list.values()])
        
        # å¥–åŠ±æƒé‡ï¼ˆä¿®æ”¹ä»¥å¼ºåŒ–å¹³è¡¡æ€§ï¼‰
        default_weights = {
            'balance': 0.4,      # è´Ÿè·å‡è¡¡ï¼ˆæé«˜æƒé‡ï¼‰
            'coupling': 0.2,     # è§£è€¦åº¦
            'connectivity': 0.15, # è¿é€šæ€§
            'power_balance': 0.1, # åŠŸç‡å¹³è¡¡
            'progress': 0.1,     # è¿›åº¦å¥–åŠ±
            'efficiency': 0.05   # æ•ˆç‡å¥–åŠ±
        }
        self.reward_weights = reward_weights or default_weights
        
        # å¥–åŠ±æ ‡å‡†åŒ–å‚æ•°ï¼ˆåœ¨çº¿æ›´æ–°ï¼‰
        self.reward_stats = {
            key: {'mean': 0.0, 'std': 1.0, 'count': 0}
            for key in ['cv', 'coupling', 'connectivity', 'balance']
        }
        
        # åŠ¨ä½œå†å²ï¼ˆç”¨äºåˆ†æï¼‰
        self.action_history = []
        self.reward_history = []
        self.metrics_history = []
        
        # é‡ç½®ç¯å¢ƒ
        self.reset()
    
    def _build_adjacency_list(self) -> Dict[int, List[int]]:
        """æ„å»ºé‚»æ¥è¡¨åŠ é€Ÿé‚»å±…æŸ¥è¯¢"""
        adj_list = defaultdict(list)
        edge_array = self.edge_index.cpu().numpy()
        
        for i in range(edge_array.shape[1]):
            u, v = edge_array[0, i], edge_array[1, i]
            adj_list[u].append(v)
        
        return dict(adj_list)
    
    def reset(self) -> Dict[str, torch.Tensor]:
        """
        é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€
        
        åˆå§‹åŒ–ç­–ç•¥ï¼š
        1. éšæœºé€‰æ‹©Kä¸ªç§å­èŠ‚ç‚¹ï¼ˆè€ƒè™‘èŠ‚ç‚¹é‡è¦æ€§ï¼‰
        2. ç§å­èŠ‚ç‚¹ä¼˜å…ˆé€‰æ‹©å‘ç”µèŠ‚ç‚¹æˆ–é«˜è´Ÿè·èŠ‚ç‚¹
        3. ç§å­èŠ‚ç‚¹ä¹‹é—´ä¿æŒä¸€å®šè·ç¦»
        """
        self.t = 0  # æ—¶é—´æ­¥
        self.z = torch.zeros(self.N, dtype=torch.long, device=self.device)  # èŠ‚ç‚¹åˆ†é…
        
        # æ™ºèƒ½é€‰æ‹©ç§å­èŠ‚ç‚¹
        seed_nodes = self._select_seed_nodes()
        for k, node in enumerate(seed_nodes):
            self.z[node] = k + 1
        
        # æ›´æ–°ç¯å¢ƒçŠ¶æ€
        self._update_state()
        
        # æ¸…ç©ºå†å²
        self.action_history.clear()
        self.reward_history.clear()
        self.metrics_history.clear()
        
        return self.get_state()
    
    def _select_seed_nodes(self) -> List[int]:
        """
        æ™ºèƒ½é€‰æ‹©ç§å­èŠ‚ç‚¹
        
        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆé€‰æ‹©å‘ç”µèŠ‚ç‚¹
        2. å…¶æ¬¡é€‰æ‹©é«˜è´Ÿè·èŠ‚ç‚¹
        3. ç¡®ä¿ç§å­ä¹‹é—´æœ‰ä¸€å®šè·ç¦»
        """
        # è®¡ç®—èŠ‚ç‚¹é‡è¦æ€§å¾—åˆ†
        importance = self.Pd + self.Pg * 2.0  # å‘ç”µèŠ‚ç‚¹æƒé‡æ›´é«˜
        
        # å¦‚æœæœ‰å‘ç”µèŠ‚ç‚¹ï¼Œä¼˜å…ˆé€‰æ‹©
        gen_nodes = torch.where(self.is_gen)[0]
        
        seeds = []
        
        # é¦–å…ˆä»å‘ç”µèŠ‚ç‚¹ä¸­é€‰æ‹©
        if len(gen_nodes) > 0:
            # é€‰æ‹©æœ€é‡è¦çš„å‘ç”µèŠ‚ç‚¹
            gen_importance = importance[gen_nodes]
            first_gen = gen_nodes[torch.argmax(gen_importance)]
            seeds.append(first_gen.item())
        
        # ç„¶åé€‰æ‹©å…¶ä»–ç§å­ï¼Œç¡®ä¿åˆ†æ•£
        while len(seeds) < self.K:
            if len(seeds) == 0:
                # å¦‚æœæ²¡æœ‰å‘ç”µèŠ‚ç‚¹ï¼Œé€‰æ‹©æœ€é«˜è´Ÿè·èŠ‚ç‚¹
                next_seed = torch.argmax(importance).item()
            else:
                # è®¡ç®—åˆ°å·²æœ‰ç§å­çš„æœ€å°è·ç¦»
                min_distances = torch.full((self.N,), float('inf'), device=self.device)
                
                for seed in seeds:
                    distances = self._compute_graph_distances(seed)
                    min_distances = torch.minimum(min_distances, distances)
                
                # æ’é™¤å·²é€‰ç§å­
                for seed in seeds:
                    min_distances[seed] = -1
                
                # é€‰æ‹©è·ç¦»æœ€è¿œä¸”é‡è¦æ€§é«˜çš„èŠ‚ç‚¹
                scores = min_distances * importance
                next_seed = torch.argmax(scores).item()
            
            seeds.append(next_seed)
        
        return seeds[:self.K]
    
    def _compute_graph_distances(self, source: int) -> torch.Tensor:
        """è®¡ç®—ä»æºèŠ‚ç‚¹åˆ°æ‰€æœ‰èŠ‚ç‚¹çš„å›¾è·ç¦»ï¼ˆBFSï¼‰"""
        distances = torch.full((self.N,), float('inf'), device=self.device)
        distances[source] = 0
        
        queue = deque([source])
        
        while queue:
            u = queue.popleft()
            
            if u in self.adj_list:
                for v in self.adj_list[u]:
                    if distances[v] == float('inf'):
                        distances[v] = distances[u] + 1
                        queue.append(v)
        
        return distances
    
    def _update_state(self):
        """æ›´æ–°ç¯å¢ƒå†…éƒ¨çŠ¶æ€"""
        # æ›´æ–°è¾¹ç•ŒèŠ‚ç‚¹é›†åˆ
        self.boundary_nodes = self._get_boundary_nodes()
        
        # æ›´æ–°åŒºåŸŸåµŒå…¥
        self.region_embeddings = self._compute_region_embeddings()
        
        # æ›´æ–°å…¨å±€ä¸Šä¸‹æ–‡
        self.global_context = self._compute_global_context()
        
        # è®¡ç®—å½“å‰æŒ‡æ ‡
        self.current_metrics = self._compute_metrics()
    
    def _get_boundary_nodes(self) -> torch.Tensor:
        """è·å–è¾¹ç•ŒèŠ‚ç‚¹ï¼ˆæœªåˆ†é…ä½†é‚»æ¥å·²åˆ†é…èŠ‚ç‚¹ï¼‰"""
        boundary_set = set()
        
        # éå†æ‰€æœ‰æœªåˆ†é…èŠ‚ç‚¹
        unassigned = torch.where(self.z == 0)[0]
        
        for node in unassigned:
            node_idx = node.item()
            if node_idx in self.adj_list:
                # æ£€æŸ¥æ˜¯å¦æœ‰å·²åˆ†é…çš„é‚»å±…
                for neighbor in self.adj_list[node_idx]:
                    if self.z[neighbor] > 0:
                        boundary_set.add(node_idx)
                        break
        
        return torch.tensor(list(boundary_set), dtype=torch.long, device=self.device)
    
    def _compute_region_embeddings(self) -> torch.Tensor:
        """
        è®¡ç®—åŒºåŸŸåµŒå…¥ï¼ˆèšåˆåŒºåŸŸå†…èŠ‚ç‚¹çš„åµŒå…¥ï¼‰
        
        ä½¿ç”¨åŠ æƒå¹³å‡ï¼Œæƒé‡åŸºäºèŠ‚ç‚¹é‡è¦æ€§ï¼ˆè´Ÿè·+å‘ç”µï¼‰
        """
        region_embs = []
        
        for k in range(1, self.K + 1):
            mask = (self.z == k)
            
            if mask.any():
                # å‘é‡åŒ–çš„åŠ æƒå¹³å‡
                node_weights = self.Pd[mask] + self.Pg[mask] + 1e-6
                node_weights = node_weights / node_weights.sum()
                
                # ä½¿ç”¨einsumè¿›è¡Œé«˜æ•ˆçš„åŠ æƒæ±‚å’Œ
                weighted_emb = torch.einsum('n,nd->d', node_weights, self.embeddings[mask])
                region_embs.append(weighted_emb)
            else:
                region_embs.append(torch.zeros_like(self.embeddings[0]))
        
        return torch.stack(region_embs)
    
    def _compute_global_context(self) -> torch.Tensor:
        """
        è®¡ç®—å…¨å±€ä¸Šä¸‹æ–‡å‘é‡
        
        åŒ…å«ï¼š
        1. å·²åˆ†é…èŠ‚ç‚¹çš„å¹³å‡åµŒå…¥
        2. åˆ†åŒºè¿›åº¦
        3. å½“å‰è´Ÿè·åˆ†å¸ƒç»Ÿè®¡
        4. å¹³è¡¡åº¦æŒ‡æ ‡
        """
        # å·²åˆ†é…èŠ‚ç‚¹çš„å¹³å‡åµŒå…¥
        assigned_mask = (self.z > 0)
        if assigned_mask.any():
            mean_embedding = self.embeddings[assigned_mask].mean(dim=0)
        else:
            mean_embedding = torch.zeros_like(self.embeddings[0])
        
        # åˆ†åŒºè¿›åº¦
        progress = assigned_mask.float().mean()
        
        # å„åŒºåŸŸè´Ÿè·æ¯”ä¾‹
        region_loads = []
        total_load = self.Pd.sum()
        
        for k in range(1, self.K + 1):
            mask = (self.z == k)
            if mask.any():
                region_load = self.Pd[mask].sum() / (total_load + 1e-10)
            else:
                region_load = 0.0
            region_loads.append(region_load)
        
        # å½“å‰å¹³è¡¡åº¦ï¼ˆè´Ÿçš„å˜å¼‚ç³»æ•°ï¼‰
        if len(region_loads) > 0 and sum(region_loads) > 0:
            loads_tensor = torch.tensor(region_loads, device=self.device)
            balance_score = -loads_tensor.std() / (loads_tensor.mean() + 1e-10)
        else:
            balance_score = torch.tensor(0.0, device=self.device)
        
        # ç»„åˆæ‰€æœ‰ä¿¡æ¯
        context = torch.cat([
            mean_embedding,
            progress.unsqueeze(0),
            torch.tensor(region_loads, device=self.device),
            balance_score.unsqueeze(0)
        ])
        
        return context
    
    def get_state(self) -> Dict[str, torch.Tensor]:
        """è·å–å½“å‰çŠ¶æ€å­—å…¸"""
        return {
            'node_embeddings': self.embeddings,
            'z': self.z,
            'region_embeddings': self.region_embeddings,
            'global_context': self.global_context,
            'boundary_nodes': self.boundary_nodes,
            't': self.t,
            'node_features': self.data.x,  # åŸå§‹ç‰¹å¾
            'edge_index': self.edge_index,
            'edge_attr': self.edge_attr
        }
    
    def get_valid_actions(self) -> List[Tuple[int, int]]:
        """
        è·å–å½“å‰æœ‰æ•ˆåŠ¨ä½œåˆ—è¡¨
        
        æœ‰æ•ˆåŠ¨ä½œï¼šå°†è¾¹ç•ŒèŠ‚ç‚¹åˆ†é…åˆ°å…¶é‚»æ¥çš„åŒºåŸŸ
        å¦‚æœå¯ç”¨ç‰©ç†çº¦æŸï¼Œè¿˜éœ€è¦é€šè¿‡çº¦æŸæ£€æŸ¥
        
        ä¿®æ”¹ï¼šä¼˜å…ˆè€ƒè™‘è¾ƒå°çš„åŒºåŸŸï¼Œé¿å…æç«¯ä¸å¹³è¡¡
        """
        valid_actions = []
        
        # è®¡ç®—å„åŒºåŸŸå½“å‰å¤§å°
        region_sizes = {}
        for k in range(1, self.K + 1):
            region_sizes[k] = (self.z == k).sum().item()
        
        # è®¡ç®—ç›®æ ‡å¹³è¡¡å¤§å°
        target_size = self.N // self.K
        max_size = target_size * 1.5  # å…è®¸æœ€å¤§ä¸ºç›®æ ‡å¤§å°çš„1.5å€
        
        for node in self.boundary_nodes:
            node_idx = node.item()
            
            # è·å–è¯¥èŠ‚ç‚¹å¯ä»¥åŠ å…¥çš„åŒºåŸŸ
            valid_regions = set()
            
            if node_idx in self.adj_list:
                for neighbor in self.adj_list[node_idx]:
                    if self.z[neighbor] > 0:
                        region = self.z[neighbor].item()
                        # æ£€æŸ¥åŒºåŸŸå¤§å°é™åˆ¶
                        if region_sizes[region] < max_size:
                            valid_regions.add(region)
            
            # å¦‚æœæ²¡æœ‰é‚»æ¥åŒºåŸŸå¯åŠ å…¥ï¼Œå…è®¸åŠ å…¥æœ€å°çš„åŒºåŸŸ
            if not valid_regions:
                min_region = min(region_sizes.keys(), key=lambda k: region_sizes[k])
                if region_sizes[min_region] < max_size:
                    valid_regions.add(min_region)
            
            # æ£€æŸ¥æ¯ä¸ªå¯èƒ½çš„åˆ†é…
            for region in valid_regions:
                # ç‰©ç†çº¦æŸæ£€æŸ¥
                if self.enable_physics_constraints:
                    if self._check_constraints(node_idx, region):
                        valid_actions.append((node_idx, region))
                else:
                    valid_actions.append((node_idx, region))
        
        return valid_actions
    
    def _check_constraints(self, node: int, region: int) -> bool:
        """
        æ£€æŸ¥ç‰©ç†çº¦æŸ
        
        åŒ…æ‹¬:
        1. åŠŸç‡å¹³è¡¡çº¦æŸ
        2. åŒºåŸŸå¤§å°çº¦æŸ
        3. è¿é€šæ€§çº¦æŸ
        """
        # 1. æ£€æŸ¥åŒºåŸŸå¤§å°ï¼ˆé¿å…æç«¯ä¸å¹³è¡¡ï¼‰
        current_size = (self.z == region).sum().item()
        max_size = self.N // self.K * 2  # æœ€å¤§ä¸ºå¹³å‡å¤§å°çš„2å€
        if current_size >= max_size:
            return False
        
        # 2. æ£€æŸ¥åŠŸç‡å¹³è¡¡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        # è®¡ç®—åŠ å…¥èŠ‚ç‚¹åçš„åŠŸç‡ä¸å¹³è¡¡åº¦
        mask = (self.z == region)
        future_mask = mask.clone()
        future_mask[node] = True
        
        future_p_gen = self.Pg[future_mask].sum()
        future_p_load = self.Pd[future_mask].sum()
        
        # å…è®¸ä¸€å®šçš„ä¸å¹³è¡¡ï¼ˆè€ƒè™‘åŒºåŸŸé—´ä¼ è¾“ï¼‰
        imbalance_ratio = abs(future_p_gen - future_p_load) / (future_p_load + 1e-10)
        if imbalance_ratio > 0.5:  # ä¸å¹³è¡¡åº¦è¶…è¿‡50%
            return False
        
        # 3. å…¶ä»–çº¦æŸå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
        
        return True
    
    def step(self, action: Tuple[int, int]) -> Tuple[Dict, float, bool, Dict]:
        """
        æ‰§è¡ŒåŠ¨ä½œ
        
        å‚æ•°:
            action: (èŠ‚ç‚¹ç´¢å¼•, åŒºåŸŸç¼–å·)
            
        è¿”å›:
            next_state: ä¸‹ä¸€çŠ¶æ€
            reward: å³æ—¶å¥–åŠ±
            done: æ˜¯å¦ç»“æŸ
            info: é¢å¤–ä¿¡æ¯
        """
        node_idx, region = action
        
        # è®°å½•åŠ¨ä½œå‰çš„æŒ‡æ ‡
        prev_metrics = copy.deepcopy(self.current_metrics)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        self.z[node_idx] = region
        self.t += 1
        
        # æ›´æ–°çŠ¶æ€
        self._update_state()
        
        # è®¡ç®—å¥–åŠ±
        reward = self._compute_reward(action, prev_metrics)
        
        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
        done = (self.z == 0).sum() == 0
        
        # è®°å½•å†å²
        self.action_history.append(action)
        self.reward_history.append(reward)
        self.metrics_history.append(self.current_metrics)
        
        # é¢å¤–ä¿¡æ¯
        info = {
            'metrics': self.current_metrics,
            'prev_metrics': prev_metrics,
            'num_assigned': (self.z > 0).sum().item(),
            'num_boundary': len(self.boundary_nodes)
        }
        
        # å¦‚æœå®Œæˆï¼Œè®¡ç®—æœ€ç»ˆå¥–åŠ±
        if done:
            final_bonus = self._compute_final_bonus()
            reward += final_bonus
            info['final_bonus'] = final_bonus
        
        return self.get_state(), reward, done, info
    
    def _compute_reward(self, action: Tuple[int, int], prev_metrics: PartitionMetrics) -> float:
        """
        è®¡ç®—å³æ—¶å¥–åŠ± - ç»“åˆç¨ å¯†å¥–åŠ±å’Œç¡¬çº¦æŸ
        """
        node_idx, region = action
        curr_metrics = self.current_metrics
        
        rewards = {}
        
        # 1. è´Ÿè·å‡è¡¡æ”¹è¿›å¥–åŠ±
        cv_improvement = prev_metrics.load_cv - curr_metrics.load_cv
        rewards['balance'] = self._normalize_reward('cv', cv_improvement)
        
        # 2. è€¦åˆåº¦å¥–åŠ±
        coupling_change = curr_metrics.total_coupling - prev_metrics.total_coupling
        rewards['coupling'] = -self._normalize_reward('coupling', coupling_change)
        
        # 3. è¿é€šæ€§å¥–åŠ±
        # è¿™é‡Œçš„å¥–åŠ±æ˜¯è½¯å¼•å¯¼ï¼ŒçœŸæ­£çš„çº¦æŸåœ¨å‡½æ•°æœ«å°¾çš„ç¡¬æƒ©ç½š
        rewards['connectivity'] = curr_metrics.connectivity
        
        # 4. é‚»å±…ä¸€è‡´æ€§å¥–åŠ±
        rewards['neighbor_consistency'] = self._compute_neighbor_consistency_reward(node_idx, region)
        
        # 5. åŒºåŸŸå¤§å°å¹³è¡¡å¥–åŠ±
        rewards['size_balance'] = self._compute_size_balance_reward(region)
        
        # 6. å…³é”®èŠ‚ç‚¹å¥–åŠ±
        rewards['node_importance'] = self._compute_node_importance_reward(node_idx)

        # åŠ æƒæ±‚å’Œ
        total_reward = sum(
            self.reward_weights.get(key, 0) * value 
            for key, value in rewards.items()
        )
        
        # å¼ºåˆ¶è¿é€šæ€§ç¡¬çº¦æŸ
        if curr_metrics.connectivity < 1.0:
            # å½“å‡ºç°ä¸è¿é€šåŒºåŸŸæ—¶ï¼Œæ–½åŠ ä¸€ä¸ªå·¨å¤§çš„æƒ©ç½š
            # æƒ©ç½šåŠ›åº¦ä¸ä¸è¿é€šåŒºåŸŸçš„æ•°é‡æˆæ­£æ¯”
            num_disconnected_regions = self.K * (1.0 - curr_metrics.connectivity)
            total_reward -= 1.0 * num_disconnected_regions  # æ ¸å¿ƒç¡¬çº¦æŸ

        # è®°å½•å„åˆ†é‡ç”¨äºè°ƒè¯•
        self.reward_components = rewards

        return total_reward
    
    def _compute_neighbor_consistency_reward(self, node_idx: int, region: int) -> float:
        """è®¡ç®—é‚»å±…ä¸€è‡´æ€§å¥–åŠ±"""
        if node_idx not in self.adj_list:
            return 0.0
        
        neighbors = self.adj_list[node_idx]
        same_region_count = 0
        assigned_neighbors = 0
        
        for neighbor in neighbors:
            if self.z[neighbor] > 0:
                assigned_neighbors += 1
                if self.z[neighbor] == region:
                    same_region_count += 1
        
        if assigned_neighbors == 0:
            return 0.0 # æ²¡æœ‰å·²åˆ†é…çš„é‚»å±…ï¼Œæ— æ‰€è°“ä¸€è‡´æ€§
        
        consistency_ratio = same_region_count / assigned_neighbors
        # å°†æ¯”ä¾‹ä»[0, 1]æ˜ å°„åˆ°[-1, 1]ï¼Œé¼“åŠ±é«˜ä¸€è‡´æ€§ï¼Œæƒ©ç½šä¸ä¸€è‡´
        return consistency_ratio * 2 - 1

    def _compute_size_balance_reward(self, region: int) -> float:
        """è®¡ç®—åŒºåŸŸå¤§å°å¹³è¡¡å¥–åŠ±ï¼Œé¿å…åˆ†åŒºå¤§å°å·®å¼‚è¿‡å¤§"""
        target_size = self.N / self.K
        current_size = (self.z == region).sum().item() + 1 # åŠ ä¸Šå³å°†åˆ†é…çš„èŠ‚ç‚¹
        
        # è®¡ç®—ä¸ç†æƒ³å¤§å°çš„åç¦»åº¦
        deviation = abs(current_size - target_size) / target_size
        
        # ä½¿ç”¨æŒ‡æ•°è¡°å‡å‡½æ•°ï¼Œåç¦»è¶Šå°å¥–åŠ±è¶Šé«˜
        return np.exp(-deviation * 3) # ä¹˜3ä½¿æƒ©ç½šæ›´æ•æ„Ÿ

    def _compute_node_importance_reward(self, node_idx: int) -> float:
        """è®¡ç®—åˆ†é…å…³é”®èŠ‚ç‚¹çš„å¥–åŠ±"""
        # åŸºäºè´Ÿè·å’Œåº¦æ•°çš„ç»¼åˆé‡è¦æ€§
        node_load = self.Pd[node_idx].item()
        node_degree = len(self.adj_list.get(node_idx, []))
        
        # ä½¿ç”¨ç¯å¢ƒçš„å¹³å‡å€¼è¿›è¡Œæ ‡å‡†åŒ–
        avg_load = self.avg_load
        avg_degree = self.avg_degree
        
        importance = (node_load / (avg_load + 1e-10)) * 0.7 + \
                     (node_degree / (avg_degree + 1e-10)) * 0.3
        
        # ä½¿ç”¨tanhå°†å¥–åŠ±ä¸­å¿ƒåŒ–å¹¶é™åˆ¶åœ¨[-1, 1]
        return np.tanh(importance - 1.0)

    def _normalize_reward(self, key: str, value: float) -> float:
        """
        åœ¨çº¿æ ‡å‡†åŒ–å¥–åŠ±åˆ†é‡
        
        ä½¿ç”¨running statisticsé¿å…å¥–åŠ±scaleé—®é¢˜
        """
        stats = self.reward_stats[key]
        
        # æ›´æ–°ç»Ÿè®¡é‡ï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰
        alpha = 0.01
        stats['mean'] = (1 - alpha) * stats['mean'] + alpha * value
        stats['std'] = (1 - alpha) * stats['std'] + alpha * abs(value - stats['mean'])
        stats['count'] += 1
        
        # æ ‡å‡†åŒ–
        if stats['count'] > 10 and stats['std'] > 1e-6:
            normalized = (value - stats['mean']) / stats['std']
        else:
            normalized = value
        
        # ä½¿ç”¨tanhé™åˆ¶èŒƒå›´
        return np.tanh(normalized)
    
    def _compute_final_bonus(self) -> float:
        """
        è®¡ç®—å®Œæˆåˆ†åŒºåçš„æœ€ç»ˆå¥–åŠ±
        
        åŸºäºæ•´ä½“åˆ†åŒºè´¨é‡ç»™äºˆé¢å¤–å¥–åŠ±
        """
        metrics = self.current_metrics
        
        # ç»¼åˆè¯„åˆ†
        quality_score = 0.0
        
        # è´Ÿè·å‡è¡¡åº¦ï¼ˆCVè¶Šå°è¶Šå¥½ï¼‰
        if metrics.load_cv < 0.1:
            quality_score += 1.0
        elif metrics.load_cv < 0.2:
            quality_score += 0.5
        
        # è€¦åˆåº¦ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        avg_coupling = metrics.total_coupling / max(metrics.inter_region_lines, 1)
        if avg_coupling < 0.5:
            quality_score += 0.5
        
        # è¿é€šæ€§ï¼ˆå¿…é¡»å…¨éƒ¨è¿é€šï¼‰
        if metrics.connectivity == 1.0:
            quality_score += 0.5
        
        # åŠŸç‡å¹³è¡¡
        if metrics.power_balance < 0.1:
            quality_score += 0.5
        
        return quality_score
    
    def _compute_metrics(self) -> PartitionMetrics:
        """
        è®¡ç®—å½“å‰åˆ†åŒºçš„æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ - å‘é‡åŒ–ç‰ˆæœ¬
        """
        # GPUå‘é‡åŒ–è®¡ç®—è´Ÿè·åˆ†å¸ƒ
        region_ids = self.z - 1  # è½¬æ¢ä¸º0-based
        valid_mask = region_ids >= 0
        
        if valid_mask.any():
            # ä½¿ç”¨scatter_addè¿›è¡Œå‘é‡åŒ–èšåˆ
            region_loads = scatter_add(self.Pd[valid_mask], region_ids[valid_mask],
                                     dim=0, dim_size=self.K)
            region_gens = scatter_add(self.Pg[valid_mask], region_ids[valid_mask],
                                    dim=0, dim_size=self.K)
            
            # è½¬æ¢ä¸ºnumpyè¿›è¡Œç»Ÿè®¡è®¡ç®—
            region_loads_np = region_loads.cpu().numpy()
            region_gens_np = region_gens.cpu().numpy()
            
            # è´Ÿè·å‡è¡¡æŒ‡æ ‡
            if region_loads_np.sum() > 0:
                load_cv = np.std(region_loads_np) / (np.mean(region_loads_np) + 1e-10)
                load_gini = self._compute_gini(region_loads_np)
            else:
                load_cv = 1.0
                load_gini = 1.0
        else:
            load_cv = 1.0
            load_gini = 1.0
            region_loads_np = np.array([])
            region_gens_np = np.array([])
        
        # å‘é‡åŒ–è®¡ç®—è€¦åˆåº¦
        edge_array = self.edge_index
        z_u = self.z[edge_array[0]]
        z_v = self.z[edge_array[1]]
        inter_mask = (z_u != z_v) & (z_u > 0) & (z_v > 0)
        
        inter_region_lines = int(inter_mask.sum().item())
        if inter_region_lines > 0:
            total_coupling = self.admittance[inter_mask].sum().item()
        else:
            total_coupling = 0.0
        
        # è¿é€šæ€§æŒ‡æ ‡
        connectivity = self._check_connectivity()
        
        # åŠŸç‡å¹³è¡¡æŒ‡æ ‡
        power_balance = 0.0
        if len(region_loads_np) > 0:
            for i, (load, gen) in enumerate(zip(region_loads_np, region_gens_np)):
                if load > 0:
                    imbalance = abs(gen - load) / load
                    power_balance += imbalance
            power_balance /= self.K
        
        # åˆ†åŒºæ•ˆç‡
        assigned_ratio = (self.z > 0).float().mean().item()
        efficiency = assigned_ratio
        
        # æ¨¡å—åº¦
        modularity = self._compute_modularity_vectorized()
        
        return PartitionMetrics(
            load_cv=load_cv,
            load_gini=load_gini,
            total_coupling=total_coupling,
            inter_region_lines=inter_region_lines,
            connectivity=connectivity,
            power_balance=power_balance,
            efficiency=efficiency,
            modularity=modularity
        )
    
    def _compute_gini(self, values: np.ndarray) -> float:
        """è®¡ç®—åŸºå°¼ç³»æ•°"""
        if len(values) == 0:
            return 0.0
        
        sorted_values = np.sort(values)
        n = len(values)
        index = np.arange(1, n + 1)
        
        return (2 * index @ sorted_values) / (n * sorted_values.sum()) - (n + 1) / n
    
    def _check_connectivity(self) -> float:
        """
        æ£€æŸ¥å„åŒºåŸŸçš„è¿é€šæ€§
        
        è¿”å›è¿é€šåŒºåŸŸçš„æ¯”ä¾‹
        """
        connected_regions = 0
        total_regions = 0
        
        for k in range(1, self.K + 1):
            mask = (self.z == k)
            if mask.sum() > 0:
                total_regions += 1
                
                # ä½¿ç”¨BFSæ£€æŸ¥è¿é€šæ€§
                region_nodes = torch.where(mask)[0].cpu().tolist()
                if self._is_connected_subgraph(region_nodes):
                    connected_regions += 1
        
        return connected_regions / max(total_regions, 1)
    
    def _is_connected_subgraph(self, nodes: List[int]) -> bool:
        """æ£€æŸ¥å­å›¾æ˜¯å¦è¿é€š"""
        if len(nodes) <= 1:
            return True
        
        # BFS
        visited = {nodes[0]}
        queue = deque([nodes[0]])
        node_set = set(nodes)
        
        while queue:
            u = queue.popleft()
            
            if u in self.adj_list:
                for v in self.adj_list[u]:
                    if v in node_set and v not in visited:
                        visited.add(v)
                        queue.append(v)
        
        return len(visited) == len(nodes)
    
    def _compute_modularity_vectorized(self) -> float:
        """
        å‘é‡åŒ–è®¡ç®—ç½‘ç»œæ¨¡å—åº¦
        
        Q = 1/(2m) * Î£(A_ij - k_i*k_j/(2m)) * Î´(c_i, c_j)
        """
        # ç®€åŒ–è®¡ç®—ï¼šåŸºäºè¾¹çš„æ¯”ä¾‹
        total_edges = self.edge_index.shape[1] // 2

        # å‘é‡åŒ–è®¡ç®—å†…éƒ¨è¾¹
        edge_array = self.edge_index
        z_u = self.z[edge_array[0]]
        z_v = self.z[edge_array[1]]
        internal_mask = (z_u > 0) & (z_u == z_v)
        internal_edges = internal_mask[::2].sum().item()  # åªè®¡ç®—ä¸€åŠé¿å…é‡å¤
        
        # æœŸæœ›çš„å†…éƒ¨è¾¹æ•°
        expected_internal = 0.0
        for k in range(1, self.K + 1):
            region_size = (self.z == k).sum().item()
            if region_size > 0:
                expected_internal += (region_size / self.N) ** 2
        expected_internal *= total_edges
        
        modularity = (internal_edges - expected_internal) / max(total_edges, 1)
        
        return modularity


# Test function can be called from main if needed
def initialize_partition_env(data, embeddings, device):
    """Test function for partition environment"""
    print("\nğŸ® æµ‹è¯•å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ...")

    # åˆ›å»ºç¯å¢ƒï¼ˆK=None å°†è‡ªåŠ¨ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰
    env = PowerGridPartitionEnv(
        data=data,
        embeddings=embeddings,
        K=None,  # è‡ªåŠ¨ä½¿ç”¨config.pyä¸­çš„NUM_REGIONS
        device=device
    )

    # é‡ç½®ç¯å¢ƒ
    state = env.reset()
    print(f"âœ… ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸï¼")
    print(f"ğŸ“Š åˆå§‹è¾¹ç•ŒèŠ‚ç‚¹æ•°: {len(state['boundary_nodes'])}")
    print(f"ğŸ“Š æœ‰æ•ˆåŠ¨ä½œæ•°: {len(env.get_valid_actions())}")

    # æµ‹è¯•ä¸€æ­¥
    valid_actions = env.get_valid_actions()
    if valid_actions:
        action = valid_actions[0]
        next_state, reward, done, info = env.step(action)
        print(f"\nğŸ¯ æ‰§è¡ŒåŠ¨ä½œ: èŠ‚ç‚¹{action[0]} â†’ åŒºåŸŸ{action[1]}")
        print(f"ğŸ’° è·å¾—å¥–åŠ±: {reward:.4f}")
        print(f"ğŸ“ˆ å½“å‰æŒ‡æ ‡: CV={info['metrics'].load_cv:.3f}, è€¦åˆåº¦={info['metrics'].total_coupling:.3f}")
    
    return env


