import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical
import numpy as np
from collections import deque
from typing import Dict, List, Tuple, Optional, Any
import os
from collections import defaultdict

# Import types that will be defined in other modules
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from metrics import PowerGridPartitionEnv

class PPOMemory:
    """
    PPOç»éªŒå­˜å‚¨å™¨
    
    ç‰¹ç‚¹ï¼š
    1. é«˜æ•ˆçš„æ‰¹é‡é‡‡æ ·
    2. æ”¯æŒGAEè®¡ç®—
    3. å†…å­˜å‹å¥½çš„å®ç°
    """
    
    def __init__(self):
        self.clear()
    
    def store(self, state: Dict, action: Tuple[int, int], action_idx: int,
             log_prob: torch.Tensor, value: torch.Tensor, reward: float,
             done: bool, valid_actions: List[Tuple[int, int]]):
        """å­˜å‚¨ä¸€æ­¥ç»éªŒ"""
        # åªå­˜å‚¨å¿…è¦çš„çŠ¶æ€ä¿¡æ¯ï¼ˆé¿å…å­˜å‚¨å¤§çš„åµŒå…¥çŸ©é˜µï¼‰
        stored_state = {
            'z': state['z'].clone(),
            'boundary_nodes': state['boundary_nodes'].clone(),
            'region_embeddings': state['region_embeddings'].clone(),
            'global_context': state['global_context'].clone(),
            't': state['t']
        }
        
        self.states.append(stored_state)
        self.actions.append(action)
        self.action_indices.append(action_idx)
        self.log_probs.append(log_prob.detach())
        self.values.append(value.detach())
        self.rewards.append(torch.tensor(reward, dtype=torch.float32))
        self.dones.append(torch.tensor(done, dtype=torch.float32))
        self.valid_actions_list.append(valid_actions.copy())
    
    def get_batch(self, batch_size: int) -> Dict:
        """è·å–éšæœºæ‰¹æ¬¡ç”¨äºè®­ç»ƒ"""
        n = len(self.rewards)
        indices = np.random.permutation(n)
        
        for start in range(0, n, batch_size):
            end = min(start + batch_size, n)
            batch_indices = indices[start:end]
            
            batch = {
                'states': [self.states[i] for i in batch_indices],
                'actions': [self.actions[i] for i in batch_indices],
                'action_indices': [self.action_indices[i] for i in batch_indices],
                'log_probs': torch.stack([self.log_probs[i] for i in batch_indices]),
                'values': torch.stack([self.values[i] for i in batch_indices]),
                'rewards': torch.stack([self.rewards[i] for i in batch_indices]),
                'dones': torch.stack([self.dones[i] for i in batch_indices]),
                'valid_actions': [self.valid_actions_list[i] for i in batch_indices],
                'advantages': torch.stack([self.advantages[i] for i in batch_indices]),
                'returns': torch.stack([self.returns[i] for i in batch_indices])
            }
            
            yield batch
    
    def compute_gae(self, gamma: float = 0.99, lam: float = 0.95, 
                   next_value: torch.Tensor = None):
        """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡(GAE)å’Œå›æŠ¥"""
        values = torch.stack(self.values)
        rewards = torch.stack(self.rewards)
        dones = torch.stack(self.dones)
        
        # è®¡ç®—GAE
        advantages = torch.zeros_like(rewards)
        last_gae = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value_t = next_value if next_value is not None else 0
            else:
                next_value_t = values[t + 1]
            
            delta = rewards[t] + gamma * next_value_t * (1 - dones[t]) - values[t]
            advantages[t] = last_gae = delta + gamma * lam * (1 - dones[t]) * last_gae
        
        # è®¡ç®—å›æŠ¥
        returns = advantages + values
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿ï¼ˆæé«˜è®­ç»ƒç¨³å®šæ€§ï¼‰
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        self.advantages = list(advantages)
        self.returns = list(returns)
    
    def clear(self):
        """æ¸…ç©ºå­˜å‚¨å™¨"""
        self.states = []
        self.actions = []
        self.action_indices = []
        self.log_probs = []
        self.values = []
        self.rewards = []
        self.dones = []
        self.valid_actions_list = []
        self.advantages = []
        self.returns = []
    
    def __len__(self):
        return len(self.rewards)


class HierarchicalActor(nn.Module):
    """
    åˆ†å±‚Actorç½‘ç»œ
    
    ä¸¤é˜¶æ®µå†³ç­–ï¼š
    1. é€‰æ‹©è¦åˆ†é…çš„èŠ‚ç‚¹
    2. é€‰æ‹©ç›®æ ‡åŒºåŸŸ
    
    è¿™ç§è®¾è®¡å¯ä»¥å¤§å¹…å‡å°‘åŠ¨ä½œç©ºé—´
    """
    
    def __init__(self, node_dim: int, region_dim: int, context_dim: int,
                 hidden_dim: int = 256, num_layers: int = 3, K: int = 3):
        super().__init__()
        
        self.K = K
        
        # å…±äº«ç‰¹å¾æå–å™¨
        self.shared_net = self._build_mlp(
            node_dim + region_dim * K + context_dim,
            hidden_dim,
            hidden_dim,
            num_layers - 1
        )
        
        # åŠ¨ä½œè¯„åˆ†å¤´
        self.action_head = nn.Linear(hidden_dim, 1)
        
        # æ­£äº¤åˆå§‹åŒ–
        for layer in self.modules():
            if isinstance(layer, nn.Linear):
                nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))
                nn.init.constant_(layer.bias, 0)
        
        # è¾“å‡ºå±‚ä½¿ç”¨è¾ƒå°çš„åˆå§‹åŒ–
        nn.init.orthogonal_(self.action_head.weight, gain=0.01)
    
    def _build_mlp(self, input_dim: int, hidden_dim: int, output_dim: int,
                   num_layers: int) -> nn.Module:
        """æ„å»ºå¤šå±‚æ„ŸçŸ¥æœº"""
        layers = []
        
        for i in range(num_layers):
            if i == 0:
                layers.extend([
                    nn.Linear(input_dim, hidden_dim),
                    nn.LayerNorm(hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(0.1)
                ])
            else:
                layers.extend([
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.LayerNorm(hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(0.1)
                ])
        
        return nn.Sequential(*layers)
    
    def forward(self, node_embeddings: torch.Tensor, state: Dict,
               valid_actions: List[Tuple[int, int]]) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        è¿”å›:
            action_probs: åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
            action_logits: åŸå§‹åˆ†æ•°ï¼ˆç”¨äºè®¡ç®—ç†µï¼‰
        """
        if len(valid_actions) == 0:
            return torch.empty(0), torch.empty(0)
        
        # æ‰¹é‡è®¡ç®—æ‰€æœ‰æœ‰æ•ˆåŠ¨ä½œçš„ç‰¹å¾
        action_features = []
        
        for node_idx, region in valid_actions:
            # æ„é€ åŠ¨ä½œç‰¹å¾
            node_emb = node_embeddings[node_idx]
            region_emb = state['region_embeddings'][region - 1]
            global_ctx = state['global_context']
            
            # æ·»åŠ é¢å¤–çš„å…³ç³»ç‰¹å¾
            all_region_embs = state['region_embeddings'].flatten()
            
            # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
            features = torch.cat([
                node_emb,
                all_region_embs,
                global_ctx
            ])
            
            action_features.append(features)
        
        # æ‰¹é‡å‰å‘ä¼ æ’­
        action_features = torch.stack(action_features)
        hidden = self.shared_net(action_features)
        action_logits = self.action_head(hidden).squeeze(-1)
        
        # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        action_probs = F.softmax(action_logits, dim=0)
        
        return action_probs, action_logits


class Critic(nn.Module):
    """
    ä»·å€¼ç½‘ç»œ
    
    ä¼°è®¡çŠ¶æ€ä»·å€¼å‡½æ•°V(s)
    """
    
    def __init__(self, region_dim: int, context_dim: int, hidden_dim: int = 256,
                 num_layers: int = 3, K: int = 3):
        super().__init__()
        
        # è¾“å…¥ç»´åº¦ï¼šåŒºåŸŸåµŒå…¥ + å…¨å±€ä¸Šä¸‹æ–‡ + ç»Ÿè®¡ä¿¡æ¯
        input_dim = region_dim * K + context_dim + K * 3  # æ¯ä¸ªåŒºåŸŸ3ä¸ªç»Ÿè®¡é‡
        
        # ä»·å€¼ç½‘ç»œ
        self.value_net = self._build_mlp(input_dim, hidden_dim, 1, num_layers)
        
        # æ­£äº¤åˆå§‹åŒ–
        for layer in self.modules():
            if isinstance(layer, nn.Linear):
                nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))
                nn.init.constant_(layer.bias, 0)
    
    def _build_mlp(self, input_dim: int, hidden_dim: int, output_dim: int,
                   num_layers: int) -> nn.Module:
        """æ„å»ºå¤šå±‚æ„ŸçŸ¥æœº"""
        layers = []
        
        for i in range(num_layers):
            if i == 0:
                layers.append(nn.Linear(input_dim, hidden_dim))
            elif i == num_layers - 1:
                layers.append(nn.Linear(hidden_dim, output_dim))
            else:
                layers.append(nn.Linear(hidden_dim, hidden_dim))
            
            if i < num_layers - 1:
                layers.extend([
                    nn.LayerNorm(hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(0.1)
                ])
        
        return nn.Sequential(*layers)
    
    def forward(self, state: Dict, env: 'PowerGridPartitionEnv') -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            state: çŠ¶æ€å­—å…¸
            env: ç¯å¢ƒå®ä¾‹ï¼ˆç”¨äºè·å–é¢å¤–ç»Ÿè®¡ä¿¡æ¯ï¼‰
        """
        # æå–ç‰¹å¾
        region_embs = state['region_embeddings'].flatten()
        global_ctx = state['global_context']
        
        # è®¡ç®—åŒºåŸŸç»Ÿè®¡ä¿¡æ¯
        region_stats = []
        for k in range(1, env.K + 1):
            mask = (state['z'] == k)
            if mask.any():
                # èŠ‚ç‚¹æ•°æ¯”ä¾‹
                size_ratio = mask.float().mean()
                # è´Ÿè·æ¯”ä¾‹
                load_ratio = env.Pd[mask].sum() / (env.Pd.sum() + 1e-10)
                # å‘ç”µæ¯”ä¾‹
                gen_ratio = env.Pg[mask].sum() / (env.Pg.sum() + 1e-10)
                
                region_stats.extend([size_ratio, load_ratio, gen_ratio])
            else:
                region_stats.extend([0.0, 0.0, 0.0])
        
        region_stats = torch.tensor(region_stats, device=state['z'].device)
        
        # ç»„åˆç‰¹å¾
        features = torch.cat([region_embs, global_ctx, region_stats])
        
        # è®¡ç®—ä»·å€¼
        value = self.value_net(features)
        
        return value


class PPOAgent:
    """
    PPOæ™ºèƒ½ä½“ï¼ˆå®Œæ•´å®ç°ï¼‰
    
    ç‰¹ç‚¹ï¼š
    1. Clipped surrogate objective
    2. å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡(GAE)
    3. å¤šepoch mini-batchæ›´æ–°
    4. è‡ªé€‚åº”KLæƒ©ç½š
    5. ç†µæ­£åˆ™åŒ–
    6. æ¢¯åº¦è£å‰ª
    """
    
    def __init__(self, actor: nn.Module, critic: nn.Module, env: 'PowerGridPartitionEnv',
                 lr_actor: float = 3e-4, lr_critic: float = 1e-3,
                 gamma: float = 0.99, lam: float = 0.95, eps_clip: float = 0.2,
                 value_coef: float = 0.5, entropy_coef: float = 0.01,
                 max_grad_norm: float = 0.5, target_kl: float = 0.01,
                 device: str = 'cpu'):
        
        self.device = torch.device(device)
        self.env = env
        
        # ç½‘ç»œ
        self.actor = actor.to(self.device)
        self.critic = critic.to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=lr_critic)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler_actor = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer_actor, mode='max', factor=0.5, patience=10
        )
        self.scheduler_critic = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer_critic, mode='max', factor=0.5, patience=10
        )
        
        # è¶…å‚æ•°
        self.gamma = gamma
        self.lam = lam
        self.eps_clip = eps_clip
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef
        self.max_grad_norm = max_grad_norm
        self.target_kl = target_kl
        
        # ç»éªŒå­˜å‚¨
        self.memory = PPOMemory()
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = defaultdict(list)
    
    def select_action(self, state: Dict, valid_actions: List[Tuple[int, int]],
                     training: bool = True) -> Optional[Tuple[Tuple[int, int], float]]:
        """
        é€‰æ‹©åŠ¨ä½œ
        
        è¿”å›:
            (action, value): åŠ¨ä½œå’ŒçŠ¶æ€ä»·å€¼
        """
        if len(valid_actions) == 0:
            return None
        
        # è·å–åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
        with torch.set_grad_enabled(training):
            action_probs, action_logits = self.actor(
                self.env.embeddings, state, valid_actions
            )
            
            # è®¡ç®—çŠ¶æ€ä»·å€¼
            value = self.critic(state, self.env)
        
        if training:
            # è®­ç»ƒæ—¶ï¼šä»æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·
            dist = Categorical(action_probs)
            action_idx = dist.sample()
            log_prob = dist.log_prob(action_idx)
            
            # å­˜å‚¨åˆ°ç»éªŒæ± 
            selected_action = valid_actions[action_idx]
            self.memory.store(
                state, selected_action, action_idx.item(),
                log_prob, value.squeeze(), 0, False, valid_actions
            )
        else:
            # è¯„ä¼°æ—¶ï¼šé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„åŠ¨ä½œ
            action_idx = torch.argmax(action_probs)
            selected_action = valid_actions[action_idx]
        
        return selected_action, value.item()
    
    def update_last_reward(self, reward: float, done: bool):
        """æ›´æ–°æœ€åä¸€æ­¥çš„å¥–åŠ±å’Œå®Œæˆæ ‡å¿—"""
        if len(self.memory) > 0:
            self.memory.rewards[-1] = torch.tensor(reward, dtype=torch.float32)
            self.memory.dones[-1] = torch.tensor(done, dtype=torch.float32)
    
    def update(self, epochs: int = 4, batch_size: int = 64) -> Dict[str, float]:
        """
        PPOæ›´æ–°
        
        å‚æ•°:
            epochs: æ›´æ–°è½®æ•°
            batch_size: æ‰¹é‡å¤§å°
            
        è¿”å›:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if len(self.memory) == 0:
            return {}
        
        # è®¡ç®—GAEå’Œå›æŠ¥
        with torch.no_grad():
            last_state = self.env.get_state()
            last_value = self.critic(last_state, self.env).squeeze()
        
        self.memory.compute_gae(self.gamma, self.lam, last_value)
        
        # è®­ç»ƒç»Ÿè®¡
        epoch_stats = defaultdict(list)
        
        # å¤šè½®æ›´æ–°
        for epoch in range(epochs):
            kl_divs = []
            
            # Mini-batchè®­ç»ƒ
            for batch in self.memory.get_batch(batch_size):
                # å‡†å¤‡æ‰¹é‡æ•°æ®
                old_log_probs = batch['log_probs'].to(self.device)
                advantages = batch['advantages'].to(self.device)
                returns = batch['returns'].to(self.device)
                
                # é‡æ–°è®¡ç®—åŠ¨ä½œæ¦‚ç‡ï¼ˆç”¨äºæ¯”ç‡è®¡ç®—ï¼‰
                new_log_probs = []
                entropies = []
                
                for i, (state, valid_actions, action_idx) in enumerate(
                    zip(batch['states'], batch['valid_actions'], batch['action_indices'])
                ):
                    # æ¢å¤å®Œæ•´çŠ¶æ€ï¼ˆæ·»åŠ åµŒå…¥ï¼‰
                    full_state = {
                        **state,
                        'node_embeddings': self.env.embeddings
                    }
                    
                    # è®¡ç®—æ–°çš„åŠ¨ä½œæ¦‚ç‡
                    action_probs, _ = self.actor(self.env.embeddings, full_state, valid_actions)
                    
                    if len(action_probs) > 0 and action_idx < len(action_probs):
                        dist = Categorical(action_probs)
                        new_log_probs.append(dist.log_prob(torch.tensor(action_idx)))
                        entropies.append(dist.entropy())
                
                if not new_log_probs:
                    continue
                
                new_log_probs = torch.stack(new_log_probs)
                entropy = torch.stack(entropies).mean()
                
                # è®¡ç®—æ¯”ç‡
                ratio = torch.exp(new_log_probs - old_log_probs[:len(new_log_probs)])
                
                # KLæ•£åº¦ï¼ˆç”¨äºæ—©åœï¼‰
                kl_div = (old_log_probs[:len(new_log_probs)] - new_log_probs).mean()
                kl_divs.append(kl_div.item())
                
                # Clipped surrogate loss
                surr1 = ratio * advantages[:len(new_log_probs)]
                surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages[:len(new_log_probs)]
                policy_loss = -torch.min(surr1, surr2).mean()
                
                # ä»·å€¼æŸå¤± - é‡æ–°è®¡ç®—ä»¥é¿å…è®¡ç®—å›¾é—®é¢˜
                values = []
                for state in batch['states'][:len(new_log_probs)]:
                    full_state = {
                        **state,
                        'node_embeddings': self.env.embeddings.detach()
                    }
                    value = self.critic(full_state, self.env)
                    values.append(value)
                
                values = torch.cat(values)
                value_loss = F.mse_loss(values, returns[:len(values)].detach())
                
                # æ€»æŸå¤±
                loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy
                
                # åå‘ä¼ æ’­
                self.optimizer_actor.zero_grad()
                self.optimizer_critic.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
                nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
                
                self.optimizer_actor.step()
                self.optimizer_critic.step()
                
                # è®°å½•ç»Ÿè®¡ï¼ˆåœ¨åå‘ä¼ æ’­åç«‹å³æå–æ•°å€¼ï¼Œé¿å…è®¡ç®—å›¾é—®é¢˜ï¼‰
                policy_loss_val = policy_loss.item()
                value_loss_val = value_loss.item()
                entropy_val = entropy.item()
                kl_div_val = kl_div.item()
                
                epoch_stats['policy_loss'].append(policy_loss_val)
                epoch_stats['value_loss'].append(value_loss_val)
                epoch_stats['entropy'].append(entropy_val)
                epoch_stats['kl_div'].append(kl_div_val)
                
                # æ¸…ç†è®¡ç®—å›¾ï¼Œé¿å…é‡å¤åå‘ä¼ æ’­
                del loss, policy_loss, value_loss, entropy, ratio, surr1, surr2, new_log_probs, values
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # æ—©åœæ£€æŸ¥
            if kl_divs and np.mean(kl_divs) > self.target_kl:
                print(f"Early stopping at epoch {epoch} due to KL divergence")
                break
        
        # æ¸…ç©ºç»éªŒæ± 
        self.memory.clear()
        
        # æ±‡æ€»ç»Ÿè®¡
        stats = {}
        for key, values in epoch_stats.items():
            if values:
                stats[key] = np.mean(values)
                self.training_stats[key].append(stats[key])
        
        return stats
    
    def state_dict(self) -> Dict:
        """è¿”å›æ™ºèƒ½ä½“çš„çŠ¶æ€å­—å…¸ï¼ˆç”¨äºæ£€æŸ¥ç‚¹ä¿å­˜ï¼‰"""
        return {
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'optimizer_actor_state_dict': self.optimizer_actor.state_dict(),
            'optimizer_critic_state_dict': self.optimizer_critic.state_dict(),
            'training_stats': dict(self.training_stats)
        }
    
    def load_state_dict(self, state_dict: Dict):
        """ä»çŠ¶æ€å­—å…¸åŠ è½½æ™ºèƒ½ä½“çŠ¶æ€"""
        self.actor.load_state_dict(state_dict['actor_state_dict'])
        self.critic.load_state_dict(state_dict['critic_state_dict'])
        self.optimizer_actor.load_state_dict(state_dict['optimizer_actor_state_dict'])
        self.optimizer_critic.load_state_dict(state_dict['optimizer_critic_state_dict'])
        self.training_stats = defaultdict(list, state_dict.get('training_stats', {}))
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'optimizer_actor_state_dict': self.optimizer_actor.state_dict(),
            'optimizer_critic_state_dict': self.optimizer_critic.state_dict(),
            'training_stats': dict(self.training_stats)
        }, path)
        print(f"ğŸ’¾ Model saved to {path}")
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.optimizer_actor.load_state_dict(checkpoint['optimizer_actor_state_dict'])
        self.optimizer_critic.load_state_dict(checkpoint['optimizer_critic_state_dict'])
        self.training_stats = defaultdict(list, checkpoint.get('training_stats', {}))
        print(f"ğŸ“‚ Model loaded from {path}")


def initialize_ppo_agent(embeddings, env, device):
    """Test function for PPO agent initialization"""
    # æµ‹è¯•PPOæ™ºèƒ½ä½“
    print("\nğŸ¤– åˆå§‹åŒ–PPOæ™ºèƒ½ä½“...")

    # åˆ›å»ºç½‘ç»œ
    actor = HierarchicalActor(
        node_dim=embeddings.shape[1],
        region_dim=embeddings.shape[1],
        context_dim=embeddings.shape[1] + 1 + env.K + 1,
        hidden_dim=256,
        K=env.K
    )

    critic = Critic(
        region_dim=embeddings.shape[1],
        context_dim=embeddings.shape[1] + 1 + env.K + 1,
        hidden_dim=256,
        K=env.K
    )

    # åˆ›å»ºPPOæ™ºèƒ½ä½“
    agent = PPOAgent(
        actor=actor,
        critic=critic,
        env=env,
        device=device
    )

    print(f"âœ… PPOæ™ºèƒ½ä½“åˆå§‹åŒ–æˆåŠŸï¼")
    print(f"ğŸ“Š Actorå‚æ•°é‡: {sum(p.numel() for p in actor.parameters()):,}")
    print(f"ğŸ“Š Criticå‚æ•°é‡: {sum(p.numel() for p in critic.parameters()):,}")
    
    return agent

def evaluate_agent(agent: PPOAgent, env: 'PowerGridPartitionEnv', 
                   return_env_state: bool = False) -> Any:
    """
    è¯„ä¼°æ™ºèƒ½ä½“åœ¨ä¸€ä¸ªå®Œæ•´å›åˆä¸­çš„è¡¨ç°
    
    å‚æ•°:
        agent: è¦è¯„ä¼°çš„æ™ºèƒ½ä½“
        env: è¯„ä¼°ç¯å¢ƒ
        return_env_state: æ˜¯å¦è¿”å›æœ€ç»ˆçš„ç¯å¢ƒçŠ¶æ€
    
    è¿”å›:
        æœ€ç»ˆæŒ‡æ ‡ï¼Œå¦‚æœreturn_env_stateä¸ºTrueï¼Œåˆ™é¢å¤–è¿”å›ç¯å¢ƒçŠ¶æ€
    """
    agent.actor.eval()
    agent.critic.eval()
    
    state = env.reset()
    done = False
    
    with torch.no_grad():
        while not done:
            valid_actions = env.get_valid_actions()
            if not valid_actions:
                break
            
            action_result = agent.select_action(state, valid_actions, training=False)
            if action_result is None:
                break
                
            action, _ = action_result
            state, _, done, _ = env.step(action)
    
    # æœ€ç»ˆæŒ‡æ ‡
    final_metrics = env.current_metrics
    
    if return_env_state:
        import copy
        return final_metrics, copy.deepcopy(env)
    
    return final_metrics

