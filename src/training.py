import torch
import numpy as np
import time
import os
from collections import deque
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
from typing import Dict, List, Union, Tuple, Optional

# Import types that will be defined in other modules
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from agent import PPOAgent
    from env import PowerGridPartitionEnv, CurriculumLearningEnv

def train_ppo(agent: 'PPOAgent', env: Union['PowerGridPartitionEnv', 'CurriculumLearningEnv'],
              n_episodes: int = 1000, max_steps: int = 500,
              update_interval: int = 10, save_interval: int = 100,
              use_tensorboard: bool = True, start_episode: int = 0,
              checkpoint_dir: str = 'models') -> Dict[str, List[float]]:
    """
    è®­ç»ƒPPOæ™ºèƒ½ä½“
    
    å‚æ•°:
        agent: PPOæ™ºèƒ½ä½“
        env: è®­ç»ƒç¯å¢ƒ
        n_episodes: è®­ç»ƒå›åˆæ•°
        max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°
        update_interval: æ›´æ–°é—´éš”
        save_interval: ä¿å­˜é—´éš”
        use_tensorboard: æ˜¯å¦ä½¿ç”¨TensorBoard
        
    è¿”å›:
        è®­ç»ƒå†å²
    """
    # TensorBoardè®°å½•å™¨
    if use_tensorboard:
        writer = SummaryWriter(f'runs/power_grid_partition_{time.strftime("%Y%m%d_%H%M%S")}')
    
    # è®­ç»ƒå†å²
    history = {
        'episode_rewards': [],
        'episode_lengths': [],
        'load_cv': [],
        'total_coupling': [],
        'success_rate': []
    }
    
    # æˆåŠŸç‡è¿½è¸ª
    recent_successes = deque(maxlen=100)
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # è¿›åº¦æ¡
    pbar = tqdm(range(start_episode, n_episodes), desc="Training Progress")
    
    for episode in pbar:
        # é‡ç½®ç¯å¢ƒ
        state = env.reset()
        episode_reward = 0
        episode_length = 0
        
        # æ‰§è¡Œä¸€ä¸ªå›åˆ
        for step in range(max_steps):
            # è·å–æœ‰æ•ˆåŠ¨ä½œ
            valid_actions = env.get_valid_actions()
            
            if not valid_actions:
                break
            
            # é€‰æ‹©åŠ¨ä½œ
            action_value = agent.select_action(state, valid_actions, training=True)
            if action_value is None:
                break
            
            action, value = action_value
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(action)
            
            # æ›´æ–°ç»éªŒæ± ä¸­çš„å¥–åŠ±
            agent.update_last_reward(reward, done)
            
            # ç´¯ç§¯å¥–åŠ±
            episode_reward += reward
            episode_length += 1
            
            # æ›´æ–°çŠ¶æ€
            state = next_state
            
            if done:
                break
        
        # è®°å½•å›åˆç»“æœ
        history['episode_rewards'].append(episode_reward)
        history['episode_lengths'].append(episode_length)
        
        # è®°å½•æœ€ç»ˆæŒ‡æ ‡
        final_metrics = env.current_metrics
        history['load_cv'].append(final_metrics.load_cv)
        history['total_coupling'].append(final_metrics.total_coupling)
        
        # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
        success = (final_metrics.load_cv < 0.3 and 
                  final_metrics.connectivity == 1.0 and
                  episode_length == env.N)
        recent_successes.append(success)
        
        # æ›´æ–°æ™ºèƒ½ä½“ï¼ˆæ¯éš”ä¸€å®šå›åˆï¼‰
        if (episode + 1) % update_interval == 0 and len(agent.memory) > 0:
            update_stats = agent.update(epochs=4, batch_size=64)
            
            # æ›´æ–°å­¦ä¹ ç‡
            avg_reward = np.mean(history['episode_rewards'][-update_interval:])
            agent.scheduler_actor.step(avg_reward)
            agent.scheduler_critic.step(avg_reward)
        
        # TensorBoardè®°å½•
        if use_tensorboard:
            writer.add_scalar('Train/Episode_Reward', episode_reward, episode)
            writer.add_scalar('Train/Episode_Length', episode_length, episode)
            writer.add_scalar('Train/Load_CV', final_metrics.load_cv, episode)
            writer.add_scalar('Train/Total_Coupling', final_metrics.total_coupling, episode)
            writer.add_scalar('Train/Success_Rate', np.mean(recent_successes), episode)
            writer.add_scalar('Train/Connectivity', final_metrics.connectivity, episode)
            writer.add_scalar('Train/Power_Balance', final_metrics.power_balance, episode)
            writer.add_scalar('Train/Modularity', final_metrics.modularity, episode)
            
            # å­¦ä¹ ç‡ç›‘æ§
            if hasattr(agent, 'scheduler_actor'):
                writer.add_scalar('Learning_Rate/Actor', agent.scheduler_actor.get_last_lr()[0], episode)
            if hasattr(agent, 'scheduler_critic'):
                writer.add_scalar('Learning_Rate/Critic', agent.scheduler_critic.get_last_lr()[0], episode)
            
            # æŸå¤±å‡½æ•°ç›‘æ§
            if (episode + 1) % update_interval == 0 and len(update_stats) > 0:
                if 'policy_loss' in update_stats:
                    writer.add_scalar('Loss/Policy', update_stats['policy_loss'], episode)
                if 'value_loss' in update_stats:
                    writer.add_scalar('Loss/Value', update_stats['value_loss'], episode)
                if 'entropy' in update_stats:
                    writer.add_scalar('Loss/Entropy', update_stats['entropy'], episode)
                if 'kl_divergence' in update_stats:
                    writer.add_scalar('Loss/KL_Divergence', update_stats['kl_divergence'], episode)
            
            # æ¯100å›åˆè®°å½•åˆ†å¸ƒç»Ÿè®¡
            if (episode + 1) % 100 == 0:
                recent_rewards = history['episode_rewards'][-100:] if len(history['episode_rewards']) >= 100 else history['episode_rewards']
                writer.add_histogram('Stats/Episode_Rewards_Distribution', np.array(recent_rewards), episode)
                writer.add_scalar('Stats/Reward_Std', np.std(recent_rewards), episode)
                writer.add_scalar('Stats/Best_Reward_Last100', np.max(recent_rewards), episode)
        
        # æ›´æ–°è¿›åº¦æ¡
        success_rate = np.mean(recent_successes) if recent_successes else 0
        pbar.set_postfix({
            'Reward': f'{episode_reward:.2f}',
            'CV': f'{final_metrics.load_cv:.3f}',
            'Success': f'{success_rate:.2%}'
        })
        
        # ä¿å­˜æ¨¡å‹å’Œæ£€æŸ¥ç‚¹
        if (episode + 1) % save_interval == 0:
            # ä¿å­˜æ™ºèƒ½ä½“æ¨¡å‹
            model_path = f'{checkpoint_dir}/ppo_checkpoint_ep{episode+1}.pth'
            agent.save(model_path)
            
            # ä¿å­˜å®Œæ•´æ£€æŸ¥ç‚¹ï¼ˆåŒ…å«è®­ç»ƒçŠ¶æ€ï¼‰
            checkpoint_path = f'{checkpoint_dir}/training_checkpoint_ep{episode+1}.pth'
            checkpoint = {
                'episode': episode + 1,
                'agent_state': agent.state_dict(),
                'history': history,
                'recent_successes': list(recent_successes),
                'env_state': getattr(env, 'get_state', lambda: None)(),
                'training_config': {
                    'n_episodes': n_episodes,
                    'max_steps': max_steps,
                    'update_interval': update_interval,
                    'save_interval': save_interval
                }
            }
            torch.save(checkpoint, checkpoint_path)
            print(f"\nğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    # å…³é—­TensorBoard
    if use_tensorboard:
        writer.close()
    
    # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
    final_checkpoint_path = f'{checkpoint_dir}/training_final.pth'
    final_checkpoint = {
        'episode': n_episodes,
        'agent_state': agent.state_dict(),
        'history': history,
        'recent_successes': list(recent_successes),
        'env_state': getattr(env, 'get_state', lambda: None)(),
        'training_completed': True
    }
    torch.save(final_checkpoint, final_checkpoint_path)
    print(f"\nğŸ æœ€ç»ˆæ£€æŸ¥ç‚¹å·²ä¿å­˜: {final_checkpoint_path}")
    
    return history


def load_training_checkpoint(checkpoint_path: str) -> Dict:
    """åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹"""
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    print(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ:")
    print(f"   - å›åˆ: {checkpoint.get('episode', 0)}")
    print(f"   - å†å²é•¿åº¦: {len(checkpoint.get('history', {}).get('episode_rewards', []))}")
    print(f"   - è®­ç»ƒå®Œæˆ: {checkpoint.get('training_completed', False)}")
    
    return checkpoint

def full_training(agent, curriculum_env, resume_from: str = None):
    """å®Œæ•´è®­ç»ƒå‡½æ•° - ç”Ÿäº§ç¯å¢ƒé…ç½®"""
    print("\nğŸš€ å¼€å§‹å®Œæ•´è®­ç»ƒ...")
    print("è¿™æ˜¯å®Œæ•´çš„è®­ç»ƒé…ç½®ï¼Œå°†è¿›è¡Œé•¿æ—¶é—´è®­ç»ƒä»¥è·å¾—æœ€ä½³æ€§èƒ½")
    
    # æ£€æŸ¥æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤
    start_episode = 0
    previous_history = None
    if resume_from and os.path.exists(resume_from):
        print(f"ğŸ“¥ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {resume_from}")
        checkpoint = load_training_checkpoint(resume_from)
        agent.load_state_dict(checkpoint['agent_state'])
        start_episode = checkpoint.get('episode', 0)
        previous_history = checkpoint.get('history', None)
        print(f"ä»ç¬¬ {start_episode} å›åˆç»§ç»­è®­ç»ƒ")
    
    # å®Œæ•´è®­ç»ƒé…ç½®
    history = train_ppo(
        agent=agent,
        env=curriculum_env,
        n_episodes=2000,  # å®Œæ•´è®­ç»ƒå›åˆæ•°
        max_steps=500,    # æ¯å›åˆæœ€å¤§æ­¥æ•°
        update_interval=10,  # æ¯10å›åˆæ›´æ–°ä¸€æ¬¡
        save_interval=50,    # æ¯50å›åˆä¿å­˜ä¸€æ¬¡
        use_tensorboard=True,  # å¯ç”¨è¯¦ç»†æ—¥å¿—
        start_episode=start_episode
    )
    
    # å¦‚æœæœ‰ä¹‹å‰çš„å†å²ï¼Œåˆå¹¶å†å²æ•°æ®
    if previous_history:
        for key in history:
            if key in previous_history:
                history[key] = previous_history[key] + history[key]
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = 'models/ppo_final_model.pth'
    agent.save(final_model_path)
    print(f"\nğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_model_path}")
    
    print("\nâœ… å®Œæ•´è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“Š æ€»è®­ç»ƒå›åˆ: {len(history['episode_rewards'])}")
    print(f"ğŸ“Š å¹³å‡å¥–åŠ±: {np.mean(history['episode_rewards']):.3f}")
    print(f"ğŸ“Š æœ€ä½³å›åˆå¥–åŠ±: {np.max(history['episode_rewards']):.3f}")
    print(f"ğŸ“Š æœ€ç»ˆLoad CV: {history['load_cv'][-1]:.3f}")
    print(f"ğŸ“Š æœ€ç»ˆè€¦åˆåº¦: {history['total_coupling'][-1]:.3f}")
    
    return history

def quick_training(agent, curriculum_env):
    """å¿«é€Ÿè®­ç»ƒå‡½æ•° - æ¼”ç¤ºå’Œæµ‹è¯•ç”¨"""
    print("\nğŸš€ å¼€å§‹å¿«é€Ÿè®­ç»ƒ...")
    print("è¿™æ˜¯å¿«é€Ÿæ¼”ç¤ºé…ç½®ï¼Œé€‚åˆæµ‹è¯•å’Œè°ƒè¯•")

    # å¿«é€Ÿè®­ç»ƒé…ç½®
    history = train_ppo(
        agent=agent,
        env=curriculum_env,
        n_episodes=100,   # é€‚ä¸­çš„å›åˆæ•°
        max_steps=200,    # é€‚ä¸­çš„æ­¥æ•°
        update_interval=10,
        save_interval=25,
        use_tensorboard=True
    )

    print("\nâœ… å¿«é€Ÿè®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“Š å¹³å‡å¥–åŠ±: {np.mean(history['episode_rewards']):.3f}")
    print(f"ğŸ“Š æœ€ç»ˆLoad CV: {history['load_cv'][-1]:.3f}")
    print(f"ğŸ“Š æœ€ç»ˆè€¦åˆåº¦: {history['total_coupling'][-1]:.3f}")
    
    return history

