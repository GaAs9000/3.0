import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATv2Conv, to_hetero, LayerNorm, global_mean_pool
from torch_geometric.data import HeteroData
from torch_geometric.utils import softmax
from typing import Optional, List, Dict, Tuple, Union
import numpy as np
import warnings

# å­˜å‚¨æ•è·åˆ°çš„æ³¨æ„åŠ›æƒé‡çš„å…¨å±€åˆ—è¡¨ï¼ˆæˆ–ç±»å±æ€§ï¼‰
captured_attention_weights = []

def attention_hook(module, input, output):
    """ä¸€ä¸ªforward hookï¼Œç”¨äºæ•è·æ³¨æ„åŠ›æƒé‡"""
    # GATv2Convåœ¨return_attention_weights=Trueæ—¶ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªå…ƒç»„(out, attention_details)
    # attention_details æ˜¯ (edge_index, alpha)
    if isinstance(output, tuple) and len(output) == 2:
        # æˆ‘ä»¬åªå…³å¿ƒ alpha å¼ é‡
        if isinstance(output[1], tuple) and len(output[1]) == 2:
            captured_attention_weights.append(output[1][1])

class PhysicsGATv2Conv(GATv2Conv):
    """
    ç‰©ç†å¼•å¯¼çš„GAT
    
    æ ¸å¿ƒåˆ›æ–°ï¼š
    1. åŸºäºæœ€æ–°çš„GATv2æ¶æ„ï¼Œå…·æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›
    2. åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­èå…¥ç”µæ°”é˜»æŠ—å…ˆéªŒ
    3. é˜»æŠ—è¶Šå°çš„è¿æ¥è·å¾—è¶Šé«˜çš„æ³¨æ„åŠ›æƒé‡
    4. å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°æ§åˆ¶ç‰©ç†å…ˆéªŒçš„å½±å“ç¨‹åº¦
    
    æ•°å­¦åŸç†ï¼š
    æ ‡å‡†GATv2: Î±_ij = softmax(W_a^T LeakyReLU(W_l[W_r h_i || W_r h_j] + b))
    ç‰©ç†GATv2: Î±_ij = softmax(W_a^T LeakyReLU(W_l[W_r h_i || W_r h_j] + b) + Ï„/|Z_ij|)
    """
    
    def __init__(self, in_channels: int, out_channels: int, heads: int = 8,
                 concat: bool = True, dropout: float = 0.6, 
                 edge_dim: Optional[int] = None, temperature: float = 1.0,
                 z_index: int = 3, physics_weight: float = 1.0, **kwargs):
        
        super().__init__(in_channels, out_channels, heads=heads, concat=concat,
                        dropout=dropout, edge_dim=edge_dim, **kwargs)
        
        # ç‰©ç†å¼•å¯¼å‚æ•°
        self.temperature = nn.Parameter(torch.tensor(temperature))
        self.physics_weight = physics_weight
        self.z_index = z_index  # é˜»æŠ—æ¨¡é•¿åœ¨è¾¹ç‰¹å¾ä¸­çš„ç´¢å¼•
        
        # è¾¹ç‰¹å¾å¤„ç†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if edge_dim is not None:
            self.edge_processor = nn.Linear(edge_dim, heads)
        else:
            self.edge_processor = None
    
    def forward(self, x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], 
                edge_index: torch.Tensor,
                edge_attr: Optional[torch.Tensor] = None,
                return_attention_weights: Optional[bool] = None):
        """
        å‰å‘ä¼ æ’­ï¼Œåœ¨æ ‡å‡†GATv2åŸºç¡€ä¸Šèå…¥ç‰©ç†å…ˆéªŒå¹¶ç¼“å­˜æ³¨æ„åŠ›æƒé‡
        """
        # å¤„ç†ç‰©ç†å…ˆéªŒå¢å¼ºçš„è¾¹ç‰¹å¾
        if edge_attr is not None:
            enhanced_edge_attr = self.physics_enhanced_edge_attr(edge_attr)
        else:
            enhanced_edge_attr = edge_attr
        
        # å§‹ç»ˆè¯·æ±‚æ³¨æ„åŠ›æƒé‡
        result = super().forward(
            x, edge_index, enhanced_edge_attr, return_attention_weights=True
        )
        
        out, attention_details = result
        
        # å°†æ³¨æ„åŠ›åˆ†æ•°å­˜å‚¨åœ¨å®ä¾‹å±æ€§ä¸­ï¼Œä»¥ä¾¿åç»­æå–
        self._alpha = attention_details[1]

        # æ ¹æ®è°ƒç”¨è€…çš„è¦æ±‚å†³å®šè¿”å›å€¼
        if return_attention_weights:
            return out, attention_details
        return out
    
    def physics_enhanced_edge_attr(self, edge_attr: torch.Tensor) -> torch.Tensor:
        """
        å¢å¼ºè¾¹ç‰¹å¾ï¼Œèå…¥ç‰©ç†å…ˆéªŒ
        """
        if edge_attr is None or edge_attr.shape[1] <= self.z_index:
            return edge_attr
        
        # æå–é˜»æŠ—æ¨¡é•¿
        z_magnitude = edge_attr[:, self.z_index].clamp(min=1e-6)
        
        # è®¡ç®—ç‰©ç†æƒé‡ï¼š1/|Z| (å¯¼çº³)
        physics_prior = self.physics_weight / z_magnitude
        
        # å°†ç‰©ç†å…ˆéªŒæ·»åŠ åˆ°è¾¹ç‰¹å¾ä¸­
        enhanced_edge_attr = edge_attr.clone()
        if enhanced_edge_attr.shape[1] < self.z_index + 2:
            # å¦‚æœç©ºé—´ä¸å¤Ÿï¼Œæ‰©å±•ç‰¹å¾ç»´åº¦
            padding = torch.zeros(
                enhanced_edge_attr.shape[0], 
                self.z_index + 2 - enhanced_edge_attr.shape[1],
                device=enhanced_edge_attr.device
            )
            enhanced_edge_attr = torch.cat([enhanced_edge_attr, padding], dim=1)
        
        # å°†ç‰©ç†å…ˆéªŒå­˜å‚¨åˆ°è¾¹ç‰¹å¾çš„æœ€åä¸€åˆ—
        enhanced_edge_attr[:, -1] = physics_prior * self.temperature
        
        return enhanced_edge_attr


class GNNEncoder(nn.Module):
    """
    åŒæ„GNNç¼–ç å™¨
    
    ç‰¹ç‚¹ï¼š
    1. å¤šå±‚PhysicsGATv2Convå †å 
    2. å±‚å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥
    3. è‡ªé€‚åº”dropout
    4. æ”¯æŒä¸åŒçš„èšåˆæ–¹å¼
    """
    
    def __init__(self, in_channels: int, hidden_channels: int, 
                 num_layers: int = 3, heads: int = 4, dropout: float = 0.3,
                 edge_dim: Optional[int] = None, activation: str = 'elu'):
        super().__init__()
        
        self.num_layers = num_layers
        self.activation = getattr(F, activation)
        
        # æ„å»ºå¤šå±‚ç½‘ç»œ
        self.convs = nn.ModuleList()
        self.norms = nn.ModuleList()
        self.residual_projs = nn.ModuleList()
        
        current_dim = in_channels
        
        for i in range(num_layers):
            # æœ€åä¸€å±‚ä¸ä½¿ç”¨å¤šå¤´concat
            is_last_layer = (i == num_layers - 1)
            concat = not is_last_layer
            
            # GATv2å·ç§¯å±‚
            conv = PhysicsGATv2Conv(
                current_dim, 
                hidden_channels,
                heads=heads,
                concat=concat,
                dropout=dropout,
                edge_dim=edge_dim,
                add_self_loops=False
            )
            self.convs.append(conv)
            
            # è®¡ç®—å®é™…è¾“å‡ºç»´åº¦
            actual_out_dim = hidden_channels * heads if concat else hidden_channels
            
            # å±‚å½’ä¸€åŒ–
            self.norms.append(LayerNorm(actual_out_dim))
            
            # æ®‹å·®æŠ•å½±
            if current_dim != actual_out_dim:
                self.residual_projs.append(nn.Linear(current_dim, actual_out_dim))
            else:
                self.residual_projs.append(nn.Identity())
            
            current_dim = actual_out_dim
        
        self.final_dim = current_dim
    
    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, 
                edge_attr: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        """
        for i, (conv, norm, residual_proj) in enumerate(
            zip(self.convs, self.norms, self.residual_projs)
        ):
            # ä¿å­˜æ®‹å·®
            residual = x
            
            # å·ç§¯
            x = conv(x, edge_index, edge_attr)
            
            # ç¡®ä¿xæ˜¯tensorè€Œä¸æ˜¯tuple
            if isinstance(x, tuple):
                x = x[0]
            
            # å±‚å½’ä¸€åŒ–
            x = norm(x)
            
            # æ®‹å·®è¿æ¥
            x = x + residual_proj(residual)
            
            # æ¿€æ´»å‡½æ•°ï¼ˆæœ€åä¸€å±‚é™¤å¤–ï¼‰
            if i < self.num_layers - 1:
                x = self.activation(x)
        
        return x


class HeteroGraphEncoder(nn.Module):
    """
    å¼‚æ„å›¾ç¼–ç å™¨ - ä¸“æ³¨äºå›¾è¡¨ç¤ºå­¦ä¹ 
    
    èŒè´£ï¼š
    1. å¤„ç†å¼‚æ„å›¾æ•°æ®ï¼ˆå¤šç§èŠ‚ç‚¹ç±»å‹å’Œè¾¹ç±»å‹ï¼‰
    2. æå–é«˜è´¨é‡çš„èŠ‚ç‚¹åµŒå…¥è¡¨ç¤º
    3. èå…¥ç”µç½‘ç‰©ç†å…ˆéªŒçŸ¥è¯†
    4. æ”¯æŒä¸‹æ¸¸ä»»åŠ¡çš„ç‰¹å¾æå–
    """
    
    def __init__(self, 
                 node_feature_dims: Dict[str, int],
                 edge_feature_dims: Dict[str, int],
                 metadata: Tuple[List[str], List[Tuple[str, str, str]]],
                 hidden_channels: int = 64,
                 gnn_layers: int = 3,
                 heads: int = 4,
                 dropout: float = 0.3,
                 output_dim: Optional[int] = None):
        super().__init__()
        
        self.node_types = metadata[0]
        self.edge_types = metadata[1]
        self.hidden_channels = hidden_channels
        self.output_dim = output_dim or hidden_channels
        
        # --- 1. ç‰¹å¾é¢„å¤„ç†å±‚ ---
        # ç»Ÿä¸€æ‰€æœ‰èŠ‚ç‚¹ç±»å‹çš„ç‰¹å¾ç»´åº¦
        max_node_dim = max(node_feature_dims.values())
        max_edge_dim = max(edge_feature_dims.values()) if edge_feature_dims else None
        
        # ä¸ºä¸åŒç±»å‹çš„èŠ‚ç‚¹åˆ›å»ºç‰¹å¾æŠ•å½±å±‚
        self.node_projectors = nn.ModuleDict()
        for node_type, feature_dim in node_feature_dims.items():
            if feature_dim != max_node_dim:
                self.node_projectors[node_type] = nn.Linear(feature_dim, max_node_dim)
            else:
                self.node_projectors[node_type] = nn.Identity()
        
        # --- 2. ä¸»å¹²ç½‘ç»œ (Backbone) ---
        # åˆ›å»ºåŒæ„GNNç¼–ç å™¨
        gnn_encoder = GNNEncoder(
            in_channels=max_node_dim,
            hidden_channels=hidden_channels,
            num_layers=gnn_layers,
            heads=heads,
            dropout=dropout,
            edge_dim=max_edge_dim
        )
        
        # è½¬æ¢ä¸ºå¼‚æ„æ¨¡å‹
        # åœ¨å®šä¹‰äº†ç¡®åˆ‡çš„ä¾èµ–å…³ç³»åï¼Œæˆ‘ä»¬ç¡®ä¿¡ to_hetero ä¼šç¨³å®šå·¥ä½œ
        self.hetero_encoder = to_hetero(gnn_encoder, metadata, aggr='sum')
        print("âœ… ä½¿ç”¨ to_hetero è½¬æ¢çš„å¼‚æ„ç¼–ç å™¨")
        
        self.final_dim = gnn_encoder.final_dim
        
        # --- 3. è¾“å‡ºæŠ•å½±å±‚ ---
        encoder_output_dim = gnn_encoder.final_dim
        if self.output_dim != encoder_output_dim:
            self.output_projector = nn.Linear(encoder_output_dim, self.output_dim)
        else:
            self.output_projector = nn.Identity()
        
        # --- 4. å›¾çº§åˆ«è¡¨ç¤ºèšåˆå™¨ ---
        self.graph_pooling = global_mean_pool
        self.graph_projector = nn.Sequential(
            nn.Linear(self.output_dim, hidden_channels),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels, self.output_dim)
        )
    
    def preprocess_node_features(self, x_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        é¢„å¤„ç†èŠ‚ç‚¹ç‰¹å¾ï¼Œç¡®ä¿æ‰€æœ‰ç±»å‹å…·æœ‰ç›¸åŒç»´åº¦
        """
        processed_x_dict = {}
        for node_type, x in x_dict.items():
            processed_x_dict[node_type] = self.node_projectors[node_type](x)
        return processed_x_dict
    
    def forward(self, data: HeteroData, 
                return_attention_weights: bool = False,
                return_graph_embedding: bool = False) -> Union[Dict[str, torch.Tensor], 
                                                               Tuple[Dict[str, torch.Tensor], ...]]:
        """
        å‰å‘ä¼ æ’­ - æå–èŠ‚ç‚¹å’Œå›¾çº§åˆ«çš„åµŒå…¥è¡¨ç¤º
        
        å‚æ•°:
            data: å¼‚æ„å›¾æ•°æ®
            return_attention_weights: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
            return_graph_embedding: æ˜¯å¦è¿”å›å›¾çº§åˆ«åµŒå…¥
        
        è¿”å›:
            node_embeddings: å„ç±»å‹èŠ‚ç‚¹çš„åµŒå…¥è¡¨ç¤º {node_type: embeddings}
            attention_weights: æ³¨æ„åŠ›æƒé‡ (å¯é€‰)
            graph_embedding: å›¾çº§åˆ«åµŒå…¥ (å¯é€‰)
        """
        # --- 1. é¢„å¤„ç†èŠ‚ç‚¹ç‰¹å¾ ---
        processed_x_dict = self.preprocess_node_features(data.x_dict)
        
        # --- 2. å¼‚æ„GNNç¼–ç  ---
        raw_embeddings = self.hetero_encoder(
            processed_x_dict,
            data.edge_index_dict,
            data.edge_attr_dict
        )
        # åº”ç”¨è¾“å‡ºæŠ•å½±
        node_embeddings = {
            node_type: self.output_projector(embeddings)
            for node_type, embeddings in raw_embeddings.items()
        }
        
        # --- 3. å‡†å¤‡è¿”å›å€¼ ---
        results = [node_embeddings]
        
        # --- 4. æ³¨æ„åŠ›æƒé‡ï¼ˆå¯é€‰ï¼‰---
        if return_attention_weights:
            attention_weights = self.get_attention_weights()
            results.append(attention_weights)
        
        # --- 5. å›¾çº§åˆ«åµŒå…¥ï¼ˆå¯é€‰ï¼‰---
        if return_graph_embedding:
            # èšåˆæ‰€æœ‰èŠ‚ç‚¹åµŒå…¥ä¸ºå›¾çº§åˆ«è¡¨ç¤º
            all_embeddings = torch.cat(list(node_embeddings.values()), dim=0)
            if all_embeddings.size(0) > 0:
                batch = torch.zeros(all_embeddings.size(0), 
                                  dtype=torch.long, 
                                  device=all_embeddings.device)
                graph_embedding = self.graph_pooling(all_embeddings, batch)
                graph_embedding = self.graph_projector(graph_embedding)
            else:
                graph_embedding = torch.zeros(1, self.output_dim, device=data.x_dict[list(data.x_dict.keys())[0]].device)
            results.append(graph_embedding)
        
        # æ ¹æ®è¿”å›å‚æ•°å†³å®šè¿”å›æ ¼å¼
        if len(results) == 1:
            return results[0]
        else:
            return tuple(results)
    
    def encode_nodes(self, data: HeteroData) -> Dict[str, torch.Tensor]:
        """
        ä¾¿æ·æ–¹æ³•ï¼šä»…æå–èŠ‚ç‚¹åµŒå…¥
        """
        return self.forward(data, return_attention_weights=False, return_graph_embedding=False)
    
    def encode_graph(self, data: HeteroData) -> torch.Tensor:
        """
        ä¾¿æ·æ–¹æ³•ï¼šä»…æå–å›¾çº§åˆ«åµŒå…¥
        """
        _, graph_embedding = self.forward(data, return_attention_weights=False, return_graph_embedding=True)
        return graph_embedding
    
    def get_attention_weights(self) -> List[torch.Tensor]:
        """
        è·å–æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ–åˆ†æ
        
        æ³¨æ„ï¼što_heteroè½¬æ¢åï¼Œæ¨¡å‹ç»“æ„ä¼šå‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦é€’å½’æœç´¢æ‰€æœ‰æ¨¡å—
        """
        attention_weights = []
        
        def collect_attention_weights(module):
            """é€’å½’æ”¶é›†æ³¨æ„åŠ›æƒé‡"""
            # æ£€æŸ¥å½“å‰æ¨¡å—æ˜¯å¦æ˜¯PhysicsGATv2Conv
            if isinstance(module, PhysicsGATv2Conv):
                if hasattr(module, '_alpha') and module._alpha is not None:
                    attention_weights.append(module._alpha)
            
            # é€’å½’æ£€æŸ¥å­æ¨¡å—
            for child in module.children():
                collect_attention_weights(child)
        
        # ä»hetero_encoderå¼€å§‹é€’å½’æœç´¢
        if hasattr(self, 'hetero_encoder'):
            collect_attention_weights(self.hetero_encoder)
        
        return attention_weights

    def get_embedding_dim(self) -> int:
        """
        è·å–åµŒå…¥ç»´åº¦
        """
        return self.output_dim


def create_hetero_graph_encoder(data: HeteroData, 
                               hidden_channels: int = 64,
                               gnn_layers: int = 3,
                               heads: int = 4,
                               dropout: float = 0.3,
                               output_dim: Optional[int] = None) -> HeteroGraphEncoder:
    """
    åˆ›å»ºå¼‚æ„å›¾ç¼–ç å™¨çš„ä¾¿æ·å‡½æ•°
    
    å‚æ•°:
        data: å¼‚æ„å›¾æ•°æ®ç¤ºä¾‹
        hidden_channels: éšè—å±‚ç»´åº¦
        gnn_layers: GNNå±‚æ•°
        heads: æ³¨æ„åŠ›å¤´æ•°
        dropout: dropoutæ¦‚ç‡
        output_dim: è¾“å‡ºåµŒå…¥ç»´åº¦
    
    è¿”å›:
        é…ç½®å¥½çš„HeteroGraphEncoderæ¨¡å‹
    """
    # æå–èŠ‚ç‚¹å’Œè¾¹ç‰¹å¾ç»´åº¦
    node_feature_dims = {node_type: x.shape[1] for node_type, x in data.x_dict.items()}
    edge_feature_dims = {edge_type: attr.shape[1] for edge_type, attr in data.edge_attr_dict.items()}
    
    # åˆ›å»ºç¼–ç å™¨
    encoder = HeteroGraphEncoder(
        node_feature_dims=node_feature_dims,
        edge_feature_dims=edge_feature_dims,
        metadata=data.metadata(),
        hidden_channels=hidden_channels,
        gnn_layers=gnn_layers,
        heads=heads,
        dropout=dropout,
        output_dim=output_dim
    )
    
    return encoder


def test_hetero_graph_encoder(data: HeteroData, device: torch.device):
    """
    æµ‹è¯•å¼‚æ„å›¾ç¼–ç å™¨
    """
    print("\nğŸ§  æµ‹è¯•å¼‚æ„Physics-Guided GATv2ç¼–ç å™¨...")
    
    # åˆ›å»ºç¼–ç å™¨
    encoder = create_hetero_graph_encoder(
        data, 
        hidden_channels=32, 
        gnn_layers=2, 
        heads=4, 
        output_dim=64
    )
    encoder = encoder.to(device)
    data = data.to(device)
    
    # æµ‹è¯•ä¸åŒçš„å‰å‘ä¼ æ’­æ¨¡å¼
    with torch.no_grad():
        # 1. ä»…æå–èŠ‚ç‚¹åµŒå…¥
        node_embeddings = encoder.encode_nodes(data)
        
        # 2. æå–å›¾çº§åˆ«åµŒå…¥
        graph_embedding = encoder.encode_graph(data)
        
        # 3. å®Œæ•´å‰å‘ä¼ æ’­ï¼ˆåŒ…å«æ³¨æ„åŠ›æƒé‡ï¼‰
        node_emb, attention_weights, graph_emb = encoder(
            data, 
            return_attention_weights=True, 
            return_graph_embedding=True
        )
    
    print(f"âœ… å¼‚æ„å›¾ç¼–ç å™¨æµ‹è¯•æˆåŠŸï¼")
    print(f"ğŸ“Š ç¼–ç å™¨å‚æ•°é‡: {sum(p.numel() for p in encoder.parameters()):,}")
    print(f"ğŸ“Š è¾“å‡ºåµŒå…¥ç»´åº¦: {encoder.get_embedding_dim()}")
    print(f"ğŸ“Š èŠ‚ç‚¹ç±»å‹æ•°é‡: {len(node_embeddings)}")
    
    for node_type, embeddings in node_embeddings.items():
        print(f"   - {node_type}: {embeddings.shape}")
    
    print(f"ğŸ“Š å›¾çº§åˆ«åµŒå…¥å½¢çŠ¶: {graph_embedding.shape}")
    print(f"ğŸ“Š æ³¨æ„åŠ›æƒé‡æ•°é‡: {len(attention_weights)}")
    
    return encoder


if __name__ == "__main__":
    print("ğŸ”¥ å¼‚æ„Physics-Guided GATv2å›¾ç¼–ç å™¨ - ä¸“æ³¨äºè¡¨ç¤ºå­¦ä¹ ï¼")
    print("ğŸ“– ä½¿ç”¨è¯´æ˜ï¼š")
    print("1. ä½¿ç”¨ create_hetero_graph_encoder() åˆ›å»ºç¼–ç å™¨")
    print("2. ä½¿ç”¨ encoder.encode_nodes() æå–èŠ‚ç‚¹åµŒå…¥")
    print("3. ä½¿ç”¨ encoder.encode_graph() æå–å›¾çº§åˆ«åµŒå…¥")
    print("4. ç¼–ç å™¨ä¸“æ³¨äºç‰¹å¾æå–ï¼Œä¸åŒ…å«å†³ç­–é€»è¾‘")

