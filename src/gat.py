import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv
from torch_geometric.utils import softmax
from typing import Optional, List, Dict, Tuple
import numpy as np

class PhysicsGuidedGATConv(GATConv):
    """
    ç‰©ç†çº¦æŸçš„å›¾æ³¨æ„åŠ›å·ç§¯å±‚
    
    æ ¸å¿ƒåˆ›æ–°ï¼š
    1. åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­èå…¥ç”µæ°”é˜»æŠ—å…ˆéªŒ
    2. é˜»æŠ—è¶Šå°çš„è¿æ¥è·å¾—è¶Šé«˜çš„æ³¨æ„åŠ›æƒé‡
    3. å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°æ§åˆ¶ç‰©ç†å…ˆéªŒçš„å½±å“ç¨‹åº¦
    
    æ•°å­¦åŸç†ï¼š
    æ ‡å‡†GAT: Î±_ij = softmax(LeakyReLU(a^T[Wh_i || Wh_j]))
    ç‰©ç†GAT: Î±_ij = softmax(LeakyReLU(a^T[Wh_i || Wh_j]) + Ï„/|Z_ij|)
    """
    
    def __init__(self, in_channels: int, out_channels: int, heads: int = 8,
                 concat: bool = True, dropout: float = 0.6, 
                 edge_dim: int = 8, physics_weight: float = 1.0, **kwargs):
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°ï¼Œå¯ç”¨è¾¹ç‰¹å¾
        super().__init__(in_channels, out_channels, heads=heads, concat=concat,
                        dropout=dropout, edge_dim=edge_dim, **kwargs)
        
        # ç‰©ç†å¼•å¯¼å‚æ•°
        self.physics_weight = physics_weight
        self.temperature = nn.Parameter(torch.tensor(1.0))  # å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°
        
        # è¾¹ç‰¹å¾æŠ•å½±å±‚ï¼ˆå°†è¾¹ç‰¹å¾æŠ•å½±åˆ°æ³¨æ„åŠ›ç©ºé—´ï¼‰
        self.edge_proj = nn.Linear(edge_dim, heads)
        
    def edge_update(self, alpha_j: torch.Tensor, alpha_i: Optional[torch.Tensor],
                   edge_attr: Optional[torch.Tensor], index: torch.Tensor,
                   ptr: Optional[torch.Tensor] = None,
                   size_i: Optional[int] = None) -> torch.Tensor:
        """
        è¾¹æ›´æ–°å‡½æ•°ï¼šèå…¥ç‰©ç†å…ˆéªŒ
        
        åœ¨è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°æ—¶åŠ å…¥é˜»æŠ—ä¿¡æ¯ï¼š
        - ä½¿ç”¨è¾¹ç‰¹å¾ä¸­çš„é˜»æŠ—æ¨¡é•¿ |Z| (ç¬¬3åˆ—)
        - é˜»æŠ—è¶Šå°ï¼Œå¯¼ç”µæ€§è¶Šå¥½ï¼Œæ³¨æ„åŠ›æƒé‡åº”è¯¥è¶Šå¤§
        """
        # è®¡ç®—åŸºç¡€æ³¨æ„åŠ›åˆ†æ•°
        alpha = alpha_j if alpha_i is None else alpha_j + alpha_i
        
        # èå…¥ç‰©ç†å…ˆéªŒ
        if edge_attr is not None and edge_attr.shape[1] > 3:
            # æå–é˜»æŠ—æ¨¡é•¿ï¼ˆç¬¬3åˆ—ï¼‰
            z_magnitude = edge_attr[:, 3].clamp(min=1e-6)  # é¿å…é™¤é›¶
            
            # è®¡ç®—ç‰©ç†æƒé‡ï¼š1/|Z| (å¯¼çº³)
            physics_prior = 1.0 / z_magnitude
            
            # æŠ•å½±è¾¹ç‰¹å¾åˆ°å¤šå¤´ç©ºé—´
            edge_weights = self.edge_proj(edge_attr)  # [E, heads]
            
            # å°†ç‰©ç†å…ˆéªŒåŠ å…¥æ³¨æ„åŠ›åˆ†æ•°
            # ä½¿ç”¨æ¸©åº¦å‚æ•°æ§åˆ¶ç‰©ç†å…ˆéªŒçš„å½±å“
            physics_contribution = self.temperature * self.physics_weight * physics_prior.unsqueeze(-1)
            
            # èåˆåŸå§‹æ³¨æ„åŠ›å’Œç‰©ç†å…ˆéªŒ
            alpha = alpha + physics_contribution + edge_weights
        
        # åº”ç”¨softmaxå½’ä¸€åŒ–
        alpha = F.leaky_relu(alpha, negative_slope=0.2)
        alpha = softmax(alpha, index, ptr, size_i)
        alpha = F.dropout(alpha, p=self.dropout, training=self.training)
        
        return alpha


class EnhancedGATEncoder(nn.Module):
    """
    å¢å¼ºçš„GATç¼–ç å™¨
    
    ç‰¹ç‚¹ï¼š
    1. å¤šå±‚Physics-Guided GAT
    2. æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
    3. è‡ªé€‚åº”dropout
    4. ä½ç½®ç¼–ç 
    5. å¤šå°ºåº¦ç‰¹å¾èåˆ
    """
    
    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int,
                 num_layers: int = 3, heads: int = 8, dropout: float = 0.3,
                 edge_dim: int = 8, add_positional_encoding: bool = True,
                 physics_weight: float = 1.0):
        super().__init__()
        
        self.num_layers = num_layers
        self.add_positional_encoding = add_positional_encoding
        
        # ä½ç½®ç¼–ç ï¼ˆåŸºäºæ‹‰æ™®æ‹‰æ–¯ç‰¹å¾å‘é‡ï¼‰
        if add_positional_encoding:
            self.pos_encoding_dim = 16
            self.pos_proj = nn.Linear(self.pos_encoding_dim, in_channels)
        
        # GATå±‚åˆ—è¡¨
        self.gat_layers = nn.ModuleList()
        self.layer_norms = nn.ModuleList()
        self.residual_projs = nn.ModuleList()
        self.dropout_layers = nn.ModuleList()
        
        # æ„å»ºå¤šå±‚ç½‘ç»œ
        current_dim = in_channels
        
        for i in range(num_layers):
            is_last_layer = (i == num_layers - 1)
            
            # è¾“å‡ºç»´åº¦
            if is_last_layer:
                out_dim = out_channels
                concat = False
            else:
                out_dim = hidden_channels
                concat = True
            
            # Physics-Guided GATå±‚
            gat = PhysicsGuidedGATConv(
                current_dim, out_dim, heads=heads,
                concat=concat, dropout=dropout,
                edge_dim=edge_dim, physics_weight=physics_weight,
                add_self_loops=False  # é¿å…é‡å¤æ·»åŠ è‡ªç¯
            )
            self.gat_layers.append(gat)
            
            # è®¡ç®—å®é™…è¾“å‡ºç»´åº¦
            actual_out_dim = out_dim * heads if concat else out_dim
            
            # å±‚å½’ä¸€åŒ–
            self.layer_norms.append(nn.LayerNorm(actual_out_dim))
            
            # æ®‹å·®æŠ•å½±
            if current_dim != actual_out_dim:
                self.residual_projs.append(nn.Linear(current_dim, actual_out_dim))
            else:
                self.residual_projs.append(nn.Identity())
            
            # è‡ªé€‚åº”dropout
            self.dropout_layers.append(nn.Dropout(dropout * (1 - i / num_layers)))
            
            current_dim = actual_out_dim
        
        # è¾“å‡ºæŠ•å½±å±‚
        self.output_proj = nn.Linear(current_dim, out_channels)
        
    def compute_positional_encoding(self, edge_index: torch.Tensor, num_nodes: int) -> torch.Tensor:
        """
        è®¡ç®—åŸºäºå›¾æ‹‰æ™®æ‹‰æ–¯çš„ä½ç½®ç¼–ç 
        
        ä½¿ç”¨å½’ä¸€åŒ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å‘é‡ä½œä¸ºä½ç½®ç¼–ç 
        è¿™èƒ½æ•æ‰å›¾çš„å…¨å±€ç»“æ„ä¿¡æ¯
        """
        device = edge_index.device
        
        # æ„å»ºé‚»æ¥çŸ©é˜µ
        row, col = edge_index
        adj = torch.zeros((num_nodes, num_nodes), device=device)
        adj[row, col] = 1.0
        adj = adj + adj.T  # å¯¹ç§°åŒ–
        
        # åº¦çŸ©é˜µ
        deg = adj.sum(dim=1)
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
        
        # å½’ä¸€åŒ–æ‹‰æ™®æ‹‰æ–¯ L = I - D^{-1/2} A D^{-1/2}
        norm_adj = deg_inv_sqrt.unsqueeze(0) * adj * deg_inv_sqrt.unsqueeze(1)
        laplacian = torch.eye(num_nodes, device=device) - norm_adj
        
        # ç‰¹å¾åˆ†è§£ï¼ˆåªå–å‰kä¸ªæœ€å°ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼‰
        try:
            eigenvalues, eigenvectors = torch.linalg.eigh(laplacian)
            # å–å‰pos_encoding_dimä¸ªç‰¹å¾å‘é‡ï¼ˆè·³è¿‡å¸¸æ•°ç‰¹å¾å‘é‡ï¼‰
            pos_encoding = eigenvectors[:, 1:self.pos_encoding_dim+1]
            
            # å¦‚æœç»´åº¦ä¸å¤Ÿï¼Œå¡«å……é›¶
            if pos_encoding.shape[1] < self.pos_encoding_dim:
                padding = torch.zeros(
                    num_nodes, 
                    self.pos_encoding_dim - pos_encoding.shape[1],
                    device=device
                )
                pos_encoding = torch.cat([pos_encoding, padding], dim=1)
        except:
            # å¦‚æœç‰¹å¾åˆ†è§£å¤±è´¥ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–
            pos_encoding = torch.randn(num_nodes, self.pos_encoding_dim, device=device) * 0.1
        
        return pos_encoding
    
    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, 
               edge_attr: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: èŠ‚ç‚¹ç‰¹å¾ [N, in_channels]
            edge_index: è¾¹ç´¢å¼• [2, E]
            edge_attr: è¾¹ç‰¹å¾ [E, edge_dim]
            
        è¿”å›:
            èŠ‚ç‚¹åµŒå…¥ [N, out_channels]
        """
        # æ·»åŠ ä½ç½®ç¼–ç 
        if self.add_positional_encoding:
            pos_encoding = self.compute_positional_encoding(edge_index, x.size(0))
            pos_features = self.pos_proj(pos_encoding)
            x = x + pos_features
        
        # å¤šå±‚GATå¤„ç†
        for i in range(self.num_layers):
            # ä¿å­˜è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
            residual = x
            
            # GATå·ç§¯
            x = self.gat_layers[i](x, edge_index, edge_attr)
            
            # å±‚å½’ä¸€åŒ–
            x = self.layer_norms[i](x)
            
            # æ®‹å·®è¿æ¥
            x = x + self.residual_projs[i](residual)
            
            # æ¿€æ´»å’Œdropoutï¼ˆæœ€åä¸€å±‚é™¤å¤–ï¼‰
            if i < self.num_layers - 1:
                x = F.elu(x)
                x = self.dropout_layers[i](x)
        
        # è¾“å‡ºæŠ•å½±
        x = self.output_proj(x)
        
        return x
    
    def get_attention_weights(self) -> List[torch.Tensor]:
        """
        è·å–å„å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        """
        attention_weights = []
        for gat_layer in self.gat_layers:
            if hasattr(gat_layer, '_alpha'):
                attention_weights.append(gat_layer._alpha)
        return attention_weights


# Test function can be called from main if needed
def initialize_gat_encoder(data, device):
    """Test function for GAT encoder"""
    print("\nğŸ§  æµ‹è¯•Physics-Guided GATç¼–ç å™¨...")
    encoder = EnhancedGATEncoder(
        in_channels=data.x.shape[1],
        hidden_channels=64,
        out_channels=64,
        num_layers=3,
        heads=8,
        edge_dim=data.edge_attr.shape[1]
    ).to(device)

    # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
    data = data.to(device)

    # å‰å‘ä¼ æ’­æµ‹è¯•
    with torch.no_grad():
        embeddings = encoder(data.x, data.edge_index, data.edge_attr)
        
    print(f"âœ… GATç¼–ç å™¨æµ‹è¯•æˆåŠŸï¼")
    print(f"ğŸ“Š è¾“å…¥ç»´åº¦: {data.x.shape}")
    print(f"ğŸ“Š è¾“å‡ºåµŒå…¥ç»´åº¦: {embeddings.shape}")
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in encoder.parameters()):,}")
    
    return encoder, embeddings

