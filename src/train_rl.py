#!/usr/bin/env python3
"""
ç”µåŠ›ç½‘ç»œåˆ†åŒºå¼ºåŒ–å­¦ä¹ ç»Ÿä¸€è®­ç»ƒè„šæœ¬

é›†æˆäº†å®Œæ•´çš„è®­ç»ƒåŸºç¡€è®¾æ–½ï¼š
- è®­ç»ƒå¾ªç¯å’Œç»éªŒæ”¶é›†
- æ¨¡å‹æ£€æŸ¥ç‚¹å’Œæ¢å¤
- æ—¥å¿—è®°å½•å’Œç›‘æ§
- å¯è§†åŒ–æ”¯æŒ
- é…ç½®æ–‡ä»¶é©±åŠ¨
"""

import torch
import numpy as np
import argparse
import os
import sys
import time
import json
import warnings
from pathlib import Path
import yaml
from typing import Dict, List, Tuple, Optional, Any
from collections import deque
import matplotlib.pyplot as plt

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False

# ç¦ç”¨è­¦å‘Š
warnings.filterwarnings('ignore')


class TrainingLogger:
    """
    è®­ç»ƒè¿‡ç¨‹çš„ç»¼åˆæ—¥å¿—è®°å½•
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è®­ç»ƒæ—¥å¿—è®°å½•å™¨
        
        Args:
            config: å®Œæ•´é…ç½®å­—å…¸
        """
        # æå–æ—¥å¿—é…ç½®
        logging_config = config.get('logging', {})
        training_config = config.get('training', {})
        
        self.log_dir = Path(training_config.get('log_dir', 'logs'))
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # é…ç½®å‚æ•°
        self.use_tensorboard = training_config.get('use_tensorboard', True)
        self.metrics_save_interval = logging_config.get('metrics_save_interval', 50)
        self.plot_save_interval = logging_config.get('plot_save_interval', 100)
        self.console_log_interval = logging_config.get('console_log_interval', 10)
        
        # TensorBoard writer
        self.writer = None
        if self.use_tensorboard and TENSORBOARD_AVAILABLE:
            self.writer = SummaryWriter(str(self.log_dir / 'tensorboard'))
            
        # è®­ç»ƒæŒ‡æ ‡
        metrics_to_track = logging_config.get('training_metrics', [
            'episode_rewards', 'episode_lengths', 'success_rates', 'load_cv',
            'coupling_edges', 'actor_losses', 'critic_losses', 'entropies'
        ])
        
        self.metrics = {metric: [] for metric in metrics_to_track}
        
        # æœ€ä½³æ¨¡å‹è·Ÿè¸ª
        self.best_reward = float('-inf')
        self.best_episode = 0
        
    def log_episode(self, episode: int, episode_data: Dict[str, Any]):
        """è®°å½•å›åˆæ•°æ®"""
        # å­˜å‚¨å¸¦æœ‰é”®æ˜ å°„çš„æŒ‡æ ‡
        key_mapping = {
            'episode_reward': 'episode_rewards',
            'episode_length': 'episode_lengths'
        }

        for key, value in episode_data.items():
            # å¦‚æœå¯ç”¨ä½¿ç”¨æ˜ å°„é”®ï¼Œå¦åˆ™ä½¿ç”¨åŸå§‹é”®
            target_key = key_mapping.get(key, key)
            if target_key in self.metrics:
                self.metrics[target_key].append(value)
                
        # TensorBoardæ—¥å¿—è®°å½•
        if self.writer:
            for key, value in episode_data.items():
                if isinstance(value, (int, float)):
                    self.writer.add_scalar(f'Episode/{key}', value, episode)
                    
        # æ›´æ–°æœ€ä½³æ¨¡å‹
        if 'episode_reward' in episode_data:
            if episode_data['episode_reward'] > self.best_reward:
                self.best_reward = episode_data['episode_reward']
                self.best_episode = episode
                
        # æ§åˆ¶å°æ—¥å¿—
        if episode % self.console_log_interval == 0:
            print(f"å›åˆ {episode}: å¥–åŠ±={episode_data.get('episode_reward', 0):.3f}, "
                  f"é•¿åº¦={episode_data.get('episode_length', 0)}")
                
    def log_training_step(self, step: int, training_data: Dict[str, Any]):
        """è®°å½•è®­ç»ƒæ­¥éª¤æ•°æ®"""
        if self.writer:
            for key, value in training_data.items():
                if isinstance(value, (int, float)):
                    self.writer.add_scalar(f'Training/{key}', value, step)
                    
    def save_metrics(self):
        """å°†æŒ‡æ ‡ä¿å­˜åˆ°æ–‡ä»¶"""
        metrics_file = self.log_dir / 'training_metrics.json'
        with open(metrics_file, 'w') as f:
            # å°†numpyæ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨ä»¥è¿›è¡ŒJSONåºåˆ—åŒ–
            serializable_metrics = {}
            for key, value in self.metrics.items():
                if isinstance(value, list):
                    serializable_metrics[key] = [float(v) if hasattr(v, 'item') else v for v in value]
                else:
                    serializable_metrics[key] = value
                    
            json.dump(serializable_metrics, f, indent=2)
            
    def plot_training_curves(self, viz_manager):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        if viz_manager.enabled:
            viz_manager.plot_training_curves(self.metrics, save_path="training_curves.png")
        
    def close(self):
        """å…³é—­æ—¥å¿—è®°å½•å™¨"""
        if self.writer:
            self.writer.close()


class CheckpointManager:
    """
    æ¨¡å‹æ£€æŸ¥ç‚¹å’Œæ¢å¤
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
        
        Args:
            config: å®Œæ•´é…ç½®å­—å…¸
        """
        training_config = config.get('training', {})
        self.checkpoint_dir = Path(training_config.get('checkpoint_dir', 'checkpoints'))
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
    def save_checkpoint(self, 
                       agent,
                       episode: int,
                       metrics: Dict[str, Any],
                       is_best: bool = False):
        """
        ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹
        
        Args:
            agent: è¦ä¿å­˜çš„PPOæ™ºèƒ½ä½“
            episode: å½“å‰å›åˆæ•°
            metrics: è®­ç»ƒæŒ‡æ ‡
            is_best: æ˜¯å¦æ˜¯ç›®å‰æœ€ä½³æ¨¡å‹
        """
        checkpoint = {
            'episode': episode,
            'agent_state_dict': agent.actor.state_dict(),
            'critic_state_dict': agent.critic.state_dict(),
            'actor_optimizer_state_dict': agent.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': agent.critic_optimizer.state_dict(),
            'metrics': metrics,
            'timestamp': time.time()
        }
        
        # ä¿å­˜å¸¸è§„æ£€æŸ¥ç‚¹
        checkpoint_path = self.checkpoint_dir / f'checkpoint_episode_{episode}.pt'
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = self.checkpoint_dir / 'best_model.pt'
            torch.save(checkpoint, best_path)
            
        # ä¿å­˜æœ€æ–°æ¨¡å‹
        latest_path = self.checkpoint_dir / 'latest_model.pt'
        torch.save(checkpoint, latest_path)
        
    def load_checkpoint(self, agent, checkpoint_path: str) -> Dict[str, Any]:
        """
        åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹
        
        Args:
            agent: è¦åŠ è½½åˆ°çš„PPOæ™ºèƒ½ä½“
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ£€æŸ¥ç‚¹å…ƒæ•°æ®
        """
        checkpoint = torch.load(checkpoint_path, map_location=agent.device)
        
        agent.actor.load_state_dict(checkpoint['agent_state_dict'])
        agent.critic.load_state_dict(checkpoint['critic_state_dict'])
        agent.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        agent.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        
        return {
            'episode': checkpoint['episode'],
            'metrics': checkpoint.get('metrics', {}),
            'timestamp': checkpoint.get('timestamp', 0)
        }
        
    def get_latest_checkpoint(self) -> Optional[str]:
        """è·å–æœ€æ–°æ£€æŸ¥ç‚¹çš„è·¯å¾„"""
        latest_path = self.checkpoint_dir / 'latest_model.pt'
        return str(latest_path) if latest_path.exists() else None
        
    def get_best_checkpoint(self) -> Optional[str]:
        """è·å–æœ€ä½³æ£€æŸ¥ç‚¹çš„è·¯å¾„"""
        best_path = self.checkpoint_dir / 'best_model.pt'
        return str(best_path) if best_path.exists() else None


class UnifiedTrainer:
    """
    ç”µåŠ›ç½‘ç»œåˆ†åŒºå¼ºåŒ–å­¦ä¹ çš„ç»Ÿä¸€è®­ç»ƒç±»
    æ•´åˆäº†è®­ç»ƒã€æ—¥å¿—ã€æ£€æŸ¥ç‚¹ã€å¯è§†åŒ–ç­‰æ‰€æœ‰åŠŸèƒ½
    """
    
    def __init__(self,
                 agent,
                 env,
                 config: Dict[str, Any]):
        """
        åˆå§‹åŒ–ç»Ÿä¸€è®­ç»ƒå™¨
        
        Args:
            agent: PPOæ™ºèƒ½ä½“
            env: è®­ç»ƒç¯å¢ƒ
            config: å®Œæ•´é…ç½®å­—å…¸
        """
        self.agent = agent
        self.env = env
        self.config = config
        
        # æå–è®­ç»ƒé…ç½®
        training_config = config.get('training', {})
        self.save_interval = training_config.get('save_interval', 100)
        self.eval_interval = training_config.get('eval_interval', 50)
        
        # æˆåŠŸåˆ¤å®šæ ‡å‡†
        success_criteria = training_config.get('success_criteria', {})
        self.load_cv_threshold = success_criteria.get('load_cv_threshold', 0.3)
        self.connectivity_threshold = success_criteria.get('connectivity_threshold', 0.9)
        self.min_length_threshold = success_criteria.get('min_length_threshold', 10)
        
        # æ”¶æ•›æ£€æµ‹
        convergence_config = training_config.get('convergence', {})
        self.convergence_window = convergence_config.get('window_size', 10)
        self.convergence_threshold = convergence_config.get('threshold', 0.01)
        
        # è®¾ç½®ç»„ä»¶
        self.logger = TrainingLogger(config)
        self.checkpoint_manager = CheckpointManager(config)
        
        # è®­ç»ƒçŠ¶æ€
        self.current_episode = 0
        self.training_step = 0
        
        # æˆåŠŸè·Ÿè¸ª
        self.recent_successes = deque(maxlen=100)
        
    def train(self,
              num_episodes: int,
              max_steps_per_episode: int = 200,
              update_interval: int = 10,
              resume_from: Optional[str] = None) -> Dict[str, List[float]]:
        """
        ä¸»è®­ç»ƒå¾ªç¯
        
        Args:
            num_episodes: è®­ç»ƒçš„å›åˆæ•°
            max_steps_per_episode: æ¯å›åˆæœ€å¤§æ­¥æ•°
            update_interval: æ™ºèƒ½ä½“æ›´æ–°é—´éš”ï¼ˆå›åˆæ•°ï¼‰
            resume_from: è¦æ¢å¤çš„æ£€æŸ¥ç‚¹è·¯å¾„
            
        Returns:
            è®­ç»ƒå†å²
        """
        # å¦‚æœæŒ‡å®šåˆ™ä»æ£€æŸ¥ç‚¹æ¢å¤
        if resume_from:
            checkpoint_info = self.checkpoint_manager.load_checkpoint(self.agent, resume_from)
            self.current_episode = checkpoint_info['episode']
            print(f"ä»ç¬¬{self.current_episode}å›åˆæ¢å¤è®­ç»ƒ")
            
        print(f"å¼€å§‹è®­ç»ƒ{num_episodes}å›åˆ...")
        print(f"ç¯å¢ƒï¼š{self.env.total_nodes}ä¸ªèŠ‚ç‚¹ï¼Œ{self.env.num_partitions}ä¸ªåˆ†åŒº")
        
        for episode in range(self.current_episode, self.current_episode + num_episodes):
            episode_start_time = time.time()
            
            # è¿è¡Œå›åˆ
            episode_data = self._run_episode(max_steps_per_episode)
            
            # è®°å½•å›åˆ
            episode_data['episode_time'] = time.time() - episode_start_time
            self.logger.log_episode(episode, episode_data)
            
            # æ›´æ–°æˆåŠŸè·Ÿè¸ª
            success = self._is_successful_episode(episode_data)
            self.recent_successes.append(success)
            
            # æ›´æ–°æ™ºèƒ½ä½“
            if episode % update_interval == 0 and len(self.agent.memory.states) > 0:
                training_stats = self.agent.update()
                self.logger.log_training_step(self.training_step, training_stats)
                self.training_step += 1
                
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if episode % self.save_interval == 0:
                is_best = episode_data['episode_reward'] >= self.logger.best_reward
                self.checkpoint_manager.save_checkpoint(
                    self.agent, episode, self.logger.metrics, is_best
                )
                
            # å®šæœŸä¿å­˜æŒ‡æ ‡å’Œå›¾è¡¨
            if episode % self.logger.metrics_save_interval == 0:
                self.logger.save_metrics()
                
            # æ£€æŸ¥æ”¶æ•›
            if self._check_convergence():
                print(f"è®­ç»ƒåœ¨ç¬¬{episode}å›åˆæ”¶æ•›ï¼")
                break
                
        # æœ€ç»ˆä¿å­˜
        self.checkpoint_manager.save_checkpoint(
            self.agent, self.current_episode + num_episodes - 1, self.logger.metrics
        )
        self.logger.save_metrics()
        
        print("è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³å¥–åŠ±ï¼š{self.logger.best_reward:.3f}ï¼Œåœ¨ç¬¬{self.logger.best_episode}å›åˆ")
        
        return self.logger.metrics
        
    def _run_episode(self, max_steps: int) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªè®­ç»ƒå›åˆ"""
        obs, info = self.env.reset()
        episode_reward = 0.0
        episode_length = 0
        
        for step in range(max_steps):
            # é€‰æ‹©åŠ¨ä½œ
            action, log_prob, value = self.agent.select_action(obs, training=True)
            
            if action is None:
                # æ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œ
                break
                
            # æ‰§è¡Œæ­¥éª¤
            next_obs, reward, terminated, truncated, next_info = self.env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            self.agent.store_experience(obs, action, reward, log_prob, value, terminated or truncated)
            
            episode_reward += reward
            episode_length += 1
            
            obs = next_obs
            info = next_info
            
            if terminated or truncated:
                break
                
        # è·å–æœ€ç»ˆæŒ‡æ ‡
        final_metrics = info.get('metrics', {})
        
        return {
            'episode_reward': episode_reward,
            'episode_length': episode_length,
            'load_cv': final_metrics.get('load_cv', 0.0),
            'coupling_edges': final_metrics.get('coupling_edges', 0),
            'connectivity': final_metrics.get('connectivity', 0.0),
            'final_metrics': final_metrics
        }
        
    def _is_successful_episode(self, episode_data: Dict[str, Any]) -> bool:
        """åˆ¤æ–­å›åˆæ˜¯å¦æˆåŠŸ"""
        return (
            episode_data.get('load_cv', 1.0) < self.load_cv_threshold and
            episode_data.get('connectivity', 0.0) > self.connectivity_threshold and
            episode_data.get('episode_length', 0) >= self.min_length_threshold
        )
        
    def _check_convergence(self) -> bool:
        """æ£€æŸ¥è®­ç»ƒæ˜¯å¦æ”¶æ•›"""
        if len(self.logger.metrics['episode_rewards']) < self.convergence_window:
            return False
            
        recent_rewards = self.logger.metrics['episode_rewards'][-self.convergence_window:]
        return np.std(recent_rewards) < self.convergence_threshold
        
    def evaluate(self, num_episodes: int = None) -> Dict[str, float]:
        """
        è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½
        
        Args:
            num_episodes: è¯„ä¼°å›åˆæ•°
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡
        """
        if num_episodes is None:
            eval_config = self.config.get('evaluation', {})
            num_episodes = eval_config.get('num_episodes', 20)
            
        print(f"è¯„ä¼°æ™ºèƒ½ä½“{num_episodes}å›åˆ...")
        
        eval_rewards = []
        eval_lengths = []
        eval_successes = []
        eval_metrics = []
        
        for episode in range(num_episodes):
            obs, info = self.env.reset()
            episode_reward = 0.0
            episode_length = 0
            
            while True:
                # é€‰æ‹©åŠ¨ä½œï¼ˆè´ªå¿ƒï¼‰
                action, _, _ = self.agent.select_action(obs, training=False)
                
                if action is None:
                    break
                    
                obs, reward, terminated, truncated, info = self.env.step(action)
                episode_reward += reward
                episode_length += 1
                
                if terminated or truncated:
                    break
                    
            eval_rewards.append(episode_reward)
            eval_lengths.append(episode_length)
            eval_successes.append(self._is_successful_episode({
                'episode_reward': episode_reward,
                'episode_length': episode_length,
                **info.get('metrics', {})
            }))
            eval_metrics.append(info.get('metrics', {}))
            
        # è®¡ç®—è¯„ä¼°ç»Ÿè®¡
        eval_stats = {
            'mean_reward': np.mean(eval_rewards),
            'std_reward': np.std(eval_rewards),
            'mean_length': np.mean(eval_lengths),
            'success_rate': np.mean(eval_successes),
            'mean_load_cv': np.mean([m.get('load_cv', 0) for m in eval_metrics]),
            'mean_coupling': np.mean([m.get('coupling_edges', 0) for m in eval_metrics])
        }
        
        print(f"è¯„ä¼°ç»“æœï¼š")
        for key, value in eval_stats.items():
            print(f"  {key}: {value:.4f}")
            
        return eval_stats
        
    def run_final_visualization(self):
        """è¿è¡Œæœ€ç»ˆå¯è§†åŒ–"""
        # åŠ¨æ€å¯¼å…¥å¯è§†åŒ–æ¨¡å—
        try:
            from visualization import VisualizationManager
            viz_manager = VisualizationManager(self.config)
            if viz_manager.enabled:
                print("\nğŸ“ˆ ç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–...")
                viz_manager.run_basic_visualization(self.env, self.logger.metrics)
        except ImportError:
            print("âš ï¸ å¯è§†åŒ–æ¨¡å—ä¸å¯ç”¨")
        
    def close(self):
        """æ¸…ç†è®­ç»ƒå™¨"""
        self.logger.close()
        if hasattr(self.env, 'close'):
            self.env.close()


def load_config(config_path: str) -> Dict[str, Any]:
    """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def create_default_config() -> Dict[str, Any]:
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    return {
        'data': {
            'case_name': 'ieee14',
            'normalize': True,
            'cache_dir': 'cache'
        },
        'environment': {
            'num_partitions': 3,
            'max_steps': 200,
            'reward_weights': {
                'load_balance': 0.4,
                'electrical_decoupling': 0.4,
                'power_balance': 0.2
            }
        },
        'gat': {
            'hidden_channels': 64,
            'gnn_layers': 3,
            'heads': 4,
            'output_dim': 128,
            'dropout': 0.1
        },
        'agent': {
            'lr_actor': 3e-4,
            'lr_critic': 1e-3,
            'gamma': 0.99,
            'eps_clip': 0.2,
            'k_epochs': 4,
            'entropy_coef': 0.01,
            'value_coef': 0.5,
            'hidden_dim': 256,
            'dropout': 0.1
        },
        'training': {
            'num_episodes': 1000,
            'max_steps_per_episode': 200,
            'update_interval': 10,
            'save_interval': 100,
            'eval_interval': 50,
            'log_dir': 'logs',
            'checkpoint_dir': 'checkpoints',
            'use_tensorboard': True,
            'success_criteria': {
                'load_cv_threshold': 0.3,
                'connectivity_threshold': 0.9,
                'min_length_threshold': 10
            },
            'convergence': {
                'window_size': 10,
                'threshold': 0.01
            }
        },
        'evaluation': {
            'num_episodes': 20
        },
        'logging': {
            'metrics_save_interval': 50,
            'plot_save_interval': 100,
            'console_log_interval': 10
        },
        'visualization': {
            'enabled': True,
            'save_figures': True,
            'figures_dir': 'figures'
        },
        'system': {
            'device': 'auto',
            'seed': 42,
            'num_threads': 1,
            'warnings': 'ignore'
        }
    }


def load_power_grid_data(case_name: str) -> Dict[str, Any]:
    """åŠ è½½ç”µç½‘æ•°æ®"""
    if case_name == 'ieee14':
        # IEEE 14-busæµ‹è¯•ç³»ç»Ÿ
        return {
            'baseMVA': 100.0,
            'bus': np.array([
                [1, 3, 0.0, 0.0, 0.0, 0.0, 1, 1.06, 0.0, 138, 1, 1.1, 0.9],
                [2, 2, 21.7, 12.7, 0.0, 0.0, 1, 1.045, -4.98, 138, 1, 1.1, 0.9],
                [3, 2, 94.2, 19.0, 0.0, 0.0, 1, 1.01, -12.72, 138, 1, 1.1, 0.9],
                [4, 1, 47.8, -3.9, 0.0, 0.0, 1, 1.019, -10.33, 138, 1, 1.1, 0.9],
                [5, 1, 7.6, 1.6, 0.0, 0.0, 1, 1.02, -8.78, 138, 1, 1.1, 0.9],
                [6, 2, 11.2, 7.5, 0.0, 0.0, 1, 1.07, -14.22, 138, 1, 1.1, 0.9],
                [7, 1, 0.0, 0.0, 0.0, 0.0, 1, 1.062, -13.37, 138, 1, 1.1, 0.9],
                [8, 2, 0.0, 0.0, 0.0, 0.0, 1, 1.09, -13.36, 138, 1, 1.1, 0.9],
                [9, 1, 29.5, 16.6, 0.0, 19.0, 1, 1.056, -14.94, 138, 1, 1.1, 0.9],
                [10, 1, 9.0, 5.8, 0.0, 0.0, 1, 1.051, -15.1, 138, 1, 1.1, 0.9],
                [11, 1, 3.5, 1.8, 0.0, 0.0, 1, 1.057, -14.79, 138, 1, 1.1, 0.9],
                [12, 1, 6.1, 1.6, 0.0, 0.0, 1, 1.055, -15.07, 138, 1, 1.1, 0.9],
                [13, 1, 13.5, 5.8, 0.0, 0.0, 1, 1.05, -15.16, 138, 1, 1.1, 0.9],
                [14, 1, 14.9, 5.0, 0.0, 0.0, 1, 1.036, -16.04, 138, 1, 1.1, 0.9],
            ]),
            'branch': np.array([
                [1, 2, 0.01938, 0.05917, 0.0528, 100, 110, 120, 0, 0, 1, -360, 360],
                [1, 5, 0.05403, 0.22304, 0.0492, 100, 110, 120, 0, 0, 1, -360, 360],
                [2, 3, 0.04699, 0.19797, 0.0438, 100, 110, 120, 0, 0, 1, -360, 360],
                [2, 4, 0.05811, 0.17632, 0.034, 100, 110, 120, 0, 0, 1, -360, 360],
                [2, 5, 0.05695, 0.17388, 0.0346, 100, 110, 120, 0, 0, 1, -360, 360],
                [3, 4, 0.06701, 0.17103, 0.0128, 100, 110, 120, 0, 0, 1, -360, 360],
                [4, 5, 0.01335, 0.04211, 0.0, 100, 110, 120, 0, 0, 1, -360, 360],
                [4, 7, 0.0, 0.20912, 0.0, 100, 110, 120, 0.978, 0, 1, -360, 360],
                [4, 9, 0.0, 0.55618, 0.0, 100, 110, 120, 0.969, 0, 1, -360, 360],
                [5, 6, 0.0, 0.25202, 0.0, 100, 110, 120, 0.932, 0, 1, -360, 360],
                [6, 11, 0.09498, 0.1989, 0.0, 100, 110, 120, 0, 0, 1, -360, 360],
                [6, 12, 0.12291, 0.25581, 0.0, 100, 110, 120, 0, 0, 1, -360, 360],
                [6, 13, 0.06615, 0.13027, 0.0, 100, 110, 120, 0, 0, 1, -360, 360],
                [7, 8, 0.0, 0.17615, 0.0, 100, 110, 120, 0, 0, 1, -360, 360],
                [7, 9, 0.0, 0.11001, 0.0, 100, 110, 120, 0, 0, 1, -360, 360],
                [9, 10, 0.03181, 0.0845, 0.0, 100, 110, 120, 0, 0, 1, -360, 360],
                [9, 14, 0.12711, 0.27038, 0.0, 100, 110, 120, 0, 0, 1, -360, 360],
                [10, 11, 0.08205, 0.19207, 0.0, 100, 110, 120, 0, 0, 1, -360, 360],
                [12, 13, 0.22092, 0.19988, 0.0, 100, 110, 120, 0, 0, 1, -360, 360],
                [13, 14, 0.17093, 0.34802, 0.0, 100, 110, 120, 0, 0, 1, -360, 360],
            ]),
            'gen': np.array([
                [1, 232.4, -16.9, 10, -10, 1.06, 100, 1, 332.4, 0],
                [2, 40.0, 43.56, 50, -40, 1.045, 100, 1, 140.0, 0],
                [3, 0.0, 25.075, 40, 0, 1.01, 100, 1, 100.0, 0],
                [6, 0.0, 12.73, 24, -6, 1.07, 100, 1, 100.0, 0],
                [8, 0.0, 17.623, 24, -6, 1.09, 100, 1, 100.0, 0],
            ])
        }
    else:
        # å°è¯•ä»æ–‡ä»¶åŠ è½½
        if os.path.exists(case_name):
            raise NotImplementedError("MATPOWERæ–‡ä»¶åŠ è½½å°šæœªå®ç°")
        else:
            raise ValueError(f"æœªçŸ¥æ¡ˆä¾‹: {case_name}")


def setup_device(device_config: str) -> torch.device:
    """è®¾ç½®è®¡ç®—è®¾å¤‡"""
    if device_config == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(device_config)
        
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    return device


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç”µåŠ›ç½‘ç»œåˆ†åŒºå¼ºåŒ–å­¦ä¹ è®­ç»ƒ')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--case', type=str, default='ieee14', help='ç”µç½‘æ¡ˆä¾‹åç§°')
    parser.add_argument('--episodes', type=int, help='è®­ç»ƒå›åˆæ•°')
    parser.add_argument('--partitions', type=int, help='åˆ†åŒºæ•°é‡')
    parser.add_argument('--resume', type=str, help='æ¢å¤æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--eval-only', action='store_true', help='ä»…è¿è¡Œè¯„ä¼°')
    parser.add_argument('--save-config', type=str, help='ä¿å­˜é»˜è®¤é…ç½®åˆ°æ–‡ä»¶')
    parser.add_argument('--preset', type=str, choices=['quick', 'full', 'large'], 
                       help='ä½¿ç”¨é¢„è®¾é…ç½®')
    
    args = parser.parse_args()
    
    # ä¿å­˜é»˜è®¤é…ç½®
    if args.save_config:
        config = create_default_config()
        with open(args.save_config, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, indent=2, allow_unicode=True)
        print(f"é»˜è®¤é…ç½®å·²ä¿å­˜åˆ° {args.save_config}")
        return
    
    # åŠ¨æ€å¯¼å…¥æ¨¡å—ï¼ˆé¿å…åœ¨ä¿å­˜é…ç½®æ—¶å‡ºé”™ï¼‰
    try:
        from data_processing import PowerGridDataProcessor
        from gat import create_hetero_graph_encoder
        from rl.environment import PowerGridPartitioningEnv
        from rl.agent import PPOAgent
        from visualization import VisualizationManager
    except ImportError as e:
        print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        return 1
    
    # åŠ è½½é…ç½®
    if args.config:
        config = load_config(args.config)
    else:
        config = create_default_config()
        
    # åº”ç”¨é¢„è®¾é…ç½®
    if args.preset:
        preset_key = f"{args.preset}_training"
        if preset_key in config:
            preset_config = config[preset_key]
            # é€’å½’æ›´æ–°é…ç½®
            def update_config(base, update):
                for key, value in update.items():
                    if isinstance(value, dict) and key in base:
                        update_config(base[key], value)
                    else:
                        base[key] = value
            update_config(config, preset_config)
        
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.case:
        config['data']['case_name'] = args.case
    if args.episodes:
        config['training']['num_episodes'] = args.episodes
    if args.partitions:
        config['environment']['num_partitions'] = args.partitions
        
    print("ğŸš€ å¯åŠ¨ç”µåŠ›ç½‘ç»œåˆ†åŒºå¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
    print("=" * 60)
    print(f"é…ç½®ä¿¡æ¯:")
    print(f"  æ¡ˆä¾‹: {config['data']['case_name']}")
    print(f"  åˆ†åŒºæ•°: {config['environment']['num_partitions']}")
    print(f"  è®­ç»ƒå›åˆ: {config['training']['num_episodes']}")
    
    # ç³»ç»Ÿè®¾ç½®
    torch.manual_seed(config['system']['seed'])
    np.random.seed(config['system']['seed'])
    torch.set_num_threads(config['system']['num_threads'])
    device = setup_device(config['system']['device'])
    
    try:
        # 1. åŠ è½½å’Œå¤„ç†æ•°æ®
        print("\nğŸ“Š åŠ è½½å’Œå¤„ç†ç”µç½‘æ•°æ®...")
        mpc = load_power_grid_data(config['data']['case_name'])
        
        processor = PowerGridDataProcessor(
            normalize=config['data']['normalize'],
            cache_dir=config['data']['cache_dir']
        )
        hetero_data = processor.graph_from_mpc(mpc)
        hetero_data = hetero_data.to(device)
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {hetero_data}")
        
        # 2. åˆ›å»ºGATç¼–ç å™¨
        print("\nğŸ§  åˆ›å»ºGATç¼–ç å™¨...")
        gat_config = config['gat']
        encoder = create_hetero_graph_encoder(
            hetero_data,
            hidden_channels=gat_config['hidden_channels'],
            gnn_layers=gat_config['gnn_layers'],
            heads=gat_config['heads'],
            output_dim=gat_config['output_dim']
        ).to(device)

        with torch.no_grad():
            node_embeddings, attention_weights = encoder.encode_nodes_with_attention(hetero_data)

        total_nodes = sum(emb.shape[0] for emb in node_embeddings.values())
        print(f"âœ… èŠ‚ç‚¹åµŒå…¥è®¡ç®—å®Œæˆ: {total_nodes}ä¸ªèŠ‚ç‚¹")
        print(f"âœ… æ³¨æ„åŠ›æƒé‡æå–å®Œæˆ: {len(attention_weights)}ç§è¾¹ç±»å‹")
        
        # 3. åˆ›å»ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
        print("\nğŸŒ åˆ›å»ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒ...")
        env_config = config['environment']
        env = PowerGridPartitioningEnv(
            hetero_data=hetero_data,
            node_embeddings=node_embeddings,
            num_partitions=env_config['num_partitions'],
            reward_weights=env_config['reward_weights'],
            max_steps=env_config['max_steps'],
            device=device,
            attention_weights=attention_weights
        )
        
        print(f"âœ… ç¯å¢ƒåˆ›å»ºå®Œæˆ: {env.total_nodes}ä¸ªèŠ‚ç‚¹, {env.num_partitions}ä¸ªåˆ†åŒº")
        
        # 4. åˆ›å»ºPPOæ™ºèƒ½ä½“
        print("\nğŸ¤– åˆ›å»ºPPOæ™ºèƒ½ä½“...")
        agent_config = config['agent']
        node_embedding_dim = env.state_manager.embedding_dim
        region_embedding_dim = node_embedding_dim * 2

        print(f"   å¢å¼ºèŠ‚ç‚¹åµŒå…¥ç»´åº¦: {node_embedding_dim}")
        print(f"   åŒºåŸŸåµŒå…¥ç»´åº¦: {region_embedding_dim}")
        
        agent = PPOAgent(
            node_embedding_dim=node_embedding_dim,
            region_embedding_dim=region_embedding_dim,
            num_partitions=env.num_partitions,
            lr_actor=agent_config['lr_actor'],
            lr_critic=agent_config['lr_critic'],
            gamma=agent_config['gamma'],
            eps_clip=agent_config['eps_clip'],
            k_epochs=agent_config['k_epochs'],
            entropy_coef=agent_config['entropy_coef'],
            value_coef=agent_config['value_coef'],
            device=device
        )
        
        print(f"âœ… æ™ºèƒ½ä½“åˆ›å»ºå®Œæˆï¼Œå‚æ•°é‡: {sum(p.numel() for p in agent.actor.parameters()):,}")
        
        # 5. åˆ›å»ºç»Ÿä¸€è®­ç»ƒå™¨
        print("\nğŸ‹ï¸ è®¾ç½®ç»Ÿä¸€è®­ç»ƒå™¨...")
        trainer = UnifiedTrainer(agent=agent, env=env, config=config)
        
        # 6. è¿è¡Œè®­ç»ƒæˆ–è¯„ä¼°
        training_config = config['training']
        if args.eval_only:
            print("\nğŸ“ˆ è¿è¡Œè¯„ä¼°...")
            if args.resume:
                trainer.checkpoint_manager.load_checkpoint(agent, args.resume)
                print(f"ä»{args.resume}åŠ è½½æ¨¡å‹")
            eval_stats = trainer.evaluate()
            print("\nè¯„ä¼°ç»“æœ:")
            for key, value in eval_stats.items():
                print(f"  {key}: {value:.4f}")
        else:
            print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
            history = trainer.train(
                num_episodes=training_config['num_episodes'],
                max_steps_per_episode=training_config['max_steps_per_episode'],
                update_interval=training_config['update_interval'],
                resume_from=args.resume
            )
            
            print("\nè®­ç»ƒå®Œæˆï¼")
            print(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(history['episode_rewards'][-10:]):.3f}")
            
            # è¿è¡Œæœ€ç»ˆè¯„ä¼°
            print("\nğŸ“ˆ è¿è¡Œæœ€ç»ˆè¯„ä¼°...")
            eval_stats = trainer.evaluate()
            print("\næœ€ç»ˆè¯„ä¼°ç»“æœ:")
            for key, value in eval_stats.items():
                print(f"  {key}: {value:.4f}")
                
            # ç”Ÿæˆå¯è§†åŒ–
            trainer.run_final_visualization()
        
        # æ¸…ç†
        trainer.close()
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
        
    print("\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
    return 0


if __name__ == "__main__":
    sys.exit(main())
