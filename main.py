#!/usr/bin/env python3
"""
ç”µåŠ›ç½‘ç»œåˆ†åŒºå¼ºåŒ–å­¦ä¹ ç»Ÿä¸€è®­ç»ƒç³»ç»Ÿ

æ•´åˆäº†æ‰€æœ‰è®­ç»ƒæ¨¡å¼ï¼š
- æ ‡å‡†è®­ç»ƒ (IEEE 14, 30, 57èŠ‚ç‚¹)
- å¤§è§„æ¨¡è®­ç»ƒ (IEEE 118èŠ‚ç‚¹)
- å¹¶è¡Œè®­ç»ƒæ”¯æŒ
- åœºæ™¯ç”Ÿæˆè®­ç»ƒ
- è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ
- åŸºçº¿æ–¹æ³•å¯¹æ¯”
"""

import torch
import numpy as np
import argparse
import yaml
import os
import sys
import time
import json
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from collections import deque
import matplotlib.pyplot as plt
from tqdm import tqdm

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / 'src'))

# --- æ–°å¢ï¼šå…¨å±€ NaN/Inf å¼‚å¸¸æ£€æµ‹å¼€å…³ ---
# ä½¿ autograd åœ¨åå‘ä¼ æ’­ä¸­é‡åˆ° NaN æ—¶æä¾›æ›´è¯¦ç»†çš„å †æ ˆè·Ÿè¸ª
torch.autograd.set_detect_anomaly(True)

# å°†ç‰¹å®šçš„è¿è¡Œæ—¶è­¦å‘Š (RuntimeWarning) å‡çº§ä¸ºé”™è¯¯ï¼Œä½¿å…¶èƒ½è¢« try-except æ•è·
warnings.filterwarnings("error", message=".*NaN.*", category=RuntimeWarning)

# ç¦ç”¨å…¶ä»–è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning)

# å¯¼å…¥ç”µåŠ›ç³»ç»Ÿæ•°æ®åŠ è½½å‡½æ•°
try:
    from scipy.io import loadmat
except ImportError:
    print("âš ï¸ è­¦å‘Š: scipyæœªå®‰è£…ï¼Œæ— æ³•åŠ è½½MATPOWERæ–‡ä»¶")

try:
    import pandapower as pp
    import pandapower.networks as pn
    _pandapower_available = True
except ImportError:
    _pandapower_available = False
    print("âš ï¸ è­¦å‘Š: pandapoweræœªå®‰è£…ï¼Œæ— æ³•åŠ è½½IEEEæ ‡å‡†æµ‹è¯•ç³»ç»Ÿ")

# åŠ¨æ€å¯¼å…¥æ£€æŸ¥
def check_dependencies():
    """æ£€æŸ¥å¯é€‰ä¾èµ–"""
    deps = {
        'stable_baselines3': False,
        'tensorboard': False,
        'plotly': False,
        'networkx': False
    }

    try:
        import stable_baselines3
        deps['stable_baselines3'] = True
    except ImportError:
        pass

    try:
        from torch.utils.tensorboard import SummaryWriter
        deps['tensorboard'] = True
    except ImportError:
        pass

    try:
        import plotly.graph_objects as go
        deps['plotly'] = True
    except ImportError:
        pass

    try:
        import networkx as nx
        deps['networkx'] = True
    except ImportError:
        pass

    return deps


def load_power_grid_data(case_name: str) -> Dict:
    """
    åŠ è½½ç”µåŠ›ç½‘ç»œæ•°æ®

    Args:
        case_name: æ¡ˆä¾‹åç§° (ieee14, ieee30, ieee57, ieee118)

    Returns:
        MATPOWERæ ¼å¼çš„ç”µç½‘æ•°æ®å­—å…¸
    """
    # é¦–å…ˆå°è¯•ä»PandaPoweråŠ è½½
    if _pandapower_available:
        try:
            return load_from_pandapower(case_name)
        except Exception as e:
            print(f"âš ï¸ PandaPoweråŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°å†…ç½®æ•°æ®...")

    # å›é€€åˆ°å†…ç½®çš„IEEEæ ‡å‡†æµ‹è¯•ç³»ç»Ÿæ•°æ®
    ieee_cases = {
        'ieee14': {
            'baseMVA': 100.0,
            'bus': np.array([
                [1, 3, 0, 0, 0, 0, 1, 1.06, 0, 345, 1, 1.06, 0.94],
                [2, 2, 21.7, 12.7, 0, 0, 1, 1.045, -4.98, 345, 1, 1.06, 0.94],
                [3, 2, 94.2, 19, 0, 0, 1, 1.01, -12.72, 345, 1, 1.06, 0.94],
                [4, 1, 47.8, -3.9, 0, 0, 1, 1.019, -10.33, 345, 1, 1.06, 0.94],
                [5, 1, 7.6, 1.6, 0, 0, 1, 1.02, -8.78, 345, 1, 1.06, 0.94],
                [6, 2, 11.2, 7.5, 0, 12.2, 1, 1.07, -14.22, 345, 1, 1.06, 0.94],
                [7, 1, 0, 0, 0, 0, 1, 1.062, -13.37, 345, 1, 1.06, 0.94],
                [8, 2, 0, 0, 0, 17.4, 1, 1.09, -13.36, 345, 1, 1.06, 0.94],
                [9, 1, 29.5, 16.6, 0, 0, 1, 1.056, -14.94, 345, 1, 1.06, 0.94],
                [10, 1, 9, 5.8, 0, 0, 1, 1.051, -15.1, 345, 1, 1.06, 0.94],
                [11, 1, 3.5, 1.8, 0, 0, 1, 1.057, -14.79, 345, 1, 1.06, 0.94],
                [12, 1, 6.1, 1.6, 0, 0, 1, 1.055, -15.07, 345, 1, 1.06, 0.94],
                [13, 1, 13.5, 5.8, 0, 0, 1, 1.05, -15.16, 345, 1, 1.06, 0.94],
                [14, 1, 14.9, 5, 0, 0, 1, 1.036, -16.04, 345, 1, 1.06, 0.94]
            ]),
            'branch': np.array([
                [1, 2, 0.01938, 0.05917, 0.0528, 0, 0, 0, 0, 0, 1, -360, 360],
                [1, 5, 0.05403, 0.22304, 0.0492, 0, 0, 0, 0, 0, 1, -360, 360],
                [2, 3, 0.04699, 0.19797, 0.0438, 0, 0, 0, 0, 0, 1, -360, 360],
                [2, 4, 0.05811, 0.17632, 0.034, 0, 0, 0, 0, 0, 1, -360, 360],
                [2, 5, 0.05695, 0.17388, 0.0346, 0, 0, 0, 0, 0, 1, -360, 360],
                [3, 4, 0.06701, 0.17103, 0.0128, 0, 0, 0, 0, 0, 1, -360, 360],
                [4, 5, 0.01335, 0.04211, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [4, 7, 0, 0.20912, 0, 0.978, 0, 0, 0, 0, 1, -360, 360],
                [4, 9, 0, 0.55618, 0, 0.969, 0, 0, 0, 0, 1, -360, 360],
                [5, 6, 0, 0.25202, 0, 0.932, 0, 0, 0, 0, 1, -360, 360],
                [6, 11, 0.09498, 0.1989, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [6, 12, 0.12291, 0.25581, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [6, 13, 0.06615, 0.13027, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [7, 8, 0, 0.17615, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [7, 9, 0, 0.11001, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [9, 10, 0.03181, 0.0845, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [9, 14, 0.12711, 0.27038, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [10, 11, 0.08205, 0.19207, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [12, 13, 0.22092, 0.19988, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [13, 14, 0.17093, 0.34802, 0, 0, 0, 0, 0, 0, 1, -360, 360]
            ])
        }
    }

    if case_name in ieee_cases:
        return ieee_cases[case_name]
    else:
        # å¯¹äºå…¶ä»–æ¡ˆä¾‹ï¼Œè¿”å›é»˜è®¤çš„IEEE14æ•°æ®
        print(f"âš ï¸ æ¡ˆä¾‹ {case_name} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨IEEE14é»˜è®¤æ•°æ®")
        return ieee_cases['ieee14']


def load_from_pandapower(case_name: str) -> Dict:
    """
    ä»PandaPoweråŠ è½½IEEEæ ‡å‡†æµ‹è¯•ç³»ç»Ÿ

    Args:
        case_name: æ¡ˆä¾‹åç§° (ieee14, ieee30, ieee57, ieee118)

    Returns:
        MATPOWERæ ¼å¼çš„ç”µç½‘æ•°æ®å­—å…¸
    """
    print(f"ğŸ”Œ ä»PandaPoweråŠ è½½ {case_name.upper()} æ•°æ®...")

    # æ˜ å°„æ¡ˆä¾‹åç§°åˆ°PandaPowerå‡½æ•°
    case_mapping = {
        'ieee14': pn.case14,
        'ieee30': pn.case30,
        'ieee57': pn.case57,
        'ieee118': pn.case118
    }

    if case_name not in case_mapping:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¡ˆä¾‹: {case_name}")

    # åŠ è½½PandaPowerç½‘ç»œ
    net = case_mapping[case_name]()
    print(f"âœ… æˆåŠŸåŠ è½½ {case_name.upper()}: {len(net.bus)} èŠ‚ç‚¹, {len(net.line)} çº¿è·¯")

    # è½¬æ¢ä¸ºMATPOWERæ ¼å¼
    mpc = convert_pandapower_to_matpower(net)

    return mpc


def convert_pandapower_to_matpower(net) -> Dict:
    """
    å°†PandaPowerç½‘ç»œè½¬æ¢ä¸ºMATPOWERæ ¼å¼

    Args:
        net: PandaPowerç½‘ç»œå¯¹è±¡

    Returns:
        MATPOWERæ ¼å¼çš„ç”µç½‘æ•°æ®å­—å…¸
    """
    # åŸºç¡€MVA
    baseMVA = net.sn_mva

    # èŠ‚ç‚¹æ•°æ®è½¬æ¢
    bus_data = []
    for idx, bus in net.bus.iterrows():
        # MATPOWER busæ ¼å¼: [bus_i, type, Pd, Qd, Gs, Bs, area, Vm, Va, baseKV, zone, Vmax, Vmin]
        bus_type = 1  # PQèŠ‚ç‚¹

        # æ£€æŸ¥æ˜¯å¦ä¸ºå‘ç”µæœºèŠ‚ç‚¹
        if idx in net.gen.bus.values:
            gen_at_bus = net.gen[net.gen.bus == idx]
            if not gen_at_bus.empty:
                # æ£€æŸ¥æ˜¯å¦ä¸ºslackèŠ‚ç‚¹
                if any(gen_at_bus.slack):
                    bus_type = 3  # slackèŠ‚ç‚¹
                else:
                    bus_type = 2  # PVèŠ‚ç‚¹

        # è·å–è´Ÿè·æ•°æ®
        pd = qd = 0
        if idx in net.load.bus.values:
            load_at_bus = net.load[net.load.bus == idx]
            pd = load_at_bus.p_mw.sum()
            qd = load_at_bus.q_mvar.sum()

        # è·å–ç”µå‹ç­‰çº§
        vn_kv = bus.vn_kv if hasattr(bus, 'vn_kv') else 345.0  # é»˜è®¤345kV

        bus_row = [
            idx + 1,  # bus number (1-indexed)
            bus_type,  # bus type
            pd,  # Pd (MW)
            qd,  # Qd (MVAr)
            0,  # Gs (MW)
            0,  # Bs (MVAr)
            1,  # area
            1.0,  # Vm (p.u.)
            0,  # Va (degrees)
            vn_kv,  # baseKV
            1,  # zone
            1.1,  # Vmax
            0.9   # Vmin
        ]
        bus_data.append(bus_row)

    # çº¿è·¯æ•°æ®è½¬æ¢
    branch_data = []
    for idx, line in net.line.iterrows():
        # è·å–çº¿è·¯ç”µå‹ç­‰çº§ï¼ˆä»è¿æ¥çš„èŠ‚ç‚¹è·å–ï¼‰
        from_bus_vn = net.bus.loc[line.from_bus, 'vn_kv'] if line.from_bus in net.bus.index else 345.0

        # MATPOWER branchæ ¼å¼: [fbus, tbus, r, x, b, rateA, rateB, rateC, ratio, angle, status, angmin, angmax]
        # ç®€åŒ–çš„é˜»æŠ—è®¡ç®—
        r_pu = line.r_ohm_per_km * line.length_km / (from_bus_vn**2 / baseMVA) if hasattr(line, 'r_ohm_per_km') else 0.01
        x_pu = line.x_ohm_per_km * line.length_km / (from_bus_vn**2 / baseMVA) if hasattr(line, 'x_ohm_per_km') else 0.05
        b_pu = 0.0  # ç®€åŒ–å¤„ç†
        rate_a = line.max_i_ka * from_bus_vn * np.sqrt(3) / 1000 if hasattr(line, 'max_i_ka') else 100.0

        branch_row = [
            line.from_bus + 1,  # from bus (1-indexed)
            line.to_bus + 1,    # to bus (1-indexed)
            r_pu,  # r (p.u.)
            x_pu,  # x (p.u.)
            b_pu,  # b (p.u.)
            rate_a,  # rateA (MVA)
            0,  # rateB
            0,  # rateC
            0,  # ratio
            0,  # angle
            1,  # status
            -360,  # angmin
            360    # angmax
        ]
        branch_data.append(branch_row)

    # å˜å‹å™¨æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
    for idx, trafo in net.trafo.iterrows():
        # ç®€åŒ–çš„å˜å‹å™¨æ¨¡å‹
        branch_row = [
            trafo.hv_bus + 1,  # from bus (1-indexed)
            trafo.lv_bus + 1,  # to bus (1-indexed)
            trafo.vk_percent / 100 * (trafo.vn_hv_kv**2 / baseMVA),  # r (p.u.)
            trafo.vkr_percent / 100 * (trafo.vn_hv_kv**2 / baseMVA),  # x (p.u.)
            0,  # b (p.u.)
            trafo.sn_mva,  # rateA (MVA)
            0,  # rateB
            0,  # rateC
            trafo.vn_hv_kv / trafo.vn_lv_kv,  # ratio
            0,  # angle
            1,  # status
            -360,  # angmin
            360    # angmax
        ]
        branch_data.append(branch_row)

    return {
        'baseMVA': baseMVA,
        'bus': np.array(bus_data),
        'branch': np.array(branch_data)
    }


class TrainingLogger:
    """å¢å¼ºçš„è®­ç»ƒæ—¥å¿—è®°å½•å™¨"""

    def __init__(self, config: Dict[str, Any], total_episodes: int):
        self.config = config
        self.best_reward = -float('inf')
        self.episode_rewards = []
        self.episode_lengths = []
        self.actor_losses = []
        self.critic_losses = []
        self.entropies = []
        self.success_rates = []
        self.load_cvs = []
        self.coupling_edges = []

        # æ—¥å¿—é…ç½®
        self.log_config = config.get('logging', {})
        self.metrics_save_interval = self.log_config.get('metrics_save_interval', 50)

        # TensorBoardè®¾ç½®
        self.use_tensorboard = self.log_config.get('use_tensorboard', False)
        self.tensorboard_writer = None
        if self.use_tensorboard:
            self._setup_tensorboard()

        # TQDM è¿›åº¦æ¡
        self.progress_bar = tqdm(total=total_episodes, desc="ğŸš€ è®­ç»ƒè¿›åº¦",
                                 bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]")
        self.start_time = time.time()

    def _setup_tensorboard(self):
        """è®¾ç½®TensorBoard"""
        try:
            from torch.utils.tensorboard import SummaryWriter
            log_dir = self.log_config.get('log_dir', 'logs')
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            self.tensorboard_writer = SummaryWriter(f"{log_dir}/training_{timestamp}")
            print(f"ğŸ“Š TensorBoardæ—¥å¿—ç›®å½•: {log_dir}/training_{timestamp}")
        except ImportError:
            print("âš ï¸ TensorBoardä¸å¯ç”¨ï¼Œè·³è¿‡TensorBoardæ—¥å¿—")
            self.use_tensorboard = False

    def log_episode(self, episode: int, reward: float, length: int, info: Dict = None):
        """è®°å½•å•ä¸ªå›åˆ"""
        self.episode_rewards.append(reward)
        self.episode_lengths.append(length)
        
        if reward > self.best_reward:
            self.best_reward = reward

        # æ›´æ–°è¿›åº¦æ¡
        self.progress_bar.update(1)
        self.progress_bar.set_postfix({
            "å¥–åŠ±": f"{reward:.2f}",
            "æœ€ä½³": f"{self.best_reward:.2f}"
        })

        # è®°å½•é¢å¤–ä¿¡æ¯
        if info:
            if 'success' in info:
                self.success_rates.append(1.0 if info['success'] else 0.0)
            if 'load_cv' in info:
                self.load_cvs.append(info['load_cv'])
            if 'coupling_edges' in info:
                self.coupling_edges.append(info['coupling_edges'])

        # TensorBoardæ—¥å¿—
        if self.use_tensorboard and self.tensorboard_writer:
            self.tensorboard_writer.add_scalar('Episode/Reward', reward, episode)
            self.tensorboard_writer.add_scalar('Episode/Length', length, episode)
            if info:
                for key, value in info.items():
                    if isinstance(value, (int, float)):
                        self.tensorboard_writer.add_scalar(f'Episode/{key}', value, episode)

    def log_training_step(self, episode: int, actor_loss: float = None,
                         critic_loss: float = None, entropy: float = None):
        """è®°å½•è®­ç»ƒæ­¥éª¤"""
        if actor_loss is not None:
            self.actor_losses.append(actor_loss)
            if self.use_tensorboard and self.tensorboard_writer:
                self.tensorboard_writer.add_scalar('Training/ActorLoss', actor_loss, episode)

        if critic_loss is not None:
            self.critic_losses.append(critic_loss)
            if self.use_tensorboard and self.tensorboard_writer:
                self.tensorboard_writer.add_scalar('Training/CriticLoss', critic_loss, episode)

        if entropy is not None:
            self.entropies.append(entropy)
            if self.use_tensorboard and self.tensorboard_writer:
                self.tensorboard_writer.add_scalar('Training/Entropy', entropy, episode)

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        if not self.episode_rewards:
            return {}

        stats = {
            'total_episodes': len(self.episode_rewards),
            'best_reward': self.best_reward,
            'mean_reward': np.mean(self.episode_rewards),
            'std_reward': np.std(self.episode_rewards),
            'mean_length': np.mean(self.episode_lengths),
            'training_time': time.time() - self.start_time
        }

        if self.success_rates:
            stats['success_rate'] = np.mean(self.success_rates)
        if self.load_cvs:
            stats['mean_load_cv'] = np.mean(self.load_cvs)
        if self.actor_losses:
            stats['mean_actor_loss'] = np.mean(self.actor_losses)
        if self.critic_losses:
            stats['mean_critic_loss'] = np.mean(self.critic_losses)

        return stats

    def close(self):
        """å…³é—­æ—¥å¿—è®°å½•å™¨"""
        self.progress_bar.close()
        if self.tensorboard_writer:
            self.tensorboard_writer.close()


class UnifiedTrainer:
    """ç»Ÿä¸€è®­ç»ƒå™¨"""

    def __init__(self, agent, env, config):
        self.agent = agent
        self.env = env
        self.config = config
        # å¤„ç†è®¾å¤‡é…ç½®
        device_config = config['system']['device']
        if device_config == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device_config)
        self.logger = None # å°†åœ¨trainæ–¹æ³•ä¸­åˆå§‹åŒ–

    def train(self, num_episodes: int, max_steps_per_episode: int, update_interval: int = 10):
        """è®­ç»ƒæ™ºèƒ½ä½“"""
        self.logger = TrainingLogger(self.config, num_episodes)
        
        print(f"ğŸ“Š ç›‘æ§ä¿¡æ¯:")
        print(f"   - TensorBoard: {'âœ…' if self.logger.use_tensorboard else 'âŒ'}")
        print(f"   - æŒ‡æ ‡ä¿å­˜é—´éš”: {self.logger.metrics_save_interval} å›åˆ")

        for episode in range(num_episodes):
            state, _ = self.env.reset()  # è§£åŒ…å…ƒç»„
            episode_reward = 0
            episode_length = 0
            episode_info = {}

            for step in range(max_steps_per_episode):
                # é€‰æ‹©åŠ¨ä½œ
                action, log_prob, value = self.agent.select_action(state)

                if action is None:  # æ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œ
                    break

                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, terminated, truncated, info = self.env.step(action)
                done = terminated or truncated

                # å­˜å‚¨ç»éªŒ
                self.agent.store_experience(state, action, reward, log_prob, value, done)

                episode_reward += reward
                episode_length += 1
                state = next_state

                # æ”¶é›†å›åˆä¿¡æ¯
                if info:
                    episode_info.update(info)

                if done:
                    break

            # è®°å½•åˆ°loggerï¼ˆåŒ…å«è¯¦ç»†ä¿¡æ¯ï¼‰
            self.logger.log_episode(episode, episode_reward, episode_length, episode_info)

            # æ›´æ–°æ™ºèƒ½ä½“å¹¶è®°å½•è®­ç»ƒæŒ‡æ ‡
            if episode % update_interval == 0 and episode > 0:
                try:
                    # --- ä½¿ç”¨ try-except åŒ…è£¹ agent.update() ---
                    training_stats = self.agent.update()
                    if training_stats:
                        self.logger.log_training_step(
                            episode,
                            training_stats.get('actor_loss'),
                            training_stats.get('critic_loss'),
                            training_stats.get('entropy')
                        )

                except RuntimeError as e:
                    if "[NaNGuard]" in str(e) or "NaN" in str(e):
                        print(f"\nâŒ [NaN Dumper] æ•è·åˆ°è‡´å‘½é”™è¯¯: {e}")
                        
                        dump_dir = "diagnostics"
                        os.makedirs(dump_dir, exist_ok=True)
                        dump_path = os.path.join(dump_dir, f"nan_dump_episode_{episode}.pt")
                        
                        print(f"  ... æ­£åœ¨ä¿å­˜è¯Šæ–­ä¿¡æ¯åˆ°: {dump_path}")

                        # æ”¶é›†è§¦å‘å¼‚å¸¸çš„æ‰¹é‡æ•°æ®
                        batch_data = {
                            'states': self.agent.memory.states,
                            'actions': self.agent.memory.actions,
                            'logprobs': self.agent.memory.logprobs,
                            'rewards': self.agent.memory.rewards,
                            'is_terminals': self.agent.memory.is_terminals,
                        }

                        torch.save({
                            "error_message": str(e),
                            "episode": episode,
                            "actor_state_dict": self.agent.actor.state_dict(),
                            "critic_state_dict": self.agent.critic.state_dict(),
                            "actor_optimizer_state_dict": self.agent.actor_optimizer.state_dict(),
                            "critic_optimizer_state_dict": self.agent.critic_optimizer.state_dict(),
                            "triggering_batch": batch_data,
                        }, dump_path)
                        
                        print("  ... ä¿å­˜å®Œæˆã€‚æ­£åœ¨ä¸­æ–­è®­ç»ƒã€‚")
                        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä»¥æ­£å¸¸é€€å‡ºè®­ç»ƒå¾ªç¯
                        raise e
                    else:
                        # å¦‚æœæ˜¯å…¶ä»–è¿è¡Œæ—¶é”™è¯¯ï¼Œåˆ™ç›´æ¥æŠ›å‡º
                        raise

            # ä¿å­˜ä¸­é—´ç»“æœ
            if episode % self.logger.metrics_save_interval == 0 and episode > 0:
                self._save_intermediate_results(episode)

        # è®­ç»ƒå®Œæˆç»Ÿè®¡
        final_stats = self.logger.get_statistics()
        print(f"\nğŸ¯ è®­ç»ƒå®Œæˆç»Ÿè®¡:")
        print(f"   - æ€»å›åˆæ•°: {final_stats.get('total_episodes', 0)}")
        print(f"   - æœ€ä½³å¥–åŠ±: {final_stats.get('best_reward', 0):.4f}")
        print(f"   - å¹³å‡å¥–åŠ±: {final_stats.get('mean_reward', 0):.4f}")
        print(f"   - è®­ç»ƒæ—¶é—´: {final_stats.get('training_time', 0)/60:.1f} åˆ†é’Ÿ")
        if 'success_rate' in final_stats:
            print(f"   - æˆåŠŸç‡: {final_stats['success_rate']:.3f}")

        return {
            'episode_rewards': self.logger.episode_rewards,
            'episode_lengths': self.logger.episode_lengths,
            'training_stats': final_stats
        }

    def _save_intermediate_results(self, episode: int):
        """ä¿å­˜ä¸­é—´è®­ç»ƒç»“æœ"""
        try:
            stats = self.logger.get_statistics()
            checkpoint_dir = Path(self.config['logging']['checkpoint_dir'])
            checkpoint_dir.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats_file = checkpoint_dir / f"training_stats_episode_{episode}.json"
            with open(stats_file, 'w') as f:
                json.dump(stats, f, indent=2)

            print(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: episode {episode}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ä¸­é—´ç»“æœå¤±è´¥: {e}")

    def evaluate(self, num_episodes: int = 10):
        """è¯„ä¼°æ™ºèƒ½ä½“"""
        print(f"ğŸ” è¯„ä¼°æ™ºèƒ½ä½“ {num_episodes} è½®...")

        eval_rewards = []
        success_count = 0

        for episode in range(num_episodes):
            state, _ = self.env.reset()  # è§£åŒ…å…ƒç»„
            episode_reward = 0

            for step in range(200):  # æœ€å¤§æ­¥æ•°
                action, _, _ = self.agent.select_action(state, training=False)

                if action is None:  # æ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œ
                    break

                state, reward, terminated, truncated, info = self.env.step(action)
                done = terminated or truncated
                episode_reward += reward

                if done:
                    if info.get('success', False):
                        success_count += 1
                    break

            eval_rewards.append(episode_reward)

        return {
            'avg_reward': np.mean(eval_rewards),
            'success_rate': success_count / num_episodes,
            'rewards': eval_rewards
        }

    def run_final_visualization(self):
        """è¿è¡Œæœ€ç»ˆå¯è§†åŒ–"""
        try:
            from code.src.visualization import VisualizationManager
            viz = VisualizationManager(self.config)
            viz.visualize_partition(self.env, title="Final Partition")
            print("âœ… å¯è§†åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ å¯è§†åŒ–å¤±è´¥: {e}")

    def close(self):
        """æ¸…ç†èµ„æº"""
        if self.logger:
            self.logger.close()


class UnifiedTrainingSystem:
    """ç»Ÿä¸€è®­ç»ƒç³»ç»Ÿ - æ•´åˆæ‰€æœ‰è®­ç»ƒæ¨¡å¼"""
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–ç»Ÿä¸€è®­ç»ƒç³»ç»Ÿ"""
        self.deps = check_dependencies()
        self.config = self._load_config(config_path)
        self.device = self._setup_device()
        self.setup_directories()
        
    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        # å¦‚æœæ²¡æœ‰æŒ‡å®šé…ç½®æ–‡ä»¶ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤çš„ config_unified.yaml
        if not config_path:
            default_config_path = 'config.yaml'
            if os.path.exists(default_config_path):
                config_path = default_config_path
                print(f"ğŸ“„ ä½¿ç”¨é»˜è®¤é…ç½®æ–‡ä»¶: {config_path}")
        
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
                print(f"ğŸ“Š æ¡ˆä¾‹åç§°: {config['data']['case_name']}")
                return config
        else:
            print("âš ï¸ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._create_default_config()
    
    def _create_default_config(self) -> Dict[str, Any]:
        """åˆ›å»ºé»˜è®¤é…ç½®"""
        return {
            'system': {
                'name': 'unified_power_grid_partitioning',
                'version': '2.0',
                'device': 'auto',
                'seed': 42,
                'num_threads': 1
            },
            'data': {
                'case_name': 'ieee14',
                'normalize': True,
                'cache_dir': 'cache'
            },
            'training': {
                'mode': 'standard',  # standard, parallel, curriculum, large_scale
                'num_episodes': 1000,
                'max_steps_per_episode': 200,
                'update_interval': 10,
                'save_interval': 100,
                'eval_interval': 50
            },
            'environment': {
                'num_partitions': 3,
                'max_steps': 200,
                'reward_weights': {
                    'load_balance': 0.4,
                    'electrical_decoupling': 0.4,
                    'power_balance': 0.2
                }
            },
            'gat': {
                'hidden_channels': 64,
                'gnn_layers': 3,
                'heads': 4,
                'output_dim': 128,
                'dropout': 0.1
            },
            'agent': {
                'type': 'ppo',  # ppo, sb3_ppo
                'lr_actor': 3e-5,  # é™ä½10å€ç”¨äºæ•°å€¼ç¨³å®š
                'lr_critic': 1e-4,  # é™ä½10å€ç”¨äºæ•°å€¼ç¨³å®š
                'gamma': 0.99,
                'eps_clip': 0.2,
                'k_epochs': 4,
                'entropy_coef': 0.01,
                'value_coef': 0.5,
                'hidden_dim': 256
            },
            'parallel_training': {
                'enabled': False,
                'num_cpus': 12,
                'total_timesteps': 5_000_000,
                'scenario_generation': True
            },
            'scenario_generation': {
                'enabled': False,
                'perturb_prob': 0.8,
                'perturb_types': ['n-1', 'load_gen_fluctuation', 'both', 'none'],
                'scale_range': [0.8, 1.2]
            },
            'curriculum': {
                'enabled': False,
                'start_partitions': 2,
                'end_partitions': 5,
                'episodes_per_stage': 200
            },
            'evaluation': {
                'num_episodes': 20,
                'include_baselines': True,
                'baseline_methods': ['spectral', 'kmeans', 'random']
            },
            'visualization': {
                'enabled': True,
                'save_figures': True,
                'figures_dir': 'figures',
                'interactive': True
            },
            'logging': {
                'use_tensorboard': True,
                'log_dir': 'logs',
                'checkpoint_dir': 'checkpoints',
                'console_log_interval': 10,
                'metrics_save_interval': 50
            }
        }
    
    def _setup_device(self) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        device_config = self.config['system'].get('device', 'auto')
        if device_config == 'auto':
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            device = torch.device(device_config)
        
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
        return device
    
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        dirs = [
            self.config['data']['cache_dir'],
            self.config['logging']['log_dir'], 
            self.config['logging']['checkpoint_dir'],
            self.config['visualization']['figures_dir'],
            'models', 'output', 'experiments'
        ]
        
        for dir_path in dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    def get_training_configs(self) -> Dict[str, Dict[str, Any]]:
        """è·å–ä¸åŒè®­ç»ƒæ¨¡å¼çš„é…ç½®"""
        base_config = self.config.copy()
        
        configs = {
            'quick': {
                **base_config,
                'training': {
                    **base_config['training'],
                    'num_episodes': 100,
                    'max_steps_per_episode': 50,
                    'update_interval': 5
                }
            },
            'standard': base_config,
            'full': {
                **base_config,
                'training': {
                    **base_config['training'],
                    'num_episodes': 2000,
                    'max_steps_per_episode': 500,
                    'update_interval': 20
                }
            },
            'ieee118': {
                **base_config,
                'data': {
                    **base_config['data'],
                    'case_name': 'ieee118'
                },
                'environment': {
                    **base_config['environment'],
                    'num_partitions': 8,
                    'max_steps': 500
                },
                'gat': {
                    **base_config['gat'],
                    'hidden_channels': 128,
                    'gnn_layers': 4,
                    'heads': 8,
                    'output_dim': 256
                },
                'training': {
                    **base_config['training'],
                    'num_episodes': 5000,
                    'max_steps_per_episode': 500
                },
                'parallel_training': {
                    **base_config['parallel_training'],
                    'enabled': True
                },
                'scenario_generation': {
                    **base_config['scenario_generation'],
                    'enabled': True
                }
            },
            'parallel': {
                **base_config,
                'parallel_training': {
                    **base_config['parallel_training'],
                    'enabled': True
                },
                'scenario_generation': {
                    **base_config['scenario_generation'],
                    'enabled': True
                }
            },
            'curriculum': {
                **base_config,
                'curriculum': {
                    **base_config['curriculum'],
                    'enabled': True
                }
            }
        }
        
        return configs
    
    def run_training(self, mode: str = 'standard', **kwargs) -> Dict[str, Any]:
        """è¿è¡Œè®­ç»ƒ"""
        print(f"\nğŸš€ å¼€å§‹{mode.upper()}æ¨¡å¼è®­ç»ƒ")
        print("=" * 60)
        
        # è·å–æ¨¡å¼é…ç½®
        configs = self.get_training_configs()
        if mode not in configs:
            print(f"âš ï¸ æœªçŸ¥è®­ç»ƒæ¨¡å¼: {mode}ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å¼")
            mode = 'standard'
        
        config = configs[mode]
        
        # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
        for key, value in kwargs.items():
            if '.' in key:
                # æ”¯æŒåµŒå¥—é…ç½®ï¼Œå¦‚ training.num_episodes
                keys = key.split('.')
                current = config
                for k in keys[:-1]:
                    if k not in current:
                        current[k] = {}
                    current = current[k]
                current[keys[-1]] = value
            else:
                config[key] = value
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(config['system']['seed'])
        np.random.seed(config['system']['seed'])
        
        try:
            if mode == 'parallel' or config['parallel_training']['enabled']:
                return self._run_parallel_training(config)
            elif mode == 'curriculum' or config['curriculum']['enabled']:
                return self._run_curriculum_training(config)
            else:
                return self._run_standard_training(config)
                
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}
    
    def _run_standard_training(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œæ ‡å‡†è®­ç»ƒ"""
        print("ğŸ“Š æ ‡å‡†è®­ç»ƒæ¨¡å¼")

        # å¯¼å…¥å¿…è¦æ¨¡å—
        from code.src.data_processing import PowerGridDataProcessor
        from code.src.gat import create_hetero_graph_encoder
        from code.src.rl.environment import PowerGridPartitioningEnv
        from code.src.rl.agent import PPOAgent

        # 1. æ•°æ®å¤„ç†
        print("\n1ï¸âƒ£ æ•°æ®å¤„ç†...")
        processor = PowerGridDataProcessor(
            normalize=config['data']['normalize'],
            cache_dir=config['data']['cache_dir']
        )

        # åŠ è½½æ•°æ®
        mpc = load_power_grid_data(config['data']['case_name'])
        
        hetero_data = processor.graph_from_mpc(mpc).to(self.device)
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {hetero_data}")
        
        # 2. GATç¼–ç å™¨
        print("\n2ï¸âƒ£ GATç¼–ç å™¨...")
        gat_config = config['gat']
        encoder = create_hetero_graph_encoder(
            hetero_data,
            hidden_channels=gat_config['hidden_channels'],
            gnn_layers=gat_config['gnn_layers'],
            heads=gat_config['heads'],
            output_dim=gat_config['output_dim']
        ).to(self.device)
        
        with torch.no_grad():
            node_embeddings, attention_weights = encoder.encode_nodes_with_attention(hetero_data)
        
        print(f"âœ… ç¼–ç å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # 3. ç¯å¢ƒ
        print("\n3ï¸âƒ£ å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ...")
        env_config = config['environment']
        env = PowerGridPartitioningEnv(
            hetero_data=hetero_data,
            node_embeddings=node_embeddings,
            num_partitions=env_config['num_partitions'],
            reward_weights=env_config['reward_weights'],
            max_steps=env_config['max_steps'],
            device=self.device,
            attention_weights=attention_weights
        )
        
        print(f"âœ… ç¯å¢ƒåˆ›å»ºå®Œæˆ: {env.total_nodes}èŠ‚ç‚¹, {env.num_partitions}åˆ†åŒº")
        
        # 4. æ™ºèƒ½ä½“
        print("\n4ï¸âƒ£ PPOæ™ºèƒ½ä½“...")
        agent_config = config['agent']
        node_embedding_dim = env.state_manager.embedding_dim
        region_embedding_dim = node_embedding_dim * 2
        
        agent = PPOAgent(
            node_embedding_dim=node_embedding_dim,
            region_embedding_dim=region_embedding_dim,
            num_partitions=env.num_partitions,
            lr_actor=agent_config['lr_actor'],
            lr_critic=agent_config['lr_critic'],
            gamma=agent_config['gamma'],
            eps_clip=agent_config['eps_clip'],
            k_epochs=agent_config['k_epochs'],
            entropy_coef=agent_config['entropy_coef'],
            value_coef=agent_config['value_coef'],
            device=self.device,
            max_grad_norm=agent_config.get('max_grad_norm', None),
            # ã€ä¿®æ”¹ã€‘ä¼ é€’ç‹¬ç«‹çš„è°ƒåº¦å™¨é…ç½®
            actor_scheduler_config=agent_config.get('actor_scheduler', {}),
            critic_scheduler_config=agent_config.get('critic_scheduler', {})
        )
        
        print(f"âœ… æ™ºèƒ½ä½“åˆ›å»ºå®Œæˆ")
        
        # 5. è®­ç»ƒ
        print("\n5ï¸âƒ£ å¼€å§‹è®­ç»ƒ...")
        trainer = UnifiedTrainer(agent=agent, env=env, config=config)
        
        training_config = config['training']
        history = trainer.train(
            num_episodes=training_config['num_episodes'],
            max_steps_per_episode=training_config['max_steps_per_episode'],
            update_interval=training_config['update_interval']
        )
        
        # 6. è¯„ä¼°
        print("\n6ï¸âƒ£ è¯„ä¼°...")
        eval_stats = trainer.evaluate()
        
        # 7. åŸºçº¿å¯¹æ¯”
        baseline_results = None
        if config['evaluation']['include_baselines']:
            print("\n7ï¸âƒ£ åŸºçº¿æ–¹æ³•å¯¹æ¯”...")
            try:
                from code.baseline import run_baseline_comparison
                baseline_results = run_baseline_comparison(env, agent, seed=42)
                print("âœ… åŸºçº¿å¯¹æ¯”å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ åŸºçº¿å¯¹æ¯”å¤±è´¥: {e}")
        
        # 8. å¯è§†åŒ–
        if config['visualization']['enabled']:
            print("\n8ï¸âƒ£ ç”Ÿæˆå¯è§†åŒ–...")
            try:
                trainer.run_final_visualization()
                if baseline_results is not None and config['visualization']['interactive']:
                    from code.src.visualization import run_interactive_visualization
                    run_interactive_visualization(env, baseline_results)
            except Exception as e:
                print(f"âš ï¸ å¯è§†åŒ–å¤±è´¥: {e}")
        
        trainer.close()
        
        return {
            'success': True,
            'mode': 'standard',
            'config': config,
            'history': history,
            'eval_stats': eval_stats,
            'baseline_results': baseline_results,
            'best_reward': trainer.logger.best_reward
        }
    
    def _run_parallel_training(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå¹¶è¡Œè®­ç»ƒ"""
        print("ğŸŒ å¹¶è¡Œè®­ç»ƒæ¨¡å¼")
        
        if not self.deps['stable_baselines3']:
            print("âŒ å¹¶è¡Œè®­ç»ƒéœ€è¦stable-baselines3ï¼Œè¯·å®‰è£…ï¼špip install stable-baselines3")
            return {'success': False, 'error': 'Missing stable-baselines3'}
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰gymå’Œstable_baselines3
            try:
                import gym
                from stable_baselines3 import PPO
                has_sb3 = True
            except ImportError:
                has_sb3 = False

            if has_sb3:
                # ä½¿ç”¨Stable-Baselines3çš„å¹¶è¡Œè®­ç»ƒ
                return self._run_sb3_parallel_training(config)
            else:
                # ä½¿ç”¨ç®€åŒ–çš„å¤šè¿›ç¨‹è®­ç»ƒ
                return self._run_simple_parallel_training(config)

        except Exception as e:
            print(f"âŒ å¹¶è¡Œè®­ç»ƒå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _run_sb3_parallel_training(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨Stable-Baselines3çš„å¹¶è¡Œè®­ç»ƒ"""
        from code.src.data_processing import PowerGridDataProcessor
        from code.src.rl.gym_wrapper import make_parallel_env
        from stable_baselines3 import PPO
        
        # ã€ä¿®å¤ã€‘åœ¨åˆ›å»ºå¹¶è¡Œç¯å¢ƒå‰ï¼Œå°†ä¸»è¿›ç¨‹ä¸­å·²è§£æå¥½çš„è®¾å¤‡åç§°æ›´æ–°åˆ°é…ç½®å­—å…¸ä¸­
        # é¿å…å­è¿›ç¨‹æ”¶åˆ° "auto" å­—ç¬¦ä¸²å¯¼è‡´ torch.device() æŠ¥é”™
        config['system']['device'] = str(self.device)
        print(f"ğŸ”§ å¹¶è¡Œè®­ç»ƒè®¾å¤‡é…ç½®å·²æ›´æ–°: {config['system']['device']}")

        # 1. æ•°æ®å¤„ç†
        print("\n1ï¸âƒ£ æ•°æ®å¤„ç†...")
        processor = PowerGridDataProcessor(
            normalize=config['data']['normalize'],
            cache_dir=config['data']['cache_dir']
        )

        # åŠ è½½æ•°æ®
        mpc = load_power_grid_data(config['data']['case_name'])

        # 2. åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
        print("\n2ï¸âƒ£ åˆ›å»ºå¹¶è¡Œç¯å¢ƒ...")
        parallel_config = config['parallel_training']

        parallel_env = make_parallel_env(
            base_case_data=mpc,
            config=config,
            num_envs=parallel_config['num_cpus'],
            use_scenario_generator=parallel_config['scenario_generation']
        )

        # 3. åˆ›å»ºå¹¶è®­ç»ƒPPOæ™ºèƒ½ä½“
        print("\n3ï¸âƒ£ åˆ›å»ºPPOæ™ºèƒ½ä½“...")
        model = PPO(
            "MlpPolicy",
            parallel_env,
            verbose=1,
            tensorboard_log=config['logging']['log_dir']
        )

        print("\n4ï¸âƒ£ å¼€å§‹å¹¶è¡Œè®­ç»ƒ...")
        model.learn(total_timesteps=parallel_config['total_timesteps'])

        # 4. ä¿å­˜æ¨¡å‹
        model_path = Path(config['logging']['checkpoint_dir']) / "parallel_model"
        model_path.parent.mkdir(exist_ok=True, parents=True)
        model.save(str(model_path))

        return {
            'success': True,
            'mode': 'parallel',
            'model_path': str(model_path),
            'total_timesteps': parallel_config['total_timesteps'],
            'config': config
        }

    def _run_simple_parallel_training(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """ç®€åŒ–çš„å¹¶è¡Œè®­ç»ƒï¼ˆä¸ä¾èµ–gymå’Œstable-baselines3ï¼‰"""
        print("\nâš ï¸ ä½¿ç”¨ç®€åŒ–çš„å¹¶è¡Œè®­ç»ƒæ¨¡å¼ï¼ˆæ— gym/stable-baselines3ï¼‰")
        
        # ã€ä¿®å¤ã€‘ç¡®ä¿è®¾å¤‡é…ç½®æ­£ç¡®ä¼ é€’ç»™å­è¿›ç¨‹
        config['system']['device'] = str(self.device)
        print(f"ğŸ”§ ç®€åŒ–å¹¶è¡Œè®­ç»ƒè®¾å¤‡é…ç½®å·²æ›´æ–°: {config['system']['device']}")

        # å¯¼å…¥å¿…è¦æ¨¡å—
        from code.src.data_processing import PowerGridDataProcessor
        from code.src.gat import create_hetero_graph_encoder
        from code.src.rl.environment import PowerGridPartitioningEnv
        from code.src.rl.agent import PPOAgent
        import multiprocessing as mp

        # 1. æ•°æ®å¤„ç†
        print("\n1ï¸âƒ£ æ•°æ®å¤„ç†...")
        processor = PowerGridDataProcessor(
            normalize=config['data']['normalize'],
            cache_dir=config['data']['cache_dir']
        )

        # åŠ è½½æ•°æ®
        mpc = load_power_grid_data(config['data']['case_name'])
        hetero_data = processor.graph_from_mpc(mpc).to(self.device)

        # 2. GATç¼–ç å™¨
        print("\n2ï¸âƒ£ GATç¼–ç å™¨...")
        gat_config = config['gat']
        encoder = create_hetero_graph_encoder(
            hetero_data,
            hidden_channels=gat_config['hidden_channels'],
            gnn_layers=gat_config['gnn_layers'],
            heads=gat_config['heads'],
            output_dim=gat_config['output_dim']
        ).to(self.device)

        with torch.no_grad():
            node_embeddings, attention_weights = encoder.encode_nodes_with_attention(hetero_data)

        # 3. è¿è¡Œå¤šä¸ªè®­ç»ƒå®ä¾‹
        print("\n3ï¸âƒ£ å¼€å§‹ç®€åŒ–å¹¶è¡Œè®­ç»ƒ...")
        parallel_config = config['parallel_training']
        num_workers = min(parallel_config['num_cpus'], 4)  # é™åˆ¶å·¥ä½œè¿›ç¨‹æ•°

        # æ¯ä¸ªå·¥ä½œè¿›ç¨‹çš„è®­ç»ƒå›åˆæ•°
        episodes_per_worker = config['training']['num_episodes'] // num_workers

        results = []
        for worker_id in range(num_workers):
            print(f"ğŸ”„ å¯åŠ¨å·¥ä½œè¿›ç¨‹ {worker_id + 1}/{num_workers}")

            # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
            env = PowerGridPartitioningEnv(
                hetero_data=hetero_data,
                node_embeddings=node_embeddings,
                num_partitions=config['environment']['num_partitions'],
                reward_weights=config['environment']['reward_weights'],
                max_steps=config['environment']['max_steps'],
                device=self.device,
                attention_weights=attention_weights
            )

            agent_config = config['agent']
            node_embedding_dim = env.state_manager.embedding_dim
            region_embedding_dim = node_embedding_dim * 2

            agent = PPOAgent(
                node_embedding_dim=node_embedding_dim,
                region_embedding_dim=region_embedding_dim,
                num_partitions=env.num_partitions,
                lr_actor=agent_config['lr_actor'],
                lr_critic=agent_config['lr_critic'],
                gamma=agent_config['gamma'],
                eps_clip=agent_config['eps_clip'],
                k_epochs=agent_config['k_epochs'],
                entropy_coef=agent_config['entropy_coef'],
                value_coef=agent_config['value_coef'],
                device=self.device,
                max_grad_norm=agent_config.get('max_grad_norm', None),
                # ã€ä¿®æ”¹ã€‘ä¼ é€’ç‹¬ç«‹çš„è°ƒåº¦å™¨é…ç½®
                actor_scheduler_config=agent_config.get('actor_scheduler', {}),
                critic_scheduler_config=agent_config.get('critic_scheduler', {})
            )

            # è®­ç»ƒ
            trainer = UnifiedTrainer(agent=agent, env=env, config=config)
            history = trainer.train(
                num_episodes=episodes_per_worker,
                max_steps_per_episode=config['training']['max_steps_per_episode'],
                update_interval=config['training']['update_interval']
            )

            results.append({
                'worker_id': worker_id,
                'history': history,
                'best_reward': trainer.logger.best_reward
            })

        # æ±‡æ€»ç»“æœ
        total_episodes = sum(len(r['history']['episode_rewards']) for r in results)
        best_reward = max(r['best_reward'] for r in results)

        return {
            'success': True,
            'mode': 'parallel_simple',
            'config': config,
            'total_episodes': total_episodes,
            'best_reward': best_reward,
            'worker_results': results
        }
    
    def _run_curriculum_training(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œè¯¾ç¨‹å­¦ä¹ è®­ç»ƒ"""
        print("ğŸ“š è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæ¨¡å¼")
        
        curriculum_config = config['curriculum']
        start_partitions = curriculum_config['start_partitions']
        end_partitions = curriculum_config['end_partitions']
        episodes_per_stage = curriculum_config['episodes_per_stage']
        
        results = []
        
        for num_partitions in range(start_partitions, end_partitions + 1):
            print(f"\nğŸ“– è¯¾ç¨‹é˜¶æ®µ: {num_partitions}ä¸ªåˆ†åŒº")
            
            # æ›´æ–°é…ç½®
            stage_config = config.copy()
            stage_config['environment']['num_partitions'] = num_partitions
            stage_config['training']['num_episodes'] = episodes_per_stage
            
            # è¿è¡Œè¯¥é˜¶æ®µçš„è®­ç»ƒ
            stage_result = self._run_standard_training(stage_config)
            results.append(stage_result)
            
            if not stage_result['success']:
                break
        
        return {
            'success': all(r['success'] for r in results),
            'mode': 'curriculum',
            'config': config,
            'stage_results': results
        }
    
    def run_demo(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´ç³»ç»Ÿæ¼”ç¤º"""
        print("\nğŸª è¿è¡Œå®Œæ•´ç³»ç»Ÿæ¼”ç¤º")
        print("=" * 60)

        try:
            # è¿è¡Œå¿«é€Ÿæ¼”ç¤ºæ¨¡å¼
            demo_config = self.get_training_configs()['quick']
            result = self._run_standard_training(demo_config)
            return {
                'success': result['success'],
                'mode': 'demo',
                'demo_result': result
            }
        except Exception as e:
            print(f"âŒ æ¼”ç¤ºå¤±è´¥: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def create_training_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        report_lines = [
            f"# ç”µåŠ›ç½‘ç»œåˆ†åŒºè®­ç»ƒæŠ¥å‘Š",
            f"",
            f"## ç³»ç»Ÿä¿¡æ¯",
            f"- è®­ç»ƒæ¨¡å¼: {results.get('mode', 'unknown')}",
            f"- è®¾å¤‡: {self.device}",
            f"- æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"",
            f"## é…ç½®ä¿¡æ¯"
        ]
        
        if 'config' in results:
            config = results['config']
            report_lines.extend([
                f"- æ¡ˆä¾‹: {config['data']['case_name']}",
                f"- åˆ†åŒºæ•°: {config['environment']['num_partitions']}",
                f"- è®­ç»ƒå›åˆ: {config['training']['num_episodes']}",
                f"- æœ€å¤§æ­¥æ•°: {config['training']['max_steps_per_episode']}"
            ])
        
        if 'eval_stats' in results:
            eval_stats = results['eval_stats']
            report_lines.extend([
                f"",
                f"## è¯„ä¼°ç»“æœ",
                f"- å¹³å‡å¥–åŠ±: {eval_stats.get('mean_reward', 0):.4f}",
                f"- æˆåŠŸç‡: {eval_stats.get('success_rate', 0):.4f}",
                f"- å¹³å‡è´Ÿè½½CV: {eval_stats.get('mean_load_cv', 0):.4f}"
            ])
        
        if 'baseline_results' in results and results['baseline_results'] is not None:
            report_lines.extend([
                f"",
                f"## åŸºçº¿æ–¹æ³•å¯¹æ¯”",
                f"åŸºçº¿æ–¹æ³•å¯¹æ¯”ç»“æœå·²ä¿å­˜"
            ])
        
        return "\n".join(report_lines)
    
    def save_results(self, results: Dict[str, Any], output_dir: str = 'experiments'):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        mode = results.get('mode', 'unknown')
        
        # ä¿å­˜ç»“æœJSON
        results_file = output_path / f"{mode}_results_{timestamp}.json"
        
        # æ¸…ç†ä¸èƒ½åºåˆ—åŒ–çš„å¯¹è±¡
        clean_results = {}
        for key, value in results.items():
            if key in ['history', 'eval_stats', 'config']:
                clean_results[key] = value
            elif key == 'baseline_results' and value is not None:
                clean_results[key] = value.to_dict() if hasattr(value, 'to_dict') else str(value)
            elif isinstance(value, (str, int, float, bool, type(None))):
                clean_results[key] = value
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(clean_results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        report = self.create_training_report(results)
        report_file = output_path / f"{mode}_report_{timestamp}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        print(f"   - ç»“æœæ–‡ä»¶: {results_file.name}")
        print(f"   - æŠ¥å‘Šæ–‡ä»¶: {report_file.name}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç”µåŠ›ç½‘ç»œåˆ†åŒºç»Ÿä¸€è®­ç»ƒç³»ç»Ÿ')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--mode', type=str, default='standard',
                       choices=['quick', 'standard', 'full', 'ieee118', 'parallel', 'curriculum', 'demo'],
                       help='è®­ç»ƒæ¨¡å¼')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--case', type=str, help='ç”µç½‘æ¡ˆä¾‹åç§°')
    parser.add_argument('--episodes', type=int, help='è®­ç»ƒå›åˆæ•°')
    parser.add_argument('--partitions', type=int, help='åˆ†åŒºæ•°é‡')
    parser.add_argument('--device', type=str, help='è®¡ç®—è®¾å¤‡')
    
    # åŠŸèƒ½å¼€å…³
    parser.add_argument('--no-baselines', action='store_true', help='è·³è¿‡åŸºçº¿æ–¹æ³•å¯¹æ¯”')
    parser.add_argument('--no-viz', action='store_true', help='è·³è¿‡å¯è§†åŒ–')
    parser.add_argument('--save-results', action='store_true', help='ä¿å­˜è®­ç»ƒç»“æœ')
    
    # ç‰¹æ®Šæ¨¡å¼
    parser.add_argument('--list-configs', action='store_true', help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨é…ç½®')
    parser.add_argument('--check-deps', action='store_true', help='æ£€æŸ¥ä¾èµ–')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    if args.check_deps:
        deps = check_dependencies()
        print("ğŸ“‹ ä¾èµ–æ£€æŸ¥ç»“æœ:")
        for dep, available in deps.items():
            status = "âœ…" if available else "âŒ"
            print(f"  {status} {dep}")
        return
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    system = UnifiedTrainingSystem(args.config)
    
    # åˆ—å‡ºé…ç½®
    if args.list_configs:
        configs = system.get_training_configs()
        print("ğŸ“‹ å¯ç”¨è®­ç»ƒé…ç½®:")
        for name in configs.keys():
            print(f"  - {name}")
        return
    
    # æ„å»ºå‚æ•°è¦†ç›–
    overrides = {}
    if args.case:
        overrides['data.case_name'] = args.case
    if args.episodes:
        overrides['training.num_episodes'] = args.episodes
    if args.partitions:
        overrides['environment.num_partitions'] = args.partitions
    if args.device:
        overrides['system.device'] = args.device
    if args.no_baselines:
        overrides['evaluation.include_baselines'] = False
    if args.no_viz:
        overrides['visualization.enabled'] = False
    
    # è¿è¡Œè®­ç»ƒ
    print("\nğŸ¯ ç”µåŠ›ç½‘ç»œåˆ†åŒºç»Ÿä¸€è®­ç»ƒç³»ç»Ÿå¯åŠ¨")
    print("=" * 60)
    
    start_time = time.time()
    
    if args.mode == 'demo':
        results = system.run_demo()
    else:
        results = system.run_training(args.mode, **overrides)
    
    elapsed_time = time.time() - start_time
    
    # ç»“æœæ±‡æ€»
    print("\nğŸ“Š è®­ç»ƒå®Œæˆæ€»ç»“")
    print("=" * 60)
    print(f"æ¨¡å¼: {args.mode}")
    print(f"è€—æ—¶: {elapsed_time/3600:.2f} å°æ—¶")
    print(f"çŠ¶æ€: {'âœ… æˆåŠŸ' if results.get('success', False) else 'âŒ å¤±è´¥'}")
    
    if results.get('success', False):
        if 'best_reward' in results:
            print(f"æœ€ä½³å¥–åŠ±: {results['best_reward']:.4f}")
        if 'eval_stats' in results:
            print(f"æœ€ç»ˆæˆåŠŸç‡: {results['eval_stats'].get('success_rate', 0):.4f}")
    else:
        print(f"é”™è¯¯: {results.get('error', 'Unknown error')}")
    
    # ä¿å­˜ç»“æœ
    if args.save_results and results.get('success', False):
        system.save_results(results)
    
    print("\nğŸ‰ ç»Ÿä¸€è®­ç»ƒç³»ç»Ÿè¿è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    main() 