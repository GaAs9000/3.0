#!/usr/bin/env python3
"""
å¢å¼ºçš„æµ‹è¯•å¯è§†åŒ–é›†æˆ
å°†é«˜çº§å¯è§†åŒ–åŠŸèƒ½é›†æˆåˆ°ç°æœ‰çš„æµ‹è¯•æµç¨‹ä¸­
"""

import sys
import numpy as np
import torch
from pathlib import Path
from typing import Dict, List, Optional, Any
import json
import time

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent))

# å¯¼å…¥ç°æœ‰æ¨¡å—
from visualization import VisualizationManager
from plotly_chart_factory import PlotlyChartFactory
from html_dashboard_generator import HTMLDashboardGenerator
from advanced_visualizations import AdvancedVisualizationSystem, create_advanced_visualizations


class EnhancedTestVisualizer:
    """å¢å¼ºçš„æµ‹è¯•å¯è§†åŒ–å™¨"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–å¢å¼ºæµ‹è¯•å¯è§†åŒ–å™¨
        
        Args:
            config: é…ç½®å‚æ•°
        """
        self.config = config or {}
        
        # åˆå§‹åŒ–å„ä¸ªå¯è§†åŒ–ç»„ä»¶
        self.basic_viz = VisualizationManager(config)
        self.chart_factory = PlotlyChartFactory(config.get('chart_factory', {}))
        self.dashboard_gen = HTMLDashboardGenerator(config.get('dashboard', {}))
        self.advanced_viz = AdvancedVisualizationSystem(config.get('advanced', {}))
        
        # è¾“å‡ºç›®å½•
        self.output_dir = Path(self.config.get('output_dir', 'evaluation/latest/enhanced_viz'))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def visualize_test_results(self, 
                             test_results: Dict[str, Any],
                             model_info: Dict[str, Any],
                             env_data: Optional[Dict[str, Any]] = None) -> Dict[str, str]:
        """
        ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•ç»“æœå¯è§†åŒ–
        
        Args:
            test_results: æµ‹è¯•ç»“æœæ•°æ®
            model_info: æ¨¡å‹ä¿¡æ¯
            env_data: ç¯å¢ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç”Ÿæˆçš„å¯è§†åŒ–æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        print("\nğŸ¨ å¼€å§‹ç”Ÿæˆå¢å¼ºçš„æµ‹è¯•å¯è§†åŒ–...")
        
        generated_files = {}
        
        # 1. åŸºç¡€å¯è§†åŒ–
        print("\nğŸ“Š [1/6] ç”ŸæˆåŸºç¡€å¯è§†åŒ–...")
        basic_files = self._generate_basic_visualizations(test_results, env_data)
        generated_files.update(basic_files)
        
        # 2. æ€§èƒ½å¯¹æ¯”å›¾è¡¨
        print("\nğŸ“ˆ [2/6] ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨...")
        perf_files = self._generate_performance_charts(test_results)
        generated_files.update(perf_files)
        
        # 3. äº¤äº’å¼ç½‘ç»œå›¾
        print("\nğŸŒ [3/6] ç”Ÿæˆäº¤äº’å¼ç½‘ç»œå›¾...")
        network_files = self._generate_network_visualizations(test_results, env_data)
        generated_files.update(network_files)
        
        # 4. ç®—æ³•å¯¹æ¯”åˆ†æ
        print("\nğŸ”¬ [4/6] ç”Ÿæˆç®—æ³•å¯¹æ¯”åˆ†æ...")
        comparison_files = self._generate_algorithm_comparisons(test_results)
        generated_files.update(comparison_files)
        
        # 5. åœºæ™¯åˆ†æå¯è§†åŒ–
        print("\nğŸ­ [5/6] ç”Ÿæˆåœºæ™¯åˆ†æå¯è§†åŒ–...")
        scenario_files = self._generate_scenario_analysis(test_results)
        generated_files.update(scenario_files)
        
        # 6. ç»¼åˆHTMLæŠ¥å‘Š
        print("\nğŸ“ [6/6] ç”Ÿæˆç»¼åˆHTMLæŠ¥å‘Š...")
        report_file = self._generate_comprehensive_report(
            test_results, model_info, generated_files
        )
        generated_files['comprehensive_report'] = str(report_file)
        
        print(f"\nâœ… æµ‹è¯•å¯è§†åŒ–å®Œæˆï¼ç”Ÿæˆäº† {len(generated_files)} ä¸ªæ–‡ä»¶")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        
        return generated_files
    
    def _generate_basic_visualizations(self, 
                                     test_results: Dict[str, Any],
                                     env_data: Optional[Dict[str, Any]]) -> Dict[str, str]:
        """ç”ŸæˆåŸºç¡€å¯è§†åŒ–"""
        files = {}
        
        # è®­ç»ƒæ›²çº¿
        if 'episode_rewards' in test_results:
            fig = self.chart_factory.create_training_curves({
                'episode_rewards': test_results['episode_rewards'],
                'episode_lengths': test_results.get('episode_lengths', []),
                'success_rates': test_results.get('success_rates', [])
            })
            
            if fig:
                output_file = self.output_dir / "training_curves.html"
                fig.write_html(str(output_file))
                files['training_curves'] = str(output_file)
        
        # åˆ†åŒºå¯è§†åŒ–ï¼ˆå¦‚æœæœ‰ç¯å¢ƒæ•°æ®ï¼‰
        if env_data and hasattr(self.basic_viz, 'visualize_partition'):
            try:
                # è¿™é‡Œéœ€è¦åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„envå¯¹è±¡æˆ–ç›´æ¥ä½¿ç”¨æ•°æ®
                output_file = self.output_dir / "partition_layout.png"
                # self.basic_viz.visualize_partition(env, save_path=str(output_file))
                # files['partition_layout'] = str(output_file)
            except Exception as e:
                print(f"   âš ï¸ åˆ†åŒºå¯è§†åŒ–å¤±è´¥: {e}")
        
        return files
    
    def _generate_performance_charts(self, test_results: Dict[str, Any]) -> Dict[str, str]:
        """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
        files = {}
        
        # æ€§èƒ½æŒ‡æ ‡æ—¶é—´åºåˆ—
        if 'metrics_history' in test_results:
            metrics = test_results['metrics_history']
            
            # CVå€¼å˜åŒ–æ›²çº¿
            if 'cv_values' in metrics:
                fig = self.chart_factory.create_metrics_timeline(
                    metrics={'CVå€¼': metrics['cv_values']},
                    title="CVå€¼å˜åŒ–è¶‹åŠ¿"
                )
                if fig:
                    output_file = self.output_dir / "cv_timeline.html"
                    fig.write_html(str(output_file))
                    files['cv_timeline'] = str(output_file)
            
            # è€¦åˆåº¦å˜åŒ–
            if 'coupling_ratios' in metrics:
                fig = self.chart_factory.create_metrics_timeline(
                    metrics={'è€¦åˆåº¦': metrics['coupling_ratios']},
                    title="è€¦åˆåº¦å˜åŒ–è¶‹åŠ¿"
                )
                if fig:
                    output_file = self.output_dir / "coupling_timeline.html"
                    fig.write_html(str(output_file))
                    files['coupling_timeline'] = str(output_file)
        
        # è®¡ç®—æ—¶é—´åˆ†å¸ƒ
        if 'computation_times' in test_results:
            import plotly.graph_objects as go
            
            fig = go.Figure()
            fig.add_trace(go.Histogram(
                x=test_results['computation_times'],
                nbinsx=30,
                name='è®¡ç®—æ—¶é—´åˆ†å¸ƒ',
                marker_color='#2E86AB'
            ))
            
            fig.update_layout(
                title="Agentå†³ç­–æ—¶é—´åˆ†å¸ƒ",
                xaxis_title="è®¡ç®—æ—¶é—´ (ms)",
                yaxis_title="é¢‘æ¬¡",
                showlegend=False
            )
            
            output_file = self.output_dir / "computation_time_dist.html"
            fig.write_html(str(output_file))
            files['computation_time_dist'] = str(output_file)
        
        return files
    
    def _generate_network_visualizations(self, 
                                       test_results: Dict[str, Any],
                                       env_data: Optional[Dict[str, Any]]) -> Dict[str, str]:
        """ç”Ÿæˆç½‘ç»œå¯è§†åŒ–"""
        files = {}
        
        if not env_data:
            return files
        
        # äº¤äº’å¼ç½‘ç»œå›¾
        if 'best_partition' in test_results:
            output_file = self.advanced_viz.create_interactive_network_graph(
                env_data,
                test_results['best_partition'],
                title="æœ€ä¼˜åˆ†åŒºæ–¹æ¡ˆ"
            )
            if output_file:
                files['interactive_network'] = output_file
        
        # 3Dç½‘ç»œå¯è§†åŒ–
        if 'node_embeddings' in test_results:
            output_file = self.advanced_viz.create_3d_network_visualization(
                env_data,
                test_results.get('best_partition', np.array([])),
                test_results['node_embeddings'],
                title="èŠ‚ç‚¹åµŒå…¥3Då¯è§†åŒ–"
            )
            if output_file:
                files['3d_network'] = output_file
        
        # GATæ³¨æ„åŠ›çƒ­å›¾
        if 'attention_weights' in test_results:
            output_file = self.advanced_viz.create_attention_heatmap(
                test_results['attention_weights'],
                title="GATæ³¨æ„åŠ›æƒé‡åˆ†æ"
            )
            if output_file:
                files['attention_heatmap'] = output_file
        
        return files
    
    def _generate_algorithm_comparisons(self, test_results: Dict[str, Any]) -> Dict[str, str]:
        """ç”Ÿæˆç®—æ³•å¯¹æ¯”åˆ†æ"""
        files = {}
        
        # ç®—æ³•æ€§èƒ½å¯¹æ¯”
        if 'algorithm_comparison' in test_results:
            # é›·è¾¾å›¾å¯¹æ¯”
            output_file = self.advanced_viz.create_performance_radar_chart(
                test_results['algorithm_comparison'],
                title="ç®—æ³•æ€§èƒ½å¤šç»´å¯¹æ¯”"
            )
            if output_file:
                files['performance_radar'] = output_file
            
            # æ€§èƒ½æ¡å½¢å›¾
            import plotly.graph_objects as go
            from plotly.subplots import make_subplots
            
            comparison_data = test_results['algorithm_comparison']
            algorithms = list(comparison_data.keys())
            
            if algorithms:
                metrics = list(comparison_data[algorithms[0]].keys())
                
                fig = make_subplots(
                    rows=2, cols=2,
                    subplot_titles=metrics[:4] if len(metrics) >= 4 else metrics
                )
                
                for i, metric in enumerate(metrics[:4]):
                    row = i // 2 + 1
                    col = i % 2 + 1
                    
                    values = [comparison_data[algo].get(metric, 0) for algo in algorithms]
                    
                    fig.add_trace(
                        go.Bar(
                            x=algorithms,
                            y=values,
                            name=metric,
                            marker_color=self.chart_factory.color_sequence[i % len(self.chart_factory.color_sequence)]
                        ),
                        row=row, col=col
                    )
                
                fig.update_layout(
                    title_text="ç®—æ³•æ€§èƒ½è¯¦ç»†å¯¹æ¯”",
                    showlegend=False,
                    height=600
                )
                
                output_file = self.output_dir / "algorithm_comparison_bars.html"
                fig.write_html(str(output_file))
                files['algorithm_comparison_bars'] = str(output_file)
        
        # æ”¶æ•›é€Ÿåº¦å¯¹æ¯”
        if 'convergence_data' in test_results:
            fig = go.Figure()
            
            for algo, data in test_results['convergence_data'].items():
                fig.add_trace(go.Scatter(
                    x=list(range(len(data))),
                    y=data,
                    mode='lines',
                    name=algo,
                    line=dict(width=2)
                ))
            
            fig.update_layout(
                title="ç®—æ³•æ”¶æ•›é€Ÿåº¦å¯¹æ¯”",
                xaxis_title="è¿­ä»£æ¬¡æ•°",
                yaxis_title="ç›®æ ‡å‡½æ•°å€¼",
                hovermode='x unified'
            )
            
            output_file = self.output_dir / "convergence_comparison.html"
            fig.write_html(str(output_file))
            files['convergence_comparison'] = str(output_file)
        
        return files
    
    def _generate_scenario_analysis(self, test_results: Dict[str, Any]) -> Dict[str, str]:
        """ç”Ÿæˆåœºæ™¯åˆ†æå¯è§†åŒ–"""
        files = {}
        
        # åœºæ™¯æ€§èƒ½çƒ­å›¾
        if 'scenario_performance' in test_results:
            import plotly.graph_objects as go
            
            scenario_data = test_results['scenario_performance']
            scenarios = list(scenario_data.keys())
            metrics = list(scenario_data[scenarios[0]].keys()) if scenarios else []
            
            # åˆ›å»ºçƒ­å›¾æ•°æ®
            z_data = []
            for scenario in scenarios:
                row = [scenario_data[scenario].get(metric, 0) for metric in metrics]
                z_data.append(row)
            
            fig = go.Figure(data=go.Heatmap(
                z=z_data,
                x=metrics,
                y=scenarios,
                colorscale='RdYlGn',
                colorbar=dict(title="æ€§èƒ½åˆ†æ•°")
            ))
            
            fig.update_layout(
                title="ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°",
                xaxis_title="æ€§èƒ½æŒ‡æ ‡",
                yaxis_title="åœºæ™¯ç±»å‹"
            )
            
            output_file = self.output_dir / "scenario_performance_heatmap.html"
            fig.write_html(str(output_file))
            files['scenario_heatmap'] = str(output_file)
        
        # åœºæ™¯åˆ†å¸ƒé¥¼å›¾
        if 'scenario_distribution' in test_results:
            import plotly.express as px
            
            dist_data = test_results['scenario_distribution']
            
            fig = px.pie(
                values=list(dist_data.values()),
                names=list(dist_data.keys()),
                title="æµ‹è¯•åœºæ™¯åˆ†å¸ƒ",
                color_discrete_sequence=self.chart_factory.color_sequence
            )
            
            output_file = self.output_dir / "scenario_distribution.html"
            fig.write_html(str(output_file))
            files['scenario_distribution'] = str(output_file)
        
        return files
    
    def _generate_comprehensive_report(self,
                                     test_results: Dict[str, Any],
                                     model_info: Dict[str, Any],
                                     generated_files: Dict[str, str]) -> Path:
        """ç”Ÿæˆç»¼åˆHTMLæŠ¥å‘Š"""
        
        # å‡†å¤‡æŠ¥å‘Šæ•°æ®
        report_data = {
            'title': 'ç”µåŠ›ç½‘ç»œåˆ†åŒºå¢å¼ºæµ‹è¯•æŠ¥å‘Š',
            'model_info': model_info,
            'test_summary': {
                'total_episodes': test_results.get('num_episodes', 0),
                'avg_reward': test_results.get('avg_reward', 0),
                'success_rate': test_results.get('success_rate', 0),
                'best_cv': test_results.get('best_cv', 0),
                'avg_computation_time': np.mean(test_results.get('computation_times', [0]))
            },
            'visualizations': generated_files,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # ä½¿ç”¨é«˜çº§å¯è§†åŒ–ç³»ç»Ÿçš„æŠ¥å‘Šç”Ÿæˆå™¨
        report_dir = self.advanced_viz.generate_comprehensive_test_report(
            test_results, model_info
        )
        
        # æ·»åŠ é¢å¤–çš„å¯è§†åŒ–é“¾æ¥
        index_file = report_dir / "index.html"
        
        # è¿™é‡Œå¯ä»¥ä¿®æ”¹index.htmlæ·»åŠ æ›´å¤šå†…å®¹
        
        return index_file
    
    def create_live_monitoring_dashboard(self, port: int = 8050):
        """
        åˆ›å»ºå®æ—¶ç›‘æ§ä»ªè¡¨æ¿
        
        Args:
            port: WebæœåŠ¡å™¨ç«¯å£
        """
        print(f"\nğŸŒ å¯åŠ¨å®æ—¶ç›‘æ§ä»ªè¡¨æ¿åœ¨ç«¯å£ {port}...")
        
        # ä½¿ç”¨é«˜çº§å¯è§†åŒ–ç³»ç»Ÿçš„å®æ—¶ç›‘æ§åŠŸèƒ½
        dashboard_url = self.advanced_viz.create_realtime_monitoring_dashboard(
            initial_data={},
            port=port
        )
        
        return dashboard_url


# é›†æˆåˆ°test.pyçš„å‡½æ•°
def enhance_test_visualization(test_results: Dict[str, Any],
                             model_info: Dict[str, Any],
                             config: Optional[Dict[str, Any]] = None) -> Dict[str, str]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä¸ºtest.pyæä¾›å¢å¼ºçš„å¯è§†åŒ–åŠŸèƒ½
    
    Args:
        test_results: æµ‹è¯•ç»“æœ
        model_info: æ¨¡å‹ä¿¡æ¯  
        config: é…ç½®å‚æ•°
        
    Returns:
        ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„å­—å…¸
    """
    visualizer = EnhancedTestVisualizer(config)
    
    # å¤„ç†ç¯å¢ƒæ•°æ®
    env_data = None
    if 'env_data' in test_results:
        env_data = test_results['env_data']
    elif 'bus_data' in test_results and 'branch_data' in test_results:
        env_data = {
            'bus_data': test_results['bus_data'],
            'branch_data': test_results['branch_data']
        }
    
    return visualizer.visualize_test_results(test_results, model_info, env_data)


# ç¤ºä¾‹ï¼šå¦‚ä½•åœ¨test.pyä¸­ä½¿ç”¨
def example_usage():
    """ç¤ºä¾‹ç”¨æ³•"""
    
    # æ¨¡æ‹Ÿæµ‹è¯•ç»“æœ
    test_results = {
        'num_episodes': 100,
        'avg_reward': -2.5,
        'success_rate': 0.85,
        'best_cv': 0.25,
        'episode_rewards': np.random.randn(100).cumsum(),
        'computation_times': np.random.exponential(10, 100),
        'best_partition': np.random.randint(1, 5, 57),
        'algorithm_comparison': {
            'RL Agent': {'CV': 0.25, 'è€¦åˆåº¦': 0.15, 'æˆåŠŸç‡': 0.85, 'è®¡ç®—æ—¶é—´': 10},
            'Spectral': {'CV': 0.35, 'è€¦åˆåº¦': 0.25, 'æˆåŠŸç‡': 0.65, 'è®¡ç®—æ—¶é—´': 5},
            'K-means': {'CV': 0.45, 'è€¦åˆåº¦': 0.35, 'æˆåŠŸç‡': 0.45, 'è®¡ç®—æ—¶é—´': 3}
        },
        'scenario_performance': {
            'æ­£å¸¸': {'CV': 0.25, 'è€¦åˆåº¦': 0.15, 'æˆåŠŸç‡': 0.90},
            'é«˜è´Ÿè½½': {'CV': 0.30, 'è€¦åˆåº¦': 0.20, 'æˆåŠŸç‡': 0.80},
            'N-1æ•…éšœ': {'CV': 0.35, 'è€¦åˆåº¦': 0.25, 'æˆåŠŸç‡': 0.70}
        }
    }
    
    model_info = {
        'network_type': 'IEEE57',
        'training_episodes': 3000,
        'best_reward': -1.8
    }
    
    # ç”Ÿæˆå¢å¼ºå¯è§†åŒ–
    generated_files = enhance_test_visualization(test_results, model_info)
    
    print("\nç”Ÿæˆçš„å¯è§†åŒ–æ–‡ä»¶:")
    for name, path in generated_files.items():
        print(f"  {name}: {path}")


if __name__ == "__main__":
    example_usage()