import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATv2Conv, to_hetero, LayerNorm, global_mean_pool
from torch_geometric.data import HeteroData
from torch_geometric.utils import softmax
from typing import Optional, List, Dict, Tuple, Union, Any
import numpy as np
import warnings

# å­˜å‚¨æ•è·åˆ°çš„æ³¨æ„åŠ›æƒé‡çš„å…¨å±€åˆ—è¡¨ï¼ˆæˆ–ç±»å±æ€§ï¼‰
captured_attention_weights = []

def attention_hook(module, input, output):
    """ä¸€ä¸ªforward hookï¼Œç”¨äºæ•è·æ³¨æ„åŠ›æƒé‡"""
    # GATv2Convåœ¨return_attention_weights=Trueæ—¶ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªå…ƒç»„(out, attention_details)
    # attention_details æ˜¯ (edge_index, alpha)
    if isinstance(output, tuple) and len(output) == 2:
        # æˆ‘ä»¬åªå…³å¿ƒ alpha å¼ é‡
        if isinstance(output[1], tuple) and len(output[1]) == 2:
            captured_attention_weights.append(output[1][1])

class PhysicsGATv2Conv(GATv2Conv):
    """
    ç‰©ç†å¼•å¯¼çš„GAT
    
    æ ¸å¿ƒåˆ›æ–°ï¼š
    1. åŸºäºæœ€æ–°çš„GATv2æ¶æ„ï¼Œå…·æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›
    2. åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­èå…¥ç”µæ°”é˜»æŠ—å…ˆéªŒ
    3. é˜»æŠ—è¶Šå°çš„è¿æ¥è·å¾—è¶Šé«˜çš„æ³¨æ„åŠ›æƒé‡
    4. å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°æ§åˆ¶ç‰©ç†å…ˆéªŒçš„å½±å“ç¨‹åº¦
    
    æ•°å­¦åŸç†ï¼š
    æ ‡å‡†GATv2: Î±_ij = softmax(W_a^T LeakyReLU(W_l[W_r h_i || W_r h_j] + b))
    ç‰©ç†GATv2: Î±_ij = softmax(W_a^T LeakyReLU(W_l[W_r h_i || W_r h_j] + b) + Ï„/|Z_ij|)
    """
    
    def __init__(self, in_channels: int, out_channels: int, heads: int = 8,
                 concat: bool = True, dropout: float = 0.6, 
                 edge_dim: Optional[int] = None, temperature: float = 1.0,
                 z_index: int = 3, physics_weight: float = 1.0, **kwargs):
        
        super().__init__(in_channels, out_channels, heads=heads, concat=concat,
                        dropout=dropout, edge_dim=edge_dim, **kwargs)
        
        # ç‰©ç†å¼•å¯¼å‚æ•°
        self.temperature = nn.Parameter(torch.tensor(temperature))
        self.physics_weight = physics_weight
        self.z_index = z_index  # é˜»æŠ—æ¨¡é•¿åœ¨è¾¹ç‰¹å¾ä¸­çš„ç´¢å¼•
        
        # è¾¹ç‰¹å¾å¤„ç†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if edge_dim is not None:
            self.edge_processor = nn.Linear(edge_dim, heads)
        else:
            self.edge_processor = None
    
    def forward(self, x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], 
                edge_index: torch.Tensor,
                edge_attr: Optional[torch.Tensor] = None,
                return_attention_weights: Optional[bool] = None):
        """
        å‰å‘ä¼ æ’­ï¼Œåœ¨æ ‡å‡†GATv2åŸºç¡€ä¸Šèå…¥ç‰©ç†å…ˆéªŒå¹¶ç¼“å­˜æ³¨æ„åŠ›æƒé‡
        """
        # å¤„ç†ç‰©ç†å…ˆéªŒå¢å¼ºçš„è¾¹ç‰¹å¾
        if edge_attr is not None:
            enhanced_edge_attr = self.physics_enhanced_edge_attr(edge_attr)
        else:
            enhanced_edge_attr = edge_attr
        
        # å§‹ç»ˆè¯·æ±‚æ³¨æ„åŠ›æƒé‡
        result = super().forward(
            x, edge_index, enhanced_edge_attr, return_attention_weights=True
        )
        
        out, attention_details = result
        
        # å°†æ³¨æ„åŠ›åˆ†æ•°å­˜å‚¨åœ¨å®ä¾‹å±æ€§ä¸­ï¼Œä»¥ä¾¿åç»­æå–
        self._alpha = attention_details[1]

        # æ ¹æ®è°ƒç”¨è€…çš„è¦æ±‚å†³å®šè¿”å›å€¼
        if return_attention_weights:
            return out, attention_details
        return out
    
    def physics_enhanced_edge_attr(self, edge_attr: torch.Tensor) -> torch.Tensor:
        """
        å¢å¼ºè¾¹ç‰¹å¾ï¼Œèå…¥ç‰©ç†å…ˆéªŒ
        """
        if edge_attr is None or edge_attr.shape[1] <= self.z_index:
            return edge_attr
        
        # æå–é˜»æŠ—æ¨¡é•¿
        z_magnitude = edge_attr[:, self.z_index].clamp(min=1e-6)
        
        # è®¡ç®—ç‰©ç†æƒé‡ï¼š1/|Z| (å¯¼çº³)
        physics_prior = self.physics_weight / z_magnitude
        
        # å°†ç‰©ç†å…ˆéªŒæ·»åŠ åˆ°è¾¹ç‰¹å¾ä¸­
        enhanced_edge_attr = edge_attr.clone()
        if enhanced_edge_attr.shape[1] < self.z_index + 2:
            # å¦‚æœç©ºé—´ä¸å¤Ÿï¼Œæ‰©å±•ç‰¹å¾ç»´åº¦
            padding = torch.zeros(
                enhanced_edge_attr.shape[0], 
                self.z_index + 2 - enhanced_edge_attr.shape[1],
                device=enhanced_edge_attr.device
            )
            enhanced_edge_attr = torch.cat([enhanced_edge_attr, padding], dim=1)
        
        # å°†ç‰©ç†å…ˆéªŒå­˜å‚¨åˆ°è¾¹ç‰¹å¾çš„æœ€åä¸€åˆ—
        enhanced_edge_attr[:, -1] = physics_prior * self.temperature
        
        return enhanced_edge_attr


class GNNEncoder(nn.Module):
    """
    åŒæ„GNNç¼–ç å™¨
    
    ç‰¹ç‚¹ï¼š
    1. å¤šå±‚PhysicsGATv2Convå †å 
    2. å±‚å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥
    3. è‡ªé€‚åº”dropout
    4. æ”¯æŒä¸åŒçš„èšåˆæ–¹å¼
    """
    
    def __init__(self, in_channels: int, hidden_channels: int, 
                 num_layers: int = 3, heads: int = 4, dropout: float = 0.3,
                 edge_dim: Optional[int] = None, activation: str = 'elu'):
        super().__init__()
        
        self.num_layers = num_layers
        self.activation = getattr(F, activation)
        
        # æ„å»ºå¤šå±‚ç½‘ç»œ
        self.convs = nn.ModuleList()
        self.norms = nn.ModuleList()
        self.residual_projs = nn.ModuleList()
        
        current_dim = in_channels
        
        for i in range(num_layers):
            # æœ€åä¸€å±‚ä¸ä½¿ç”¨å¤šå¤´concat
            is_last_layer = (i == num_layers - 1)
            concat = not is_last_layer
            
            # GATv2å·ç§¯å±‚
            conv = PhysicsGATv2Conv(
                current_dim, 
                hidden_channels,
                heads=heads,
                concat=concat,
                dropout=dropout,
                edge_dim=edge_dim,
                add_self_loops=False
            )
            self.convs.append(conv)
            
            # è®¡ç®—å®é™…è¾“å‡ºç»´åº¦
            actual_out_dim = hidden_channels * heads if concat else hidden_channels
            
            # å±‚å½’ä¸€åŒ–
            self.norms.append(LayerNorm(actual_out_dim))
            
            # æ®‹å·®æŠ•å½±
            if current_dim != actual_out_dim:
                self.residual_projs.append(nn.Linear(current_dim, actual_out_dim))
            else:
                self.residual_projs.append(nn.Identity())
            
            current_dim = actual_out_dim
        
        self.final_dim = current_dim
    
    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, 
                edge_attr: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        """
        for i, (conv, norm, residual_proj) in enumerate(
            zip(self.convs, self.norms, self.residual_projs)
        ):
            # ä¿å­˜æ®‹å·®
            residual = x
            
            # å·ç§¯
            x = conv(x, edge_index, edge_attr)
            
            # ç¡®ä¿xæ˜¯tensorè€Œä¸æ˜¯tuple
            if isinstance(x, tuple):
                x = x[0]
            
            # å±‚å½’ä¸€åŒ–
            x = norm(x)
            
            # æ®‹å·®è¿æ¥
            x = x + residual_proj(residual)
            
            # æ¿€æ´»å‡½æ•°ï¼ˆæœ€åä¸€å±‚é™¤å¤–ï¼‰
            if i < self.num_layers - 1:
                x = self.activation(x)
        
        return x


class HeteroGraphEncoder(nn.Module):
    """
    å¼‚æ„å›¾ç¼–ç å™¨ - ä¸“æ³¨äºå›¾è¡¨ç¤ºå­¦ä¹ 
    
    èŒè´£ï¼š
    1. å¤„ç†å¼‚æ„å›¾æ•°æ®ï¼ˆå¤šç§èŠ‚ç‚¹ç±»å‹å’Œè¾¹ç±»å‹ï¼‰
    2. æå–é«˜è´¨é‡çš„èŠ‚ç‚¹åµŒå…¥è¡¨ç¤º
    3. èå…¥ç”µç½‘ç‰©ç†å…ˆéªŒçŸ¥è¯†
    4. æ”¯æŒä¸‹æ¸¸ä»»åŠ¡çš„ç‰¹å¾æå–
    """
    
    def __init__(self, 
                 node_feature_dims: Dict[str, int],
                 edge_feature_dims: Dict[str, int],
                 metadata: Tuple[List[str], List[Tuple[str, str, str]]],
                 hidden_channels: int = 64,
                 gnn_layers: int = 3,
                 heads: int = 4,
                 dropout: float = 0.3,
                 output_dim: Optional[int] = None):
        super().__init__()
        
        self.node_types = metadata[0]
        self.edge_types = metadata[1]
        self.hidden_channels = hidden_channels
        self.output_dim = output_dim or hidden_channels
        
        # --- 1. ç‰¹å¾é¢„å¤„ç†å±‚ ---
        # ç»Ÿä¸€æ‰€æœ‰èŠ‚ç‚¹ç±»å‹çš„ç‰¹å¾ç»´åº¦
        max_node_dim = max(node_feature_dims.values())
        max_edge_dim = max(edge_feature_dims.values()) if edge_feature_dims else None
        
        # ä¸ºä¸åŒç±»å‹çš„èŠ‚ç‚¹åˆ›å»ºç‰¹å¾æŠ•å½±å±‚
        self.node_projectors = nn.ModuleDict()
        for node_type, feature_dim in node_feature_dims.items():
            if feature_dim != max_node_dim:
                self.node_projectors[node_type] = nn.Linear(feature_dim, max_node_dim)
            else:
                self.node_projectors[node_type] = nn.Identity()
        
        # --- 2. ä¸»å¹²ç½‘ç»œ (Backbone) ---
        # åˆ›å»ºåŒæ„GNNç¼–ç å™¨
        gnn_encoder = GNNEncoder(
            in_channels=max_node_dim,
            hidden_channels=hidden_channels,
            num_layers=gnn_layers,
            heads=heads,
            dropout=dropout,
            edge_dim=max_edge_dim
        )
        
        # è½¬æ¢ä¸ºå¼‚æ„æ¨¡å‹
        # åœ¨å®šä¹‰äº†ç¡®åˆ‡çš„ä¾èµ–å…³ç³»åï¼Œæˆ‘ä»¬ç¡®ä¿¡ to_hetero ä¼šç¨³å®šå·¥ä½œ
        self.hetero_encoder = to_hetero(gnn_encoder, metadata, aggr='sum')
        try:
            from rich_output import rich_debug
            rich_debug("ä½¿ç”¨ to_hetero è½¬æ¢çš„å¼‚æ„ç¼–ç å™¨", "attention")
        except ImportError:
            pass
        
        self.final_dim = gnn_encoder.final_dim
        
        # --- 3. è¾“å‡ºæŠ•å½±å±‚ ---
        encoder_output_dim = gnn_encoder.final_dim
        if self.output_dim != encoder_output_dim:
            self.output_projector = nn.Linear(encoder_output_dim, self.output_dim)
        else:
            self.output_projector = nn.Identity()
        
        # --- 4. å›¾çº§åˆ«è¡¨ç¤ºèšåˆå™¨ ---
        self.graph_pooling = global_mean_pool
        self.graph_projector = nn.Sequential(
            nn.Linear(self.output_dim, hidden_channels),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels, self.output_dim)
        )
    
    def preprocess_node_features(self, x_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        é¢„å¤„ç†èŠ‚ç‚¹ç‰¹å¾ï¼Œç¡®ä¿æ‰€æœ‰ç±»å‹å…·æœ‰ç›¸åŒç»´åº¦
        """
        processed_x_dict = {}
        for node_type, x in x_dict.items():
            processed_x_dict[node_type] = self.node_projectors[node_type](x)
        return processed_x_dict
    
    def forward(self, data: HeteroData, 
                return_attention_weights: bool = False,
                return_graph_embedding: bool = False) -> Union[Dict[str, torch.Tensor], 
                                                               Tuple[Dict[str, torch.Tensor], ...]]:
        """
        å‰å‘ä¼ æ’­ - æå–èŠ‚ç‚¹å’Œå›¾çº§åˆ«çš„åµŒå…¥è¡¨ç¤º
        
        å‚æ•°:
            data: å¼‚æ„å›¾æ•°æ®
            return_attention_weights: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
            return_graph_embedding: æ˜¯å¦è¿”å›å›¾çº§åˆ«åµŒå…¥
        
        è¿”å›:
            node_embeddings: å„ç±»å‹èŠ‚ç‚¹çš„åµŒå…¥è¡¨ç¤º {node_type: embeddings}
            attention_weights: æ³¨æ„åŠ›æƒé‡ (å¯é€‰)
            graph_embedding: å›¾çº§åˆ«åµŒå…¥ (å¯é€‰)
        """
        # --- 1. é¢„å¤„ç†èŠ‚ç‚¹ç‰¹å¾ ---
        processed_x_dict = self.preprocess_node_features(data.x_dict)
        
        # --- 2. å¼‚æ„GNNç¼–ç  ---
        raw_embeddings = self.hetero_encoder(
            processed_x_dict,
            data.edge_index_dict,
            data.edge_attr_dict
        )
        # åº”ç”¨è¾“å‡ºæŠ•å½±
        node_embeddings = {
            node_type: self.output_projector(embeddings)
            for node_type, embeddings in raw_embeddings.items()
        }
        
        # --- 3. å‡†å¤‡è¿”å›å€¼ ---
        results = [node_embeddings]
        
        # --- 4. æ³¨æ„åŠ›æƒé‡ï¼ˆå¯é€‰ï¼‰---
        if return_attention_weights:
            attention_weights = self.get_attention_weights()
            results.append(attention_weights)
        
        # --- 5. å›¾çº§åˆ«åµŒå…¥ï¼ˆå¯é€‰ï¼‰---
        if return_graph_embedding:
            # èšåˆæ‰€æœ‰èŠ‚ç‚¹åµŒå…¥ä¸ºå›¾çº§åˆ«è¡¨ç¤º
            all_embeddings = torch.cat(list(node_embeddings.values()), dim=0)
            if all_embeddings.size(0) > 0:
                batch = torch.zeros(all_embeddings.size(0), 
                                  dtype=torch.long, 
                                  device=all_embeddings.device)
                graph_embedding = self.graph_pooling(all_embeddings, batch)
                graph_embedding = self.graph_projector(graph_embedding)
            else:
                graph_embedding = torch.zeros(1, self.output_dim, device=data.x_dict[list(data.x_dict.keys())[0]].device)
            results.append(graph_embedding)
        
        # æ ¹æ®è¿”å›å‚æ•°å†³å®šè¿”å›æ ¼å¼
        if len(results) == 1:
            return results[0]
        else:
            return tuple(results)
    
    def encode_nodes(self, data: HeteroData) -> Dict[str, torch.Tensor]:
        """
        ä¾¿æ·æ–¹æ³•ï¼šä»…æå–èŠ‚ç‚¹åµŒå…¥
        """
        return self.forward(data, return_attention_weights=False, return_graph_embedding=False)

    def encode_nodes_with_attention(self, data: HeteroData, config: Dict[str, Any] = None) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:
        """
        ä¾¿æ·æ–¹æ³•ï¼šæå–èŠ‚ç‚¹åµŒå…¥å’Œæ³¨æ„åŠ›æƒé‡

        è¿”å›:
            node_embeddings: èŠ‚ç‚¹åµŒå…¥å­—å…¸
            attention_weights: æ³¨æ„åŠ›æƒé‡å­—å…¸
        """
        node_embeddings = self.forward(data, return_attention_weights=False, return_graph_embedding=False)
        attention_weights = self.get_attention_weights(config)
        return node_embeddings, attention_weights
    
    def encode_graph(self, data: HeteroData) -> torch.Tensor:
        """
        ä¾¿æ·æ–¹æ³•ï¼šä»…æå–å›¾çº§åˆ«åµŒå…¥
        """
        _, graph_embedding = self.forward(data, return_attention_weights=False, return_graph_embedding=True)
        return graph_embedding
    
    def get_attention_weights(self, config: Dict[str, Any] = None) -> Dict[str, torch.Tensor]:
        """
        è·å–æ³¨æ„åŠ›æƒé‡ç”¨äºå¢å¼ºåµŒå…¥ç”Ÿæˆ

        è¿”å›:
            attention_weights: æŒ‰è¾¹ç±»å‹ç»„ç»‡çš„æ³¨æ„åŠ›æƒé‡å­—å…¸
                æ ¼å¼: {edge_type_key: attention_tensor}
                å…¶ä¸­ edge_type_key = f"{src_type}__{relation}__{dst_type}"
        """
        # è·å–è°ƒè¯•é…ç½®
        debug_config = config.get('debug', {}) if config else {}
        training_output = debug_config.get('training_output', {})
        show_attention_collection = training_output.get('show_attention_collection', True)
        only_show_errors = training_output.get('only_show_errors', False)

        attention_weights = {}

        if show_attention_collection and not only_show_errors:
            try:
                from rich_output import rich_debug
                rich_debug("å¼€å§‹æ”¶é›†æ³¨æ„åŠ›æƒé‡...", "attention")
            except ImportError:
                pass

        def collect_attention_weights_with_names(module, prefix=""):
            """é€’å½’æ”¶é›†æ³¨æ„åŠ›æƒé‡å¹¶è®°å½•å¯¹åº”çš„è¾¹ç±»å‹"""
            # æ£€æŸ¥å½“å‰æ¨¡å—æ˜¯å¦æ˜¯PhysicsGATv2Conv
            if isinstance(module, PhysicsGATv2Conv):
                if hasattr(module, '_alpha') and module._alpha is not None:
                    if show_attention_collection and not only_show_errors:
                        print(f"  ğŸ“ å‘ç°PhysicsGATv2Convæ¨¡å—: {prefix}")
                        print(f"     æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {module._alpha.shape}")

                    # ä»æ¨¡å—åç§°æ¨æ–­è¾¹ç±»å‹
                    edge_type_key = self._extract_edge_type_from_module_name(prefix)
                    if edge_type_key:
                        attention_weights[edge_type_key] = module._alpha
                        if show_attention_collection and not only_show_errors:
                            print(f"     âœ… æ˜ å°„åˆ°è¾¹ç±»å‹: {edge_type_key}")
                    else:
                        if show_attention_collection and not only_show_errors:
                            print(f"     âš ï¸ æ— æ³•æ˜ å°„è¾¹ç±»å‹ï¼Œæ¨¡å—å: {prefix}")
                        # ä½¿ç”¨æ¨¡å—åä½œä¸ºå¤‡ç”¨é”®
                        fallback_key = f"module_{prefix.replace('.', '_')}"
                        attention_weights[fallback_key] = module._alpha
                        if show_attention_collection and not only_show_errors:
                            print(f"     ğŸ”„ ä½¿ç”¨å¤‡ç”¨é”®: {fallback_key}")
                elif show_attention_collection and not only_show_errors:
                    print(f"  ğŸ“ å‘ç°PhysicsGATv2Convæ¨¡å—ä½†æ— æ³¨æ„åŠ›æƒé‡: {prefix}")

            # é€’å½’æ£€æŸ¥å­æ¨¡å—
            for name, child in module.named_children():
                new_prefix = f"{prefix}.{name}" if prefix else name
                collect_attention_weights_with_names(child, new_prefix)

        # ä»hetero_encoderå¼€å§‹é€’å½’æœç´¢
        if hasattr(self, 'hetero_encoder'):
            collect_attention_weights_with_names(self.hetero_encoder)

        if show_attention_collection and not only_show_errors:
            print(f"ğŸ¯ æ”¶é›†åˆ° {len(attention_weights)} ä¸ªæ³¨æ„åŠ›æƒé‡:")
            for key, tensor in attention_weights.items():
                print(f"   - {key}: {tensor.shape}")

        return attention_weights

    def _extract_edge_type_from_module_name(self, module_name: str) -> Optional[str]:
        """
        ä»æ¨¡å—åç§°ä¸­æå–è¾¹ç±»å‹ä¿¡æ¯

        to_heteroè½¬æ¢åçš„æ¨¡å—åç§°æ ¼å¼é€šå¸¸ä¸º:
        convs.{layer_idx}.{edge_type_encoded}

        å‚æ•°:
            module_name: æ¨¡å—çš„å®Œæ•´åç§°è·¯å¾„

        è¿”å›:
            edge_type_key: æ ¼å¼åŒ–çš„è¾¹ç±»å‹é”®ï¼Œå¦‚ "bus_pv__connects_line__bus_slack"
        """
        try:
            from rich_output import rich_debug
            rich_debug(f"å°è¯•ä»æ¨¡å—åæå–è¾¹ç±»å‹: {module_name}", "attention")
        except ImportError:
            pass

        # è§£ææ¨¡å—åç§°ä»¥æå–è¾¹ç±»å‹ä¿¡æ¯
        # to_heteroä¼šå°†è¾¹ç±»å‹ç¼–ç åˆ°æ¨¡å—åä¸­

        # æ–¹æ³•1: å°è¯•ä»å·²çŸ¥çš„è¾¹ç±»å‹ä¸­åŒ¹é…
        for edge_type in self.edge_types:
            src_type, relation, dst_type = edge_type
            edge_type_key = f"{src_type}__{relation}__{dst_type}"

            # æ£€æŸ¥æ¨¡å—åç§°æ˜¯å¦åŒ…å«è¾¹ç±»å‹ä¿¡æ¯
            if (src_type in module_name and dst_type in module_name and
                relation in module_name):
                try:
                    from rich_output import rich_debug
                    rich_debug(f"åŒ¹é…åˆ°è¾¹ç±»å‹: {edge_type_key}", "attention")
                except ImportError:
                    pass
                return edge_type_key

        # æ–¹æ³•2: å°è¯•è§£æto_heteroçš„ç¼–ç æ ¼å¼
        # to_heteroé€šå¸¸ä½¿ç”¨æ•°å­—ç´¢å¼•æ¥ç¼–ç è¾¹ç±»å‹
        import re

        # æŸ¥æ‰¾ç±»ä¼¼ "convs.0.1" çš„æ¨¡å¼ï¼Œå…¶ä¸­æœ€åçš„æ•°å­—å¯èƒ½æ˜¯è¾¹ç±»å‹ç´¢å¼•
        pattern = r'convs\.(\d+)\.(\d+)'
        match = re.search(pattern, module_name)

        if match:
            layer_idx = int(match.group(1))
            edge_type_idx = int(match.group(2))

            # å°è¯•æ ¹æ®ç´¢å¼•æ˜ å°„åˆ°è¾¹ç±»å‹
            if edge_type_idx < len(self.edge_types):
                edge_type = self.edge_types[edge_type_idx]
                src_type, relation, dst_type = edge_type
                edge_type_key = f"{src_type}__{relation}__{dst_type}"
                try:
                    from rich_output import rich_debug
                    rich_debug(f"é€šè¿‡ç´¢å¼•åŒ¹é…åˆ°è¾¹ç±»å‹: {edge_type_key} (ç´¢å¼•: {edge_type_idx})", "attention")
                except ImportError:
                    pass
                return edge_type_key

        # æ–¹æ³•3: å¦‚æœæ— æ³•åŒ¹é…ï¼Œè¿”å›é€šç”¨é”®
        if "conv" in module_name:
            fallback_key = f"unknown_edge_type_{module_name.replace('.', '_')}"
            try:
                from rich_output import rich_debug
                rich_debug(f"ä½¿ç”¨å¤‡ç”¨é”®: {fallback_key}", "attention")
            except ImportError:
                pass
            return fallback_key

        try:
            from rich_output import rich_debug
            rich_debug("æ— æ³•æå–è¾¹ç±»å‹", "attention")
        except ImportError:
            pass
        return None

    def get_attention_weights_legacy(self) -> List[torch.Tensor]:
        """
        è·å–æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ–åˆ†æï¼ˆæ—§ç‰ˆæœ¬æ¥å£ï¼‰

        æ³¨æ„ï¼što_heteroè½¬æ¢åï¼Œæ¨¡å‹ç»“æ„ä¼šå‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦é€’å½’æœç´¢æ‰€æœ‰æ¨¡å—
        """
        attention_weights = []

        def collect_attention_weights(module):
            """é€’å½’æ”¶é›†æ³¨æ„åŠ›æƒé‡"""
            # æ£€æŸ¥å½“å‰æ¨¡å—æ˜¯å¦æ˜¯PhysicsGATv2Conv
            if isinstance(module, PhysicsGATv2Conv):
                if hasattr(module, '_alpha') and module._alpha is not None:
                    attention_weights.append(module._alpha)

            # é€’å½’æ£€æŸ¥å­æ¨¡å—
            for child in module.children():
                collect_attention_weights(child)

        # ä»hetero_encoderå¼€å§‹é€’å½’æœç´¢
        if hasattr(self, 'hetero_encoder'):
            collect_attention_weights(self.hetero_encoder)

        return attention_weights

    def get_embedding_dim(self) -> int:
        """
        è·å–åµŒå…¥ç»´åº¦
        """
        return self.output_dim


def create_hetero_graph_encoder(data: HeteroData, 
                               hidden_channels: int = 64,
                               gnn_layers: int = 3,
                               heads: int = 4,
                               dropout: float = 0.3,
                               output_dim: Optional[int] = None) -> HeteroGraphEncoder:
    """
    åˆ›å»ºå¼‚æ„å›¾ç¼–ç å™¨çš„ä¾¿æ·å‡½æ•°
    
    å‚æ•°:
        data: å¼‚æ„å›¾æ•°æ®ç¤ºä¾‹
        hidden_channels: éšè—å±‚ç»´åº¦
        gnn_layers: GNNå±‚æ•°
        heads: æ³¨æ„åŠ›å¤´æ•°
        dropout: dropoutæ¦‚ç‡
        output_dim: è¾“å‡ºåµŒå…¥ç»´åº¦
    
    è¿”å›:
        é…ç½®å¥½çš„HeteroGraphEncoderæ¨¡å‹
    """
    # æå–èŠ‚ç‚¹å’Œè¾¹ç‰¹å¾ç»´åº¦
    node_feature_dims = {node_type: x.shape[1] for node_type, x in data.x_dict.items()}
    edge_feature_dims = {edge_type: attr.shape[1] for edge_type, attr in data.edge_attr_dict.items()}
    
    # åˆ›å»ºç¼–ç å™¨
    encoder = HeteroGraphEncoder(
        node_feature_dims=node_feature_dims,
        edge_feature_dims=edge_feature_dims,
        metadata=data.metadata(),
        hidden_channels=hidden_channels,
        gnn_layers=gnn_layers,
        heads=heads,
        dropout=dropout,
        output_dim=output_dim
    )
    
    return encoder


def validate_encoder_architecture(data: HeteroData,
                                 hidden_channels: int = 64,
                                 gnn_layers: int = 3,
                                 heads: int = 8,
                                 output_dim: int = 128,
                                 device: torch.device = None) -> 'HeteroGraphEncoder':
    """
    éªŒè¯å¹¶åˆ›å»ºç”Ÿäº§çº§å¼‚æ„å›¾ç¼–ç å™¨

    Args:
        data: å¼‚æ„å›¾æ•°æ®
        hidden_channels: éšè—å±‚ç»´åº¦
        gnn_layers: GNNå±‚æ•°
        heads: æ³¨æ„åŠ›å¤´æ•°
        output_dim: è¾“å‡ºåµŒå…¥ç»´åº¦
        device: è®¡ç®—è®¾å¤‡

    Returns:
        éªŒè¯é€šè¿‡çš„ç¼–ç å™¨å®ä¾‹

    Raises:
        ValueError: å¦‚æœæ¶æ„éªŒè¯å¤±è´¥
        RuntimeError: å¦‚æœå‰å‘ä¼ æ’­å¤±è´¥
    """
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # éªŒè¯è¾“å…¥æ•°æ®æ ¼å¼
    if not isinstance(data, HeteroData):
        raise ValueError("è¾“å…¥æ•°æ®å¿…é¡»æ˜¯HeteroDataæ ¼å¼")

    # éªŒè¯å¿…è¦çš„èŠ‚ç‚¹ç±»å‹å’Œè¾¹ç±»å‹
    required_node_types = ['bus']
    required_edge_types = [('bus', 'connects', 'bus')]

    for node_type in required_node_types:
        if node_type not in data.node_types:
            raise ValueError(f"ç¼ºå°‘å¿…è¦çš„èŠ‚ç‚¹ç±»å‹: {node_type}")

    for edge_type in required_edge_types:
        if edge_type not in data.edge_types:
            raise ValueError(f"ç¼ºå°‘å¿…è¦çš„è¾¹ç±»å‹: {edge_type}")

    # åˆ›å»ºç¼–ç å™¨
    try:
        encoder = create_hetero_graph_encoder(
            data,
            hidden_channels=hidden_channels,
            gnn_layers=gnn_layers,
            heads=heads,
            output_dim=output_dim
        )
        encoder = encoder.to(device)
        data = data.to(device)
    except Exception as e:
        raise RuntimeError(f"ç¼–ç å™¨åˆ›å»ºå¤±è´¥: {str(e)}")

    # éªŒè¯å‰å‘ä¼ æ’­
    try:
        with torch.no_grad():
            # éªŒè¯èŠ‚ç‚¹åµŒå…¥æå–
            node_embeddings = encoder.encode_nodes(data)

            # éªŒè¯å›¾çº§åˆ«åµŒå…¥
            graph_embedding = encoder.encode_graph(data)

            # éªŒè¯å®Œæ•´å‰å‘ä¼ æ’­
            node_emb, attention_weights, graph_emb = encoder(
                data,
                return_attention_weights=True,
                return_graph_embedding=True
            )

            # éªŒè¯è¾“å‡ºç»´åº¦
            expected_dim = encoder.get_embedding_dim()
            for node_type, embeddings in node_embeddings.items():
                if embeddings.shape[-1] != expected_dim:
                    raise ValueError(f"èŠ‚ç‚¹ç±»å‹ {node_type} çš„åµŒå…¥ç»´åº¦ä¸åŒ¹é…: "
                                   f"æœŸæœ› {expected_dim}, å®é™… {embeddings.shape[-1]}")

            if graph_embedding.shape[-1] != expected_dim:
                raise ValueError(f"å›¾çº§åˆ«åµŒå…¥ç»´åº¦ä¸åŒ¹é…: "
                               f"æœŸæœ› {expected_dim}, å®é™… {graph_embedding.shape[-1]}")

    except Exception as e:
        raise RuntimeError(f"å‰å‘ä¼ æ’­éªŒè¯å¤±è´¥: {str(e)}")

    return encoder


def create_production_encoder(data: HeteroData, config: Dict[str, Any] = None) -> 'HeteroGraphEncoder':
    """
    åˆ›å»ºç”Ÿäº§çº§å¼‚æ„å›¾ç¼–ç å™¨

    Args:
        data: å¼‚æ„å›¾æ•°æ®
        config: ç¼–ç å™¨é…ç½®å‚æ•°

    Returns:
        é…ç½®å¥½çš„ç¼–ç å™¨å®ä¾‹
    """
    if config is None:
        config = {
            'hidden_channels': 64,
            'gnn_layers': 3,
            'heads': 8,
            'output_dim': 128,
            'dropout': 0.1,
            'physics_enhanced': True,
            'temperature': 1.0,
            'physics_weight': 1.0
        }

    return validate_encoder_architecture(
        data,
        hidden_channels=config.get('hidden_channels', 64),
        gnn_layers=config.get('gnn_layers', 3),
        heads=config.get('heads', 8),
        output_dim=config.get('output_dim', 128)
    )

