#!/usr/bin/env python3
"""
Enhanced Actor Network with Two-Tower Architecture

é‡æ„çš„Actorç½‘ç»œï¼Œå®ç°åŒå¡”æ¶æ„ï¼š
- çŠ¶æ€å¡”ï¼šè¾“å‡ºå›ºå®šç»´åº¦çš„ç­–ç•¥å‘é‡
- åŠ¨ä½œå¡”ï¼šç”±PartitionEncoderå®ç°ï¼ˆå¤–éƒ¨ç»„ä»¶ï¼‰

å…³é”®æ”¹è¿›ï¼š
- è§£è€¦Kå€¼ä¾èµ–
- æ”¯æŒåŠ¨ä½œåµŒå…¥æœºåˆ¶
- ä¿æŒä¸ç°æœ‰æ¥å£çš„å…¼å®¹æ€§
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import logging

logger = logging.getLogger(__name__)


def scatter_mean(src: torch.Tensor, index: torch.Tensor, dim: int = 0) -> torch.Tensor:
    """
    è®¡ç®—åˆ†ç»„å¹³å‡ï¼ˆscatter meanï¼‰

    Args:
        src: æºå¼ é‡
        index: åˆ†ç»„ç´¢å¼•
        dim: èšåˆç»´åº¦

    Returns:
        åˆ†ç»„å¹³å‡ç»“æœ
    """
    if src.numel() == 0:
        return torch.zeros(0, *src.shape[1:], dtype=src.dtype, device=src.device)

    dim_size = int(index.max()) + 1
    out = torch.zeros(dim_size, *src.shape[1:], dtype=src.dtype, device=src.device)

    # ç¡®ä¿indexçš„å½¢çŠ¶ä¸srcçš„ç¬¬ä¸€ä¸ªç»´åº¦åŒ¹é…
    if index.dim() != 1 or index.size(0) != src.size(0):
        raise ValueError(f"Index shape {index.shape} doesn't match src first dimension {src.size(0)}")

    ones = torch.ones(index.size(0), dtype=src.dtype, device=src.device)
    count = torch.zeros(dim_size, dtype=src.dtype, device=src.device)
    count.scatter_add_(0, index, ones)

    # æ‰©å±•ç´¢å¼•å¼ é‡ä»¥åŒ¹é…æºå¼ é‡çš„ç»´åº¦
    if src.dim() > 1:
        # å°†1ç»´ç´¢å¼•æ‰©å±•ä¸ºä¸srcç›¸åŒçš„ç»´åº¦
        index_expanded = index.unsqueeze(-1).expand(-1, src.size(1))
        out.scatter_add_(0, index_expanded, src)
    else:
        out.scatter_add_(0, index, src)
    count = count.clamp(min=1)
    count = count.view(-1, *([1] * (src.dim() - 1)))
    return out / count


class EnhancedActorNetwork(nn.Module):
    """
    å¢å¼ºçš„Actorç½‘ç»œ - çŠ¶æ€å¡”å®ç°
    
    ä¸å†ç›´æ¥è¾“å‡ºåˆ†åŒºæ¦‚ç‡ï¼Œè€Œæ˜¯è¾“å‡ºç­–ç•¥å‘é‡ï¼Œ
    ç”¨äºä¸åˆ†åŒºåµŒå…¥è¿›è¡Œç›¸ä¼¼åº¦åŒ¹é…ã€‚
    """
    
    def __init__(self,
                 node_embedding_dim: int,
                 region_embedding_dim: int,
                 strategy_vector_dim: int = 128,  # æ›¿ä»£num_partitions
                 hidden_dim: int = 256,
                 dropout: float = 0.1,
                 use_layer_norm: bool = True):
        """
        Args:
            node_embedding_dim: èŠ‚ç‚¹åµŒå…¥ç»´åº¦
            region_embedding_dim: åŒºåŸŸåµŒå…¥ç»´åº¦
            strategy_vector_dim: ç­–ç•¥å‘é‡ç»´åº¦ï¼ˆéœ€ä¸PartitionEncoderçš„embedding_dimä¸€è‡´ï¼‰
            hidden_dim: éšè—å±‚ç»´åº¦
            dropout: Dropoutæ¦‚ç‡
            use_layer_norm: æ˜¯å¦ä½¿ç”¨å±‚å½’ä¸€åŒ–
        """
        super().__init__()
        
        self.node_embedding_dim = node_embedding_dim
        self.region_embedding_dim = region_embedding_dim
        self.strategy_vector_dim = strategy_vector_dim
        self.hidden_dim = hidden_dim
        
        # èŠ‚ç‚¹é€‰æ‹©ç½‘ç»œï¼ˆä¿æŒä¸å˜ï¼‰
        self.node_selector = nn.Sequential(
            nn.Linear(node_embedding_dim + region_embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1)  # èŠ‚ç‚¹è¯„åˆ†
        )
        
        # ç­–ç•¥ç”Ÿæˆå™¨ï¼ˆæ›¿ä»£åŸpartition_selectorï¼‰
        # è¾“å‡ºå›ºå®šç»´åº¦çš„ç­–ç•¥å‘é‡ï¼Œè€ŒéKä¸ªåˆ†åŒºçš„æ¦‚ç‡
        layers = [
            nn.Linear(node_embedding_dim + region_embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, strategy_vector_dim)
        ]
        
        # å¯é€‰çš„è¾“å‡ºå½’ä¸€åŒ–
        if use_layer_norm:
            layers.append(nn.LayerNorm(strategy_vector_dim))
            
        self.strategy_generator = nn.Sequential(*layers)
        
        # å…¨å±€çŠ¶æ€ç¼–ç å™¨ï¼ˆä¿æŒä¸å˜ï¼‰
        self.global_encoder = nn.Sequential(
            nn.Linear(region_embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, region_embedding_dim)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
        
        logger.info(f"EnhancedActorNetwork initialized: "
                   f"strategy_vector_dim={strategy_vector_dim}, "
                   f"hidden_dim={hidden_dim}")
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)
    
    def forward(self, batched_state):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            batched_state: æ‰¹å¤„ç†çŠ¶æ€ï¼ˆä¸åŸæ¥å£å…¼å®¹ï¼‰
        
        Returns:
            Tuple[torch.Tensor, torch.Tensor]:
                - node_logits: è¾¹ç•ŒèŠ‚ç‚¹é€‰æ‹©åˆ†æ•° [num_boundary_nodes]
                - strategy_vectors: æ¯ä¸ªè¾¹ç•ŒèŠ‚ç‚¹çš„ç­–ç•¥å‘é‡ [num_boundary_nodes, strategy_vector_dim]
        """
        bs = batched_state
        
        # ç©ºæ‰¹æ¬¡ä¿æŠ¤
        if bs.region_embeddings.numel() == 0 or bs.boundary_nodes.numel() == 0:
            return (torch.zeros(0, device=bs.node_embeddings.device),
                    torch.zeros(0, self.strategy_vector_dim, device=bs.node_embeddings.device))
        
        # è®¡ç®—æ‰¹æ¬¡çº§å…¨å±€ä¸Šä¸‹æ–‡
        global_context_per_batch = self.global_encoder(
            scatter_mean(bs.region_embeddings, bs.region_batch_idx, dim=0)
        )  # [batch_size, region_dim]
        
        # è·å–è¾¹ç•ŒèŠ‚ç‚¹çš„æ‰¹æ¬¡ç´¢å¼•å’Œä¸Šä¸‹æ–‡
        boundary_batch_idx = bs.node_batch_idx[bs.boundary_nodes]
        global_context = global_context_per_batch[boundary_batch_idx]
        
        # è·å–è¾¹ç•ŒèŠ‚ç‚¹åµŒå…¥
        boundary_embeddings = bs.node_embeddings[bs.boundary_nodes]
        
        # ç»„åˆç‰¹å¾
        combined_features = torch.cat([boundary_embeddings, global_context], dim=1)
        
        # è®¡ç®—èŠ‚ç‚¹é€‰æ‹©logits - ä¿®å¤ç»´åº¦ä¸åŒ¹é…é—®é¢˜
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿combined_featuresçš„ç»´åº¦ä¸node_selectoræœŸæœ›çš„ç»´åº¦åŒ¹é…
        if combined_features.size(-1) != self.node_selector[0].in_features:
            expected_dim = self.node_selector[0].in_features
            actual_dim = combined_features.size(-1)
            
            if actual_dim < expected_dim:
                # ç»´åº¦ä¸è¶³ï¼šé›¶å¡«å……
                padding = torch.zeros(*combined_features.shape[:-1], expected_dim - actual_dim, 
                                    device=combined_features.device, dtype=combined_features.dtype)
                combined_features = torch.cat([combined_features, padding], dim=-1)
            else:
                # ç»´åº¦è¿‡å¤šï¼šæˆªå–
                combined_features = combined_features[..., :expected_dim]
        
        node_logits = self.node_selector(combined_features).squeeze(-1)
        
        # ç”Ÿæˆç­–ç•¥å‘é‡ï¼ˆæ›¿ä»£partition_logitsï¼‰
        strategy_vectors = self.strategy_generator(combined_features)
        
        return node_logits, strategy_vectors
    
    def get_node_probabilities(self, node_logits: torch.Tensor, 
                              temperature: float = 1.0) -> torch.Tensor:
        """
        è®¡ç®—èŠ‚ç‚¹é€‰æ‹©æ¦‚ç‡
        
        Args:
            node_logits: èŠ‚ç‚¹logits
            temperature: æ¸©åº¦å‚æ•°
        
        Returns:
            èŠ‚ç‚¹é€‰æ‹©æ¦‚ç‡åˆ†å¸ƒ
        """
        return F.softmax(node_logits / temperature, dim=0)
    
    def compute_action_probabilities(self, 
                                   strategy_vector: torch.Tensor,
                                   partition_embeddings: torch.Tensor,
                                   action_mask: torch.Tensor,
                                   temperature: float = 1.0) -> torch.Tensor:
        """
        åŸºäºç­–ç•¥å‘é‡å’Œåˆ†åŒºåµŒå…¥è®¡ç®—åŠ¨ä½œæ¦‚ç‡
        
        Args:
            strategy_vector: ç­–ç•¥å‘é‡ [strategy_vector_dim]
            partition_embeddings: åˆ†åŒºåµŒå…¥ [num_partitions, embedding_dim]
            action_mask: åŠ¨ä½œæ©ç  [num_partitions]
            temperature: æ¸©åº¦å‚æ•°
        
        Returns:
            åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ [num_partitions]
        """
        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç‚¹ç§¯ï¼‰
        similarities = torch.matmul(strategy_vector, partition_embeddings.t())
        
        # åº”ç”¨æ¸©åº¦ç¼©æ”¾
        logits = similarities / temperature
        
        # åº”ç”¨æ©ç 
        masked_logits = logits.masked_fill(~action_mask, -1e9)
        
        # è®¡ç®—æ¦‚ç‡
        return F.softmax(masked_logits, dim=-1)





def create_actor_network(config: Dict, use_two_tower: bool = True) -> nn.Module:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºActorç½‘ç»œ
    
    Args:
        config: é…ç½®å­—å…¸
        use_two_tower: æ˜¯å¦ä½¿ç”¨åŒå¡”æ¶æ„
    
    Returns:
        Actorç½‘ç»œå®ä¾‹
    """
    return EnhancedActorNetwork(
        node_embedding_dim=config['node_embedding_dim'],
        region_embedding_dim=config['region_embedding_dim'],
        strategy_vector_dim=config.get('strategy_vector_dim', 128),
        hidden_dim=config.get('hidden_dim', 256),
        dropout=config.get('dropout', 0.1)
    )