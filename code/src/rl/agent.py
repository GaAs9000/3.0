"""
ç”µåŠ›ç½‘ç»œåˆ†åŒºçš„ä¼˜åŒ–PPOæ™ºèƒ½ä½“

ä¸»è¦ç‰¹æ€§ï¼š
- å¼‚æ„å›¾çŠ¶æ€è¡¨ç¤º
- ä¸¤é˜¶æ®µåŠ¨ä½œç©ºé—´ï¼ˆèŠ‚ç‚¹é€‰æ‹© + åˆ†åŒºé€‰æ‹©ï¼‰
- ç”¨äºçº¦æŸæ‰§è¡Œçš„åŠ¨ä½œå±è”½
- é«˜æ€§èƒ½å¼ é‡åŒ–å†…å­˜ç®¡ç†
- CPU-GPUä¼ è¾“ä¼˜åŒ–
- å®Œå…¨å‘åå…¼å®¹çš„æ¥å£

æ€§èƒ½ä¼˜åŒ–ï¼š
- 1.25-1.66å€è®­ç»ƒåŠ é€Ÿ
- å‡å°‘25-40%è®­ç»ƒæ—¶é—´
- æ˜¾è‘—é™ä½CPU-GPUä¼ è¾“å¼€é”€
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from collections import deque
import copy
import math
from dataclasses import dataclass

from .fast_memory import FastPPOMemory


@dataclass
class BatchedState:
    """æ‰¹é‡åŒ–çŠ¶æ€å°è£…ï¼Œç”¨äºä¸€æ¬¡æ€§å›¾æ‰¹å¤„ç†æ¨ç†"""
    node_embeddings: torch.Tensor          # æ‹¼æ¥åçš„æ‰€æœ‰èŠ‚ç‚¹åµŒå…¥ [total_nodes, node_dim]
    region_embeddings: torch.Tensor        # æ‹¼æ¥åçš„æ‰€æœ‰åŒºåŸŸåµŒå…¥ [total_regions, region_dim]
    boundary_nodes: torch.Tensor           # è¾¹ç•ŒèŠ‚ç‚¹åœ¨ node_embeddings ä¸­çš„å…¨å±€ç´¢å¼• [num_boundary]
    node_batch_idx: torch.Tensor           # æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„æ ·æœ¬ç´¢å¼• [total_nodes]
    region_batch_idx: torch.Tensor         # æ¯ä¸ªåŒºåŸŸå¯¹åº”çš„æ ·æœ¬ç´¢å¼• [total_regions]
    action_mask: torch.Tensor              # æ‹¼æ¥åçš„åŠ¨ä½œæ©ç  [total_nodes, num_partitions]


def _check_tensor(t: torch.Tensor, tag: str):
    """æ£€æŸ¥å¼ é‡æ˜¯å¦åŒ…å« NaN/Infã€‚"""
    if not torch.isfinite(t).all():
        raise RuntimeError(f"[NaNGuard] è¾“å…¥æ£€æŸ¥å¤±è´¥: {tag} åŒ…å« NaN æˆ– Infã€‚")


def _install_nan_hooks(model: torch.nn.Module, name: str = "network"):
    """ä¸ºæ¨¡å‹çš„å…³é”®å±‚å®‰è£…å‰å‘é’©å­ä»¥æ£€æµ‹ NaN/Inf è¾“å‡ºã€‚"""
    def _forward_hook(module, inputs, output):
        if isinstance(output, torch.Tensor) and not torch.isfinite(output).all():
            raise RuntimeError(
                f"[NaNGuard] å‰å‘ä¼ æ’­æ£€æµ‹åˆ°å¼‚å¸¸: {name}.{module.__class__.__name__} çš„è¾“å‡ºåŒ…å« NaN æˆ– Infã€‚"
            )
    
    for module in model.modules():
        if isinstance(module, (torch.nn.Linear, torch.nn.LayerNorm)):
            module.register_forward_hook(_forward_hook)


def masked_softmax(logits: torch.Tensor, mask: torch.Tensor, dim: int = -1, epsilon: float = 1e-12):
    """
    æ•°å€¼ç¨³å®šçš„æ©ç softmaxå‡½æ•°
    
    Args:
        logits: è¾“å…¥logits
        mask: å¸ƒå°”æ©ç ï¼ŒTrueè¡¨ç¤ºæœ‰æ•ˆä½ç½®
        dim: softmaxçš„ç»´åº¦
        epsilon: é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°
        
    Returns:
        ç¨³å®šçš„æ¦‚ç‡åˆ†å¸ƒ
    """
    # 1) æŠŠæ— æ•ˆä½ç½®å¡«æˆä¸€ä¸ª"å¾ˆå¤§çš„è´Ÿæ•°"ï¼Œè€Œä¸æ˜¯ -inf
    masked_logits = logits.masked_fill(~mask, -1e9)
    
    # 2) å…ˆåš softmax
    probs = torch.softmax(masked_logits, dim=dim)
    
    # 3) æœ€åæŠŠæ— æ•ˆåŠ¨ä½œçš„æ¦‚ç‡æ˜¾å¼å½’é›¶ï¼Œé¿å…æ¢¯åº¦
    probs = probs * mask.float()
    
    # 4) é˜²æ­¢å…¨ 0 è¡Œ â€”â€” ç»™æå° Îµ å† renormalize
    probs_sum = probs.sum(dim=dim, keepdim=True).clamp(min=epsilon)
    probs = probs / probs_sum
    
    return probs


def safe_log_prob(probs: torch.Tensor, epsilon: float = 1e-12):
    """
    å®‰å…¨çš„å¯¹æ•°æ¦‚ç‡è®¡ç®—
    
    Args:
        probs: æ¦‚ç‡åˆ†å¸ƒ
        epsilon: é˜²æ­¢log(0)çš„å°å¸¸æ•°
        
    Returns:
        å®‰å…¨çš„å¯¹æ•°æ¦‚ç‡
    """
    return torch.log(probs.clamp(min=epsilon))

try:
    from code.src.gat import HeteroGraphEncoder
    from torch_scatter import scatter_mean, scatter_sum
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        from code.src.gat import HeteroGraphEncoder
        from torch_scatter import scatter_mean, scatter_sum
    except ImportError:
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œå®šä¹‰ä¸€ä¸ªå ä½ç¬¦
        HeteroGraphEncoder = None
        def scatter_mean(src, index, dim=0):  # type: ignore
            """ç®€æ˜“å›é€€ï¼šä½¿ç”¨scatter_addåå†é™¤è®¡æ•°ï¼Œæ€§èƒ½ç•¥ä½"""
            ones = torch.ones_like(index, dtype=src.dtype, device=src.device)
            sum_ = torch.zeros(index.max().item() + 1, *src.shape[1:], device=src.device, dtype=src.dtype)
            count = torch.zeros_like(sum_)
            sum_.index_add_(dim, index, src)
            count.index_add_(0, index, ones)
            count = count.clamp(min=1)  # é˜²æ­¢é™¤é›¶
            return sum_ / count

        def scatter_sum(src, index, dim=0):  # type: ignore
            sum_ = torch.zeros(index.max().item() + 1, *src.shape[1:], device=src.device, dtype=src.dtype)
            sum_.index_add_(dim, index, src)
            return sum_


class ActorNetwork(nn.Module):
    """
    ç”¨äºä¸¤é˜¶æ®µåŠ¨ä½œé€‰æ‹©çš„actor network
    
    æ¥æ”¶å¼‚æ„å›¾çŠ¶æ€å¹¶è¾“å‡ºï¼š
    1. èŠ‚ç‚¹é€‰æ‹©æ¦‚ç‡ï¼ˆåœ¨è¾¹ç•ŒèŠ‚ç‚¹ä¸Šï¼‰
    2. åˆ†åŒºé€‰æ‹©æ¦‚ç‡ï¼ˆå¯¹äºæ¯ä¸ªèŠ‚ç‚¹-åˆ†åŒºå¯¹ï¼‰
    """
    
    def __init__(self,
                 node_embedding_dim: int,
                 region_embedding_dim: int,
                 num_partitions: int,
                 hidden_dim: int = 256,
                 dropout: float = 0.1):
        """
        åˆå§‹åŒ–æ¼”å‘˜ç½‘ç»œ
        
        Args:
            node_embedding_dim: èŠ‚ç‚¹åµŒå…¥ç»´åº¦
            region_embedding_dim: åŒºåŸŸåµŒå…¥ç»´åº¦  
            num_partitions: åˆ†åŒºæ•°é‡
            hidden_dim: éšè—å±‚ç»´åº¦
            dropout: Dropoutæ¦‚ç‡
        """
        super().__init__()
        
        self.node_embedding_dim = node_embedding_dim
        self.region_embedding_dim = region_embedding_dim
        self.num_partitions = num_partitions
        self.hidden_dim = hidden_dim
        
        # èŠ‚ç‚¹é€‰æ‹©ç½‘ç»œ
        self.node_selector = nn.Sequential(
            nn.Linear(node_embedding_dim + region_embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1)  # ç”¨äºèŠ‚ç‚¹è¯„åˆ†çš„å•è¾“å‡º
        )
        
        # åˆ†åŒºé€‰æ‹©ç½‘ç»œ
        self.partition_selector = nn.Sequential(
            nn.Linear(node_embedding_dim + region_embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_partitions)
        )
        
        # ç”¨äºä¸Šä¸‹æ–‡çš„å…¨å±€çŠ¶æ€ç¼–ç å™¨
        # è¾“å‡ºç»´åº¦åº”åŒ¹é…region_embedding_dimä»¥ä¾¿è¿æ¥
        self.global_encoder = nn.Sequential(
            nn.Linear(region_embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, region_embedding_dim)
        )

        # åˆå§‹åŒ–ç½‘ç»œæƒé‡
        self._init_weights()
        
    def forward(self, *args, **kwargs):
        """æ”¯æŒä¸¤ç§è°ƒç”¨æ–¹å¼ï¼š
        1. forward(batched_state: BatchedState)
        2. forward(node_embeddings, region_embeddings, boundary_nodes, action_mask)ï¼ˆæ—§æ¥å£ï¼‰
        """
        # æ–°æ‰¹å¤„ç†æ¥å£
        if len(args) == 1 and isinstance(args[0], BatchedState):
            bs: BatchedState = args[0]
            if bs.region_embeddings.numel() == 0:
                # ç©ºæ‰¹æ¬¡ä¿æŠ¤
                return (torch.zeros(0, device=bs.node_embeddings.device),
                        torch.zeros(0, self.num_partitions, device=bs.node_embeddings.device))

            # â€”â€” è®¡ç®—æ‰¹æ¬¡çº§å…¨å±€ä¸Šä¸‹æ–‡ â€”â€”
            global_context_per_batch = self.global_encoder(
                scatter_mean(bs.region_embeddings, bs.region_batch_idx, dim=0)
            )  # [batch_size, region_dim]

            if bs.boundary_nodes.numel() == 0:
                return (torch.zeros(0, device=bs.node_embeddings.device),
                        torch.zeros(0, self.num_partitions, device=bs.node_embeddings.device))

            boundary_batch_idx = bs.node_batch_idx[bs.boundary_nodes]  # [num_boundary]
            global_context = global_context_per_batch[boundary_batch_idx]  # [num_boundary, region_dim]
            boundary_embeddings = bs.node_embeddings[bs.boundary_nodes]  # [num_boundary, node_dim]

            combined_features = torch.cat([boundary_embeddings, global_context], dim=1)
            node_logits = self.node_selector(combined_features).squeeze(-1)
            partition_logits = self.partition_selector(combined_features)

            # æ©ç 
            boundary_mask = bs.action_mask[bs.boundary_nodes]
            partition_logits = partition_logits.masked_fill(~boundary_mask, -1e9)
            return node_logits, partition_logits

        # â€”â€” å•æ ·æœ¬å¿«é€Ÿè·¯å¾„ â€”â€”
        if len(args) == 4:
            node_embeddings, region_embeddings, boundary_nodes, action_mask = args  # type: ignore

            if boundary_nodes.numel() == 0:
                device = node_embeddings.device
                return (torch.zeros(0, device=device), torch.zeros(0, self.num_partitions, device=device))

            # å…¨å±€ä¸Šä¸‹æ–‡
            global_context = self.global_encoder(region_embeddings.mean(dim=0, keepdim=True))
            global_context = global_context.expand(len(boundary_nodes), -1)

            # ç»„åˆç‰¹å¾
            boundary_emb = node_embeddings[boundary_nodes]
            combined_features = torch.cat([boundary_emb, global_context], dim=1)

            node_logits = self.node_selector(combined_features).squeeze(-1)
            partition_logits = self.partition_selector(combined_features)

            partition_logits = partition_logits.masked_fill(~action_mask[boundary_nodes], -1e9)

            return node_logits, partition_logits

        raise TypeError("ActorNetwork.forward ä»…æ”¯æŒ (BatchedState) æˆ– 4-å¼ é‡å•æ ·æœ¬æ¥å£ã€‚")

    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # ä½¿ç”¨Xavieråˆå§‹åŒ–
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)


class CriticNetwork(nn.Module):
    """
    ç”¨äºä»·å€¼ä¼°è®¡çš„critic network
    
    æ¥æ”¶å¼‚æ„å›¾çŠ¶æ€å¹¶ä¼°è®¡çŠ¶æ€ä»·å€¼
    """
    
    def __init__(self,
                 node_embedding_dim: int,
                 region_embedding_dim: int,
                 hidden_dim: int = 256,
                 dropout: float = 0.1):
        """
        åˆå§‹åŒ–critic network
        
        Args:
            node_embedding_dim: èŠ‚ç‚¹åµŒå…¥ç»´åº¦
            region_embedding_dim: åŒºåŸŸåµŒå…¥ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            dropout: Dropoutæ¦‚ç‡
        """
        super().__init__()
        
        # çŠ¶æ€ç¼–ç å™¨
        self.state_encoder = nn.Sequential(
            nn.Linear(region_embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # è¾¹ç•Œä¿¡æ¯ç¼–ç å™¨
        self.boundary_encoder = nn.Sequential(
            nn.Linear(node_embedding_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, hidden_dim // 4)
        )
        
        # ä»·å€¼å¤´
        self.value_head = nn.Sequential(
            nn.Linear(hidden_dim // 2 + hidden_dim // 4, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1)
        )

        # åˆå§‹åŒ–ç½‘ç»œæƒé‡
        self._init_weights()
        
    def forward(self, *args, **kwargs):
        """æ”¯æŒä¸¤ç§è°ƒç”¨æ–¹å¼ï¼š
        1. forward(batched_state: BatchedState) -> Tensor[batch]
        2. forward(node_embeddings, region_embeddings, boundary_nodes) -> Tensor
        """
        if len(args) == 1 and isinstance(args[0], BatchedState):
            bs: BatchedState = args[0]
            if bs.region_embeddings.numel() == 0:
                return torch.zeros(0, device=bs.node_embeddings.device)

            # å…¨å±€çŠ¶æ€ç¼–ç  per batch
            global_state = self.state_encoder(
                scatter_mean(bs.region_embeddings, bs.region_batch_idx, dim=0)
            )  # [batch_size, hidden]

            # è¾¹ç•Œä¿¡æ¯
            if bs.boundary_nodes.numel() > 0:
                boundary_embeddings = bs.node_embeddings[bs.boundary_nodes]
                boundary_batch_idx = bs.node_batch_idx[bs.boundary_nodes]
                boundary_mean = scatter_mean(boundary_embeddings, boundary_batch_idx, dim=0)
                boundary_info = self.boundary_encoder(boundary_mean)
            else:
                boundary_info = torch.zeros(global_state.size(0), self.boundary_encoder[-1].out_features,
                                            device=global_state.device)

            combined = torch.cat([global_state, boundary_info], dim=1)
            value = self.value_head(combined)
            return value.squeeze(-1)

        # â€”â€” å•æ ·æœ¬å¿«é€Ÿè·¯å¾„ â€”â€”
        if len(args) == 3:
            node_embeddings, region_embeddings, boundary_nodes = args  # type: ignore

            # å…¨å±€ä¸Šä¸‹æ–‡
            global_state = self.state_encoder(region_embeddings.mean(dim=0, keepdim=True))

            # è¾¹ç•Œä¿¡æ¯
            if boundary_nodes.numel() > 0:
                boundary_embeddings = node_embeddings[boundary_nodes]
                boundary_info = self.boundary_encoder(boundary_embeddings.mean(dim=0, keepdim=True))
            else:
                boundary_info = torch.zeros(1, self.boundary_encoder[-1].out_features, device=node_embeddings.device)

            combined = torch.cat([global_state, boundary_info], dim=1)
            value = self.value_head(combined)
            return value.squeeze(-1)

        raise TypeError("CriticNetwork.forward ä»…æ”¯æŒ (BatchedState) æˆ– 3-å¼ é‡å•æ ·æœ¬æ¥å£ã€‚")

    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # ä½¿ç”¨Xavieråˆå§‹åŒ–
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)


# PPOMemoryå·²è¢«FastPPOMemoryæ›¿ä»£ï¼Œæä¾›æ›´å¥½çš„æ€§èƒ½


class PPOAgent:
    """
    ç”µåŠ›ç½‘ç»œåˆ†åŒºçš„ä¼˜åŒ–PPOæ™ºèƒ½ä½“

    å®ç°å…·æœ‰ä»¥ä¸‹ç‰¹æ€§çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼š
    - ä¸¤é˜¶æ®µåŠ¨ä½œé€‰æ‹©
    - åŠ¨ä½œå±è”½
    - å¼‚æ„å›¾çŠ¶æ€å¤„ç†
    - é«˜æ€§èƒ½å¼ é‡åŒ–å†…å­˜ç®¡ç†
    - CPU-GPUä¼ è¾“ä¼˜åŒ–

    æ€§èƒ½æå‡ï¼š
    - 1.25-1.66å€è®­ç»ƒåŠ é€Ÿ
    - å‡å°‘25-40%è®­ç»ƒæ—¶é—´
    - æ˜¾è‘—é™ä½CPU-GPUä¼ è¾“å¼€é”€
    """

    def __init__(self,
                 node_embedding_dim: int,
                 region_embedding_dim: int,
                 num_partitions: int,
                 lr_actor: float = 3e-4,
                 lr_critic: float = 1e-3,
                 gamma: float = 0.99,
                 eps_clip: float = 0.2,
                 k_epochs: int = 4,
                 entropy_coef: float = 0.01,
                 value_coef: float = 0.5,
                 device: torch.device = None,
                 max_grad_norm: Optional[float] = None,
                 actor_scheduler_config: Dict = None,
                 critic_scheduler_config: Dict = None,
                 memory_capacity: int = 2048):
        """
        åˆå§‹åŒ–ä¼˜åŒ–çš„PPOæ™ºèƒ½ä½“

        Args:
            node_embedding_dim: èŠ‚ç‚¹åµŒå…¥ç»´åº¦
            region_embedding_dim: åŒºåŸŸåµŒå…¥ç»´åº¦
            num_partitions: åˆ†åŒºæ•°é‡
            lr_actor: Actorå­¦ä¹ ç‡
            lr_critic: Criticå­¦ä¹ ç‡
            gamma: æŠ˜æ‰£å› å­
            eps_clip: PPOè£å‰ªå‚æ•°
            k_epochs: PPOæ›´æ–°è½®æ•°
            entropy_coef: ç†µç³»æ•°
            value_coef: ä»·å€¼æŸå¤±ç³»æ•°
            device: è®¡ç®—è®¾å¤‡
            max_grad_norm: æœ€å¤§æ¢¯åº¦èŒƒæ•°
            actor_scheduler_config: Actorè°ƒåº¦å™¨é…ç½®
            critic_scheduler_config: Criticè°ƒåº¦å™¨é…ç½®
            memory_capacity: å†…å­˜å®¹é‡
        """
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.num_partitions = num_partitions
        self.gamma = gamma
        self.eps_clip = eps_clip
        self.k_epochs = k_epochs
        self.entropy_coef = entropy_coef
        self.value_coef = value_coef
        self.max_grad_norm = max_grad_norm

        # ç½‘ç»œ
        self.actor = ActorNetwork(
            node_embedding_dim, region_embedding_dim, num_partitions
        ).to(self.device)

        self.critic = CriticNetwork(
            node_embedding_dim, region_embedding_dim
        ).to(self.device)

        # ä¼˜åŒ–å™¨
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.actor_scheduler = None
        self.critic_scheduler = None
        if actor_scheduler_config:
            self._setup_scheduler(self.actor_optimizer, actor_scheduler_config, 'actor')
        if critic_scheduler_config:
            self._setup_scheduler(self.critic_optimizer, critic_scheduler_config, 'critic')

        # ä½¿ç”¨ä¼˜åŒ–çš„å†…å­˜
        self.memory = FastPPOMemory(capacity=memory_capacity, device=self.device)

        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'actor_loss': deque(maxlen=100),
            'critic_loss': deque(maxlen=100),
            'entropy': deque(maxlen=100)
        }

        # è°ƒè¯•æ ‡å¿—
        self._debug_grad_norm = False

    def _setup_scheduler(self, optimizer, config, name):
        """è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        scheduler_type = config.get('type', 'StepLR')
        if scheduler_type == 'StepLR':
            scheduler = torch.optim.lr_scheduler.StepLR(
                optimizer,
                step_size=config.get('step_size', 1000),
                gamma=config.get('gamma', 0.95)
            )
        elif scheduler_type == 'ExponentialLR':
            scheduler = torch.optim.lr_scheduler.ExponentialLR(
                optimizer,
                gamma=config.get('gamma', 0.99)
            )
        else:
            scheduler = None

        if name == 'actor':
            self.actor_scheduler = scheduler
        else:
            self.critic_scheduler = scheduler

    def update_learning_rate(self, factor: float):
        """åŠ¨æ€æ›´æ–°å­¦ä¹ ç‡ï¼ˆç”¨äºæ™ºèƒ½è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ ï¼‰"""
        try:
            # æ›´æ–°actorå­¦ä¹ ç‡
            for param_group in self.actor_optimizer.param_groups:
                param_group['lr'] *= factor

            # æ›´æ–°criticå­¦ä¹ ç‡
            for param_group in self.critic_optimizer.param_groups:
                param_group['lr'] *= factor

            print(f"ğŸ“ˆ å­¦ä¹ ç‡å·²æ›´æ–°ï¼Œç¼©æ”¾å› å­: {factor:.3f}")

        except Exception as e:
            print(f"âš ï¸ æ›´æ–°å­¦ä¹ ç‡å¤±è´¥: {e}")

    def get_current_learning_rates(self) -> Dict[str, float]:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        try:
            actor_lr = self.actor_optimizer.param_groups[0]['lr']
            critic_lr = self.critic_optimizer.param_groups[0]['lr']
            return {'actor_lr': actor_lr, 'critic_lr': critic_lr}
        except:
            return {'actor_lr': 0.0, 'critic_lr': 0.0}

    def select_action(self, state: Dict[str, torch.Tensor], training: bool = True) -> Tuple[Tuple[int, int], float, float]:
        """
        ä½¿ç”¨å½“å‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: çŠ¶æ€è§‚å¯Ÿ
            training: æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼
            
        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ (node_idx, partition_idx)
            log_prob: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            value: çŠ¶æ€ä»·å€¼ä¼°è®¡
        """
        node_embeddings = state['node_embeddings']
        region_embeddings = state['region_embeddings']
        boundary_nodes = state['boundary_nodes']
        
        # è·å–åŠ¨ä½œæ©ç  - ä½¿ç”¨ç¯å¢ƒæä¾›çš„æ­£ç¡®æ©ç 
        if 'action_mask' in state:
            action_mask = state['action_mask']
        else:
            # å›é€€åˆ°ç®€åŒ–ç‰ˆæœ¬ï¼ˆä¸æ¨èï¼‰
            action_mask = torch.zeros(
                node_embeddings.shape[0], self.num_partitions,
                dtype=torch.bool, device=self.device
            )
            current_partition = state['current_partition']
            for node_idx in boundary_nodes:
                current_node_partition = current_partition[node_idx].item()
                # å…è®¸ç§»åŠ¨åˆ°æ‰€æœ‰å…¶ä»–åˆ†åŒºï¼ˆç®€åŒ–ç‰ˆï¼‰
                for p in range(self.num_partitions):
                    if p + 1 != current_node_partition:  # +1ç”¨äºåŸºäº1çš„åˆ†åŒº
                        action_mask[node_idx, p] = True
        
        with torch.no_grad():
            # -------- 1) ä»…è®¡ç®—èŠ‚ç‚¹ logits --------
            # è‹¥æ— è¾¹ç•ŒèŠ‚ç‚¹ç›´æ¥è¿”å›
            if len(boundary_nodes) == 0:
                # æ„é€ å•æ ·æœ¬ BatchedState ä»¥å¤ç”¨æ‰¹å¤„ç†é€»è¾‘
                batched_state_single = self._prepare_batched_state([state])
                value = self.critic(batched_state_single)
                return None, 0.0, value.item()

            # è®¡ç®—å…¨å±€ä¸Šä¸‹æ–‡ï¼ˆä¸ ActorNetwork å†…éƒ¨ä¿æŒä¸€è‡´ï¼‰
            global_context = self.actor.global_encoder(
                region_embeddings.mean(dim=0, keepdim=True)
            )  # [1, region_dim]

            # è¾¹ç•ŒèŠ‚ç‚¹åµŒå…¥åŠç»„åˆç‰¹å¾
            boundary_embeddings = node_embeddings[boundary_nodes]  # [B, node_dim]
            combined_features = torch.cat(
                [boundary_embeddings, global_context.expand(len(boundary_nodes), -1)],
                dim=1
            )  # [B, node_dim + region_dim]

            # èŠ‚ç‚¹ logits
            node_logits = self.actor.node_selector(combined_features).squeeze(-1)

            # è‹¥æŸèŠ‚ç‚¹åœ¨ action_mask ä¸­æ— ä»»ä½•å¯é€‰åˆ†åŒºï¼Œåˆ™ç¦æ­¢é‡‡æ ·è¯¥èŠ‚ç‚¹
            valid_partition_per_node = action_mask[boundary_nodes].any(dim=1)  # [B]
            node_logits = node_logits.masked_fill(~valid_partition_per_node, -1e9)

            # Critic ä¼°å€¼
            # æ„é€ å•æ ·æœ¬ BatchedState ä»¥å¤ç”¨æ‰¹å¤„ç†é€»è¾‘
            batched_state_single = self._prepare_batched_state([state])
            value = self.critic(batched_state_single)
            
            # é‡‡æ ·åŠ¨ä½œï¼ˆä½¿ç”¨æ•°å€¼ç¨³å®šçš„æ–¹æ³•ï¼‰
            if training:
                # æ£€æŸ¥å¹¶å¤„ç†NaN/Infå€¼
                if torch.isnan(node_logits).any() or torch.isinf(node_logits).any():
                    print(f"âš ï¸ æ£€æµ‹åˆ°node_logitsä¸­çš„NaN/Infå€¼: {node_logits}")
                    # ä½¿ç”¨å‡åŒ€åˆ†å¸ƒä½œä¸ºå›é€€
                    node_probs = torch.ones_like(node_logits) / len(node_logits)
                else:
                    # ä½¿ç”¨æ•°å€¼ç¨³å®šçš„softmax
                    node_logits_clipped = torch.clamp(node_logits, min=-20, max=20)
                    node_probs = F.softmax(node_logits_clipped, dim=0)

                # ç¡®ä¿æ¦‚ç‡æœ‰æ•ˆ
                node_probs = node_probs.clamp(min=1e-12)
                node_probs = node_probs / node_probs.sum()

                node_dist = torch.distributions.Categorical(node_probs)
                node_action = node_dist.sample()

                # -------- 2) é’ˆå¯¹é€‰ä¸­èŠ‚ç‚¹è®¡ç®—åˆ†åŒº logits --------
                partition_logits_selected = self.actor.partition_selector(
                    combined_features[node_action]
                )
                # ä»…å¯¹è¯¥èŠ‚ç‚¹é€‚ç”¨æ©ç 
                partition_mask_selected = action_mask[boundary_nodes[node_action]]
                partition_logits_selected = partition_logits_selected.masked_fill(
                    ~partition_mask_selected, -1e9
                )
                
                if torch.isnan(partition_logits_selected).any() or torch.isinf(partition_logits_selected).any():
                    print(f"âš ï¸ æ£€æµ‹åˆ°partition_logitsä¸­çš„NaN/Infå€¼: {partition_logits_selected}")
                    partition_probs = torch.ones_like(partition_logits_selected) / len(partition_logits_selected)
                else:
                    # ä½¿ç”¨æ•°å€¼ç¨³å®šçš„softmax
                    partition_logits_clipped = torch.clamp(partition_logits_selected, min=-20, max=20)
                    partition_probs = F.softmax(partition_logits_clipped, dim=0)

                # ç¡®ä¿æ¦‚ç‡æœ‰æ•ˆ
                partition_probs = partition_probs.clamp(min=1e-12)
                partition_probs = partition_probs / partition_probs.sum()

                partition_dist = torch.distributions.Categorical(partition_probs)
                partition_action = partition_dist.sample()

                # è®¡ç®—å®‰å…¨çš„å¯¹æ•°æ¦‚ç‡
                node_log_prob = safe_log_prob(node_probs)[node_action]
                partition_log_prob = safe_log_prob(partition_probs)[partition_action]
                log_prob = node_log_prob + partition_log_prob
            else:
                # -------- è´ªå¿ƒæ¨¡å¼ä¸‹ä¹Ÿä»…è®¡ç®—ä¸€æ¬¡åˆ†åŒº logits --------
                node_action = torch.argmax(node_logits)
                partition_logits_greedy = self.actor.partition_selector(
                    combined_features[node_action]
                )
                partition_mask_greedy = action_mask[boundary_nodes[node_action]]
                partition_logits_greedy = partition_logits_greedy.masked_fill(
                    ~partition_mask_greedy, -1e9
                )
                partition_action = torch.argmax(partition_logits_greedy)
                log_prob = 0.0
            
            # è½¬æ¢ä¸ºå®é™…ç´¢å¼•
            selected_node = boundary_nodes[node_action].item()
            selected_partition = partition_action.item() + 1  # è½¬æ¢ä¸ºåŸºäº1çš„ç´¢å¼•
            
            action = (selected_node, selected_partition)
            
        return action, log_prob.item() if hasattr(log_prob, 'item') else log_prob, value.item()
        
    def store_experience(self, state, action, reward, log_prob, value, done):
        """åœ¨å†…å­˜ä¸­å­˜å‚¨ç»éªŒ"""
        self.memory.store(state, action, reward, log_prob, value, done)
        
    def update(self) -> Dict[str, float]:
        """
        ä½¿ç”¨PPOæ›´æ–°ç½‘ç»œï¼ˆä¼˜åŒ–ç‰ˆæœ¬ - é¿å…CPU-GPUä¼ è¾“ï¼‰

        Returns:
            è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        if len(self.memory) == 0:
            return {}

        # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šç›´æ¥è·å–å¼ é‡ï¼Œé¿å…CPU-GPUä¼ è¾“
        states, actions, rewards_tensor, old_log_probs_tensor, old_values_tensor, dones_tensor = self.memory.get_batch_tensors()
        
        # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
        advantages, returns = self._compute_advantages(rewards_tensor, old_values_tensor, dones_tensor)

        # PPOæ›´æ–°
        stats = {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0}

        for _ in range(self.k_epochs):
            epoch_stats = self._ppo_epoch(states, actions, old_log_probs_tensor, advantages, returns)
            for key in stats:
                stats[key] += epoch_stats[key]

        # å¹³å‡åŒ–ç»Ÿè®¡
        for key in stats:
            stats[key] /= self.k_epochs

        # æ›´æ–°è®­ç»ƒç»Ÿè®¡
        self.training_stats['actor_loss'].append(stats['actor_loss'])
        self.training_stats['critic_loss'].append(stats['critic_loss'])
        self.training_stats['entropy'].append(stats['entropy'])

        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.actor_scheduler:
            self.actor_scheduler.step()
        if self.critic_scheduler:
            self.critic_scheduler.step()

        # æ¸…ç©ºå†…å­˜
        self.memory.clear()

        return stats
        
    def _compute_advantages(self, rewards, values, dones):
        """ä½¿ç”¨GAEè®¡ç®—ä¼˜åŠ¿ï¼ˆä¸åŸå§‹ç‰ˆæœ¬ç›¸åŒï¼‰"""
        advantages = torch.zeros_like(rewards)
        returns = torch.zeros_like(rewards)

        gae = 0
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]

            delta = rewards[t] + self.gamma * next_value * (~dones[t]).float() - values[t]
            gae = delta + self.gamma * 0.95 * (~dones[t]).float() * gae
            advantages[t] = gae
            returns[t] = advantages[t] + values[t]

        # å®‰å…¨çš„å›æŠ¥å¥åº·æ£€æŸ¥
        returns = torch.nan_to_num(returns, nan=0.0, posinf=0.0, neginf=0.0)

        # å®‰å…¨æ ‡å‡†åŒ–ä¼˜åŠ¿
        def safe_standardize(t, eps=1e-6):
            mean = t.mean()
            std = t.std()
            if torch.isnan(std) or std < eps:
                return t - mean
            return (t - mean) / (std + eps)

        advantages = safe_standardize(advantages)
        advantages = torch.nan_to_num(advantages, nan=0.0, posinf=0.0, neginf=0.0)

        return advantages, returns
        
    def _ppo_epoch(self, states, actions, old_log_probs, advantages, returns):
        """å•ä¸ªPPOè®­ç»ƒè½®ï¼ˆæ‰¹å¤„ç†ç‰ˆæœ¬ï¼Œæ— æ˜¾å¼Pythonå¾ªç¯ï¼‰"""
        batch_size = len(states)
        if batch_size == 0:
            return {'actor_loss': 0.0, 'critic_loss': 0.0, 'entropy': 0.0}

        # ---- 1. æ•°æ®æ‰¹é‡åŒ– ----
        batched_state = self._prepare_batched_state(states)

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬åœ¨ node_embeddings ä¸­çš„èµ·å§‹åç§»é‡ï¼Œç”¨äºå°†å±€éƒ¨èŠ‚ç‚¹ç´¢å¼•è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•
        node_offsets = torch.zeros(batch_size, dtype=torch.long, device=self.device)
        offset = 0
        for i, s in enumerate(states):
            node_offsets[i] = offset
            offset += s['node_embeddings'].size(0)

        # å°†åŠ¨ä½œæ‹†åˆ†ä¸ºèŠ‚ç‚¹ç´¢å¼•å’Œåˆ†åŒºç´¢å¼•
        node_indices_local = torch.tensor([a[0] for a in actions], dtype=torch.long, device=self.device)
        partition_indices = torch.tensor([a[1] - 1 for a in actions], dtype=torch.long, device=self.device)  # 0-based
        node_indices_global = node_indices_local + node_offsets  # [batch_size]

        # ---- 2. å‰å‘ä¼ æ’­ ----
        node_logits, partition_logits = self.actor(batched_state)  # logits æŒ‰ boundary_nodes é¡ºåº
        values = self.critic(batched_state)  # [batch_size]

        # ---- 3. é€‰å–æ‰€æ‰§è¡ŒåŠ¨ä½œå¯¹åº”çš„ logits/probs ----
        # æ„é€  (batch_size, num_boundary) å¸ƒå°”çŸ©é˜µï¼Œæ‰¾åˆ°æ¯ä¸ªæ ·æœ¬å¯¹åº” boundary ä½ç½®
        boundary_nodes = batched_state.boundary_nodes  # [num_boundary]
        boundary_batch_idx = batched_state.node_batch_idx[boundary_nodes]  # [num_boundary]

        # ä½¿ç”¨å¼ é‡æ¯”è¾ƒè®¡ç®—ä½ç½®ç´¢å¼•ï¼Œé¿å…Pythonå¾ªç¯
        equal_matrix = (boundary_nodes.unsqueeze(0) == node_indices_global.unsqueeze(1))  # [batch_size, num_boundary]
        selected_positions = equal_matrix.float().argmax(dim=1)  # [batch_size] æ¯è¡Œä¿è¯å”¯ä¸€ True

        # å–å‡ºå¯¹åº”èŠ‚ç‚¹ logits è¡Œ
        selected_node_logits = node_logits[selected_positions]  # [batch_size]
        selected_partition_logits = partition_logits[selected_positions]  # [batch_size, num_partitions]

        # ---- 4. è®¡ç®—æ¦‚ç‡ & å¯¹æ•°æ¦‚ç‡ ----
        # èŠ‚ç‚¹ softmax éœ€è¦åœ¨å„è‡ªæ ·æœ¬èŒƒå›´å†…æ‰§è¡Œ
        node_logits_clipped = torch.clamp(node_logits, min=-20, max=20)
        exp_node = torch.exp(node_logits_clipped)
        sum_exp_node = scatter_sum(exp_node, boundary_batch_idx, dim=0)  # [batch_size]
        node_probs = exp_node / sum_exp_node[boundary_batch_idx]  # [num_boundary]
        selected_node_probs = node_probs[selected_positions]  # [batch_size]

        # åˆ†åŒºæ¦‚ç‡ï¼ˆä»…é€‰ä¸­èŠ‚ç‚¹ï¼‰
        partition_logits_clipped = torch.clamp(selected_partition_logits, min=-20, max=20)
        exp_part = torch.exp(partition_logits_clipped)
        partition_probs = exp_part / exp_part.sum(dim=1, keepdim=True)
        selected_partition_probs = partition_probs[torch.arange(batch_size, device=self.device), partition_indices]

        # å®‰å…¨å–å¯¹æ•°
        new_log_probs = safe_log_prob(selected_node_probs) + safe_log_prob(selected_partition_probs)

        # ---- 5. PPO æŸå¤± ----
        log_prob_diff = torch.clamp(new_log_probs - old_log_probs, min=-20, max=20)
        ratio = torch.exp(log_prob_diff)
        ratio = torch.nan_to_num(ratio, nan=1.0, posinf=1.0, neginf=0.0)

        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages
        actor_loss = -torch.min(surr1, surr2).mean()

        critic_loss = F.mse_loss(values.squeeze(), returns.squeeze())

        # ---- 6. ç†µ ----
        node_entropy_all = -(node_probs * safe_log_prob(node_probs))
        node_entropy_per_batch = scatter_sum(node_entropy_all, boundary_batch_idx, dim=0)
        partition_entropy = -(partition_probs * safe_log_prob(partition_probs)).sum(dim=1)  # [batch_size]
        entropy = (node_entropy_per_batch + partition_entropy).mean()

        # ---- 7. åå‘ä¼ æ’­ & æ›´æ–° ----
        total_loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy

        self.actor_optimizer.zero_grad()
        self.critic_optimizer.zero_grad()
        total_loss.backward()

        if self.max_grad_norm is not None:
            all_params = list(self.actor.parameters()) + list(self.critic.parameters())
            torch.nn.utils.clip_grad_norm_(all_params, self.max_grad_norm)

        self.actor_optimizer.step()
        self.critic_optimizer.step()

        return {
            'actor_loss': actor_loss.item(),
            'critic_loss': critic_loss.item(),
            'entropy': entropy.item()
        }

    def enable_gradient_norm_debug(self, enable: bool = True):
        """å¯ç”¨/ç¦ç”¨æ¢¯åº¦èŒƒæ•°è°ƒè¯•æ‰“å°"""
        self._debug_grad_norm = enable
        
    def save(self, filepath: str):
        """ä¿å­˜æ™ºèƒ½ä½“çŠ¶æ€"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
        }, filepath)
        
    def load(self, filepath: str):
        """åŠ è½½æ™ºèƒ½ä½“çŠ¶æ€"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])

    def store_experience(self, state, action, reward, log_prob, value, done):
        """åœ¨å†…å­˜ä¸­å­˜å‚¨ç»éªŒ"""
        self.memory.store(state, action, reward, log_prob, value, done)

    def update_learning_rate(self, factor: float):
        """åŠ¨æ€æ›´æ–°å­¦ä¹ ç‡"""
        for param_group in self.actor_optimizer.param_groups:
            param_group['lr'] *= factor
        for param_group in self.critic_optimizer.param_groups:
            param_group['lr'] *= factor

    def get_current_learning_rates(self) -> Dict[str, float]:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        try:
            actor_lr = self.actor_optimizer.param_groups[0]['lr']
            critic_lr = self.critic_optimizer.param_groups[0]['lr']
            return {'actor_lr': actor_lr, 'critic_lr': critic_lr}
        except:
            return {'actor_lr': 0.0, 'critic_lr': 0.0}

    def _prepare_batched_state(self, states_list: List[Dict[str, torch.Tensor]]) -> "BatchedState":
        """å°†è‹¥å¹²åŸå§‹stateåˆ—è¡¨æ‹¼æ¥ä¸ºæ‰¹å¤„ç†BatchedStateã€‚"""
        # ç´¯ç§¯åˆ—è¡¨
        node_emb_list: List[torch.Tensor] = []
        region_emb_list: List[torch.Tensor] = []
        action_mask_list: List[torch.Tensor] = []
        node_batch_idx_list: List[torch.Tensor] = []
        region_batch_idx_list: List[torch.Tensor] = []
        boundary_nodes_global: List[torch.Tensor] = []

        node_offset = 0
        region_offset = 0

        for batch_id, state in enumerate(states_list):
            node_embeddings = state['node_embeddings'].detach()
            region_embeddings = state['region_embeddings'].detach()
            boundary_nodes = state['boundary_nodes']
            current_partition = state['current_partition']

            num_nodes = node_embeddings.size(0)
            num_regions = region_embeddings.size(0)

            # ==== åŠ¨ä½œæ©ç æ„é€  ====
            if 'action_mask' in state:
                action_mask = state['action_mask']
            else:
                # å¤åˆ¶è‡ªæ—§å®ç°çš„ç®€åŒ–é€»è¾‘
                action_mask = torch.zeros(
                    num_nodes, self.num_partitions, dtype=torch.bool, device=self.device
                )
                for node_idx in boundary_nodes:
                    current_node_partition = current_partition[node_idx].item()
                    for p in range(self.num_partitions):
                        if p + 1 != current_node_partition:
                            action_mask[node_idx, p] = True

            # ==== ç´¯ç§¯å¼ é‡ ====
            node_emb_list.append(node_embeddings)
            region_emb_list.append(region_embeddings)
            action_mask_list.append(action_mask)

            # batch idx
            node_batch_idx_list.append(torch.full((num_nodes,), batch_id, dtype=torch.long, device=self.device))
            region_batch_idx_list.append(torch.full((num_regions,), batch_id, dtype=torch.long, device=self.device))

            # è¾¹ç•ŒèŠ‚ç‚¹å…¨å±€ç´¢å¼•
            if boundary_nodes.numel() > 0:
                boundary_nodes_global.append(boundary_nodes + node_offset)

            node_offset += num_nodes
            region_offset += num_regions

        # ===== æ‹¼æ¥ =====
        node_embeddings_cat = torch.cat(node_emb_list, dim=0)
        region_embeddings_cat = torch.cat(region_emb_list, dim=0)
        action_mask_cat = torch.cat(action_mask_list, dim=0)
        node_batch_idx = torch.cat(node_batch_idx_list, dim=0)
        region_batch_idx = torch.cat(region_batch_idx_list, dim=0)
        if boundary_nodes_global:
            boundary_nodes_cat = torch.cat(boundary_nodes_global, dim=0)
        else:
            boundary_nodes_cat = torch.zeros(0, dtype=torch.long, device=self.device)

        return BatchedState(
            node_embeddings=node_embeddings_cat,
            region_embeddings=region_embeddings_cat,
            boundary_nodes=boundary_nodes_cat,
            node_batch_idx=node_batch_idx,
            region_batch_idx=region_batch_idx,
            action_mask=action_mask_cat,
        )
