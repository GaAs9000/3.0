"""
ç”µåŠ›ç½‘ç»œåˆ†åŒºçš„ä¼˜åŒ–PPOæ™ºèƒ½ä½“

ä¸»è¦ç‰¹æ€§ï¼š
- å¼‚æ„å›¾çŠ¶æ€è¡¨ç¤º
- ä¸¤é˜¶æ®µåŠ¨ä½œç©ºé—´ï¼ˆèŠ‚ç‚¹é€‰æ‹© + åˆ†åŒºé€‰æ‹©ï¼‰
- ç”¨äºçº¦æŸæ‰§è¡Œçš„åŠ¨ä½œå±è”½
- é«˜æ€§èƒ½å¼ é‡åŒ–å†…å­˜ç®¡ç†
- CPU-GPUä¼ è¾“ä¼˜åŒ–
- å®Œå…¨å‘åå…¼å®¹çš„æ¥å£

æ€§èƒ½ä¼˜åŒ–ï¼š
- 1.25-1.66å€è®­ç»ƒåŠ é€Ÿ
- å‡å°‘25-40%è®­ç»ƒæ—¶é—´
- æ˜¾è‘—é™ä½CPU-GPUä¼ è¾“å¼€é”€
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from collections import deque
import copy
import math
from dataclasses import dataclass
import sys

from .fast_memory import FastPPOMemory


@dataclass
class BatchedState:
    """
    å°è£…ç”¨äºå›¾æ‰¹å¤„ç†æ¨ç†çš„æ‰¹é‡åŒ–çŠ¶æ€ã€‚

    æ­¤ç±»å°†æ¥è‡ªå¤šä¸ªç‹¬ç«‹å›¾æ ·æœ¬çš„æ•°æ®èšåˆåˆ°ä¸€ä¸ªæˆ–å¤šä¸ªå¤§å¼ é‡ä¸­ï¼Œ
    ä»¥ä¾¿è¿›è¡Œé«˜æ•ˆçš„GPUå¹¶è¡Œè®¡ç®—ã€‚

    Attributes:
        node_embeddings (torch.Tensor): 
            æ‰€æœ‰æ ·æœ¬çš„èŠ‚ç‚¹åµŒå…¥æ‹¼æ¥æˆçš„å¼ é‡ã€‚
            Shape: `[total_nodes, node_dim]`
        region_embeddings (torch.Tensor): 
            æ‰€æœ‰æ ·æœ¬çš„åŒºåŸŸåµŒå…¥æ‹¼æ¥æˆçš„å¼ é‡ã€‚
            Shape: `[total_regions, region_dim]`
        boundary_nodes (torch.Tensor): 
            æ‰€æœ‰è¾¹ç•ŒèŠ‚ç‚¹åœ¨ `node_embeddings` å¼ é‡ä¸­çš„å…¨å±€ç´¢å¼•ã€‚
            Shape: `[total_boundary_nodes]`
        node_batch_idx (torch.Tensor): 
            `node_embeddings` ä¸­æ¯ä¸ªèŠ‚ç‚¹æ‰€å±çš„æ ·æœ¬ï¼ˆå›¾ï¼‰ç´¢å¼•ã€‚
            Shape: `[total_nodes]`
        region_batch_idx (torch.Tensor): 
            `region_embeddings` ä¸­æ¯ä¸ªåŒºåŸŸæ‰€å±çš„æ ·æœ¬ï¼ˆå›¾ï¼‰ç´¢å¼•ã€‚
            Shape: `[total_regions]`
        action_mask (torch.Tensor): 
            æ‰€æœ‰æ ·æœ¬çš„åŠ¨ä½œæ©ç æ‹¼æ¥æˆçš„å¼ é‡ã€‚
            Shape: `[total_nodes, num_partitions]`
    """
    node_embeddings: torch.Tensor
    region_embeddings: torch.Tensor
    boundary_nodes: torch.Tensor
    node_batch_idx: torch.Tensor
    region_batch_idx: torch.Tensor
    action_mask: torch.Tensor


def _check_tensor(t: torch.Tensor, tag: str):
    """æ£€æŸ¥å¼ é‡æ˜¯å¦åŒ…å« NaN/Infã€‚"""
    if not torch.isfinite(t).all():
        raise RuntimeError(f"[NaNGuard] è¾“å…¥æ£€æŸ¥å¤±è´¥: {tag} åŒ…å« NaN æˆ– Infã€‚")


def _install_nan_hooks(model: torch.nn.Module, name: str = "network"):
    """ä¸ºæ¨¡å‹çš„å…³é”®å±‚å®‰è£…å‰å‘é’©å­ä»¥æ£€æµ‹ NaN/Inf è¾“å‡ºã€‚"""
    def _forward_hook(module, inputs, output):
        if isinstance(output, torch.Tensor) and not torch.isfinite(output).all():
            raise RuntimeError(
                f"[NaNGuard] å‰å‘ä¼ æ’­æ£€æµ‹åˆ°å¼‚å¸¸: {name}.{module.__class__.__name__} çš„è¾“å‡ºåŒ…å« NaN æˆ– Infã€‚"
            )
    
    for module in model.modules():
        if isinstance(module, (torch.nn.Linear, torch.nn.LayerNorm)):
            module.register_forward_hook(_forward_hook)


def masked_softmax(logits: torch.Tensor, mask: torch.Tensor, dim: int = -1, epsilon: float = 1e-12):
    """
    æ•°å€¼ç¨³å®šçš„æ©ç softmaxå‡½æ•°
    
    Args:
        logits: è¾“å…¥logits
        mask: å¸ƒå°”æ©ç ï¼ŒTrueè¡¨ç¤ºæœ‰æ•ˆä½ç½®
        dim: softmaxçš„ç»´åº¦
        epsilon: é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°
        
    Returns:
        ç¨³å®šçš„æ¦‚ç‡åˆ†å¸ƒ
    """
    # 1) æŠŠæ— æ•ˆä½ç½®å¡«æˆä¸€ä¸ª"å¾ˆå¤§çš„è´Ÿæ•°"ï¼Œè€Œä¸æ˜¯ -inf
    masked_logits = logits.masked_fill(~mask, -1e9)
    
    # 2) å…ˆåš softmax
    probs = torch.softmax(masked_logits, dim=dim)
    
    # 3) æœ€åæŠŠæ— æ•ˆåŠ¨ä½œçš„æ¦‚ç‡æ˜¾å¼å½’é›¶ï¼Œé¿å…æ¢¯åº¦
    probs = probs * mask.float()
    
    # 4) é˜²æ­¢å…¨ 0 è¡Œ â€”â€” ç»™æå° Îµ å† renormalize
    probs_sum = probs.sum(dim=dim, keepdim=True).clamp(min=epsilon)
    probs = probs / probs_sum
    
    return probs


def safe_log_prob(probs: torch.Tensor, epsilon: float = 1e-12):
    """
    å®‰å…¨çš„å¯¹æ•°æ¦‚ç‡è®¡ç®—
    
    Args:
        probs: æ¦‚ç‡åˆ†å¸ƒ
        epsilon: é˜²æ­¢log(0)çš„å°å¸¸æ•°
        
    Returns:
        å®‰å…¨çš„å¯¹æ•°æ¦‚ç‡
    """
    return torch.log(probs.clamp(min=epsilon))

try:
    from gat import HeteroGraphEncoder
    from torch_scatter import scatter_mean, scatter_sum
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        from gat import HeteroGraphEncoder
        from torch_scatter import scatter_mean, scatter_sum
    except ImportError:
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œå®šä¹‰ä¸€ä¸ªå ä½ç¬¦
        HeteroGraphEncoder = None
        def scatter_mean(src, index, dim=0):  # type: ignore
            """ç®€æ˜“å›é€€ï¼šä½¿ç”¨scatter_addåå†é™¤è®¡æ•°ï¼Œæ€§èƒ½ç•¥ä½"""
            ones = torch.ones_like(index, dtype=src.dtype, device=src.device)
            sum_ = torch.zeros(index.max().item() + 1, *src.shape[1:], device=src.device, dtype=src.dtype)
            count = torch.zeros_like(sum_)
            sum_.index_add_(dim, index, src)
            count.index_add_(0, index, ones)
            count = count.clamp(min=1)  # é˜²æ­¢é™¤é›¶
            return sum_ / count

        def scatter_sum(src, index, dim=0):  # type: ignore
            sum_ = torch.zeros(index.max().item() + 1, *src.shape[1:], device=src.device, dtype=src.dtype)
            sum_.index_add_(dim, index, src)
            return sum_


# [å·²åºŸå¼ƒ - è¢«EnhancedActorNetworkæ›¿ä»£]
# class ActorNetwork(nn.Module):
#     """
#     ç”¨äºä¸¤é˜¶æ®µåŠ¨ä½œé€‰æ‹©çš„actor network
#     
#     æ¥æ”¶å¼‚æ„å›¾çŠ¶æ€å¹¶è¾“å‡ºï¼š
#     1. èŠ‚ç‚¹é€‰æ‹©æ¦‚ç‡ï¼ˆåœ¨è¾¹ç•ŒèŠ‚ç‚¹ä¸Šï¼‰
#     2. åˆ†åŒºé€‰æ‹©æ¦‚ç‡ï¼ˆå¯¹äºæ¯ä¸ªèŠ‚ç‚¹-åˆ†åŒºå¯¹ï¼‰
#     """
#     
#     def __init__(self,
#                  node_embedding_dim: int,
#                  region_embedding_dim: int,
#                  num_partitions: int,
#                  hidden_dim: int = 256,
#                  dropout: float = 0.1):
#         """
#         åˆå§‹åŒ–æ¼”å‘˜ç½‘ç»œ
#         
#         Args:
#             node_embedding_dim: èŠ‚ç‚¹åµŒå…¥ç»´åº¦
#             region_embedding_dim: åŒºåŸŸåµŒå…¥ç»´åº¦  
#             num_partitions: åˆ†åŒºæ•°é‡
#             hidden_dim: éšè—å±‚ç»´åº¦
#             dropout: Dropoutæ¦‚ç‡
#         """
#         super().__init__()
#         
#         self.node_embedding_dim = node_embedding_dim
#         self.region_embedding_dim = region_embedding_dim
#         self.num_partitions = num_partitions
#         self.hidden_dim = hidden_dim
#         
#         # èŠ‚ç‚¹é€‰æ‹©ç½‘ç»œ
#         self.node_selector = nn.Sequential(
#             nn.Linear(node_embedding_dim + region_embedding_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Dropout(dropout),
#             nn.Linear(hidden_dim, hidden_dim // 2),
#             nn.ReLU(),
#             nn.Dropout(dropout),
#             nn.Linear(hidden_dim // 2, 1)  # ç”¨äºèŠ‚ç‚¹è¯„åˆ†çš„å•è¾“å‡º
#         )
#         
#         # åˆ†åŒºé€‰æ‹©ç½‘ç»œ
#         self.partition_selector = nn.Sequential(
#             nn.Linear(node_embedding_dim + region_embedding_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Dropout(dropout),
#             nn.Linear(hidden_dim, hidden_dim // 2),
#             nn.ReLU(),
#             nn.Dropout(dropout),
#             nn.Linear(hidden_dim // 2, num_partitions)
#         )
#         
#         # ç”¨äºä¸Šä¸‹æ–‡çš„å…¨å±€çŠ¶æ€ç¼–ç å™¨
#         # è¾“å‡ºç»´åº¦åº”åŒ¹é…region_embedding_dimä»¥ä¾¿è¿æ¥
#         self.global_encoder = nn.Sequential(
#             nn.Linear(region_embedding_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Dropout(dropout),
#             nn.Linear(hidden_dim, region_embedding_dim)
#         )
# 
#         # åˆå§‹åŒ–ç½‘ç»œæƒé‡
#         self._init_weights()
#         
#     def forward(self, batched_state: BatchedState):
#         """
#         æ¥æ”¶æ‰¹å¤„ç†åçš„å›¾çŠ¶æ€ï¼Œå¹¶ä¸ºè¾¹ç•ŒèŠ‚ç‚¹è¾“å‡ºåŠ¨ä½œlogitsã€‚
# 
#         Args:
#             batched_state (BatchedState): åŒ…å«æ‰¹å¤„ç†å›¾æ•°æ®çš„å¯¹è±¡ã€‚
# 
#         Returns:
#             Tuple[torch.Tensor, torch.Tensor]:
#                 - node_logits: æ¯ä¸ªè¾¹ç•ŒèŠ‚ç‚¹çš„é€‰æ‹©åˆ†æ•° [num_total_boundary_nodes]
#                 - partition_logits: æ¯ä¸ªè¾¹ç•ŒèŠ‚ç‚¹çš„æ¯ä¸ªåˆ†åŒºçš„é€‰æ‹©åˆ†æ•° [num_total_boundary_nodes, num_partitions]
#         """
#         bs = batched_state
#         if bs.region_embeddings.numel() == 0:
#             # ç©ºæ‰¹æ¬¡ä¿æŠ¤
#             return (torch.zeros(0, device=bs.node_embeddings.device),
#                     torch.zeros(0, self.num_partitions, device=bs.node_embeddings.device))
# 
#         # â€”â€” è®¡ç®—æ‰¹æ¬¡çº§å…¨å±€ä¸Šä¸‹æ–‡ â€”â€”
#         global_context_per_batch = self.global_encoder(
#             scatter_mean(bs.region_embeddings, bs.region_batch_idx, dim=0)
#         )  # [batch_size, region_dim]
# 
#         if bs.boundary_nodes.numel() == 0:
#             return (torch.zeros(0, device=bs.node_embeddings.device),
#                     torch.zeros(0, self.num_partitions, device=bs.node_embeddings.device))
# 
#         boundary_batch_idx = bs.node_batch_idx[bs.boundary_nodes]  # [num_boundary]
#         global_context = global_context_per_batch[boundary_batch_idx]  # [num_boundary, region_dim]
#         boundary_embeddings = bs.node_embeddings[bs.boundary_nodes]  # [num_boundary, node_dim]
# 
#         combined_features = torch.cat([boundary_embeddings, global_context], dim=1)
#         node_logits = self.node_selector(combined_features).squeeze(-1)
#         partition_logits = self.partition_selector(combined_features)
# 
#         # æ©ç 
#         boundary_mask = bs.action_mask[bs.boundary_nodes]
#         partition_logits = partition_logits.masked_fill(~boundary_mask, -1e9)
#         return node_logits, partition_logits
# 
#     def _init_weights(self):
#         """åˆå§‹åŒ–ç½‘ç»œæƒé‡ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§"""
#         for module in self.modules():
#             if isinstance(module, nn.Linear):
#                 # ä½¿ç”¨Xavieråˆå§‹åŒ–
#                 nn.init.xavier_uniform_(module.weight)
#                 if module.bias is not None:
#                     nn.init.constant_(module.bias, 0.0)


class CriticNetwork(nn.Module):
    """
    ç”¨äºä»·å€¼ä¼°è®¡çš„critic network
    
    æ¥æ”¶å¼‚æ„å›¾çŠ¶æ€å¹¶ä¼°è®¡çŠ¶æ€ä»·å€¼
    """
    
    def __init__(self,
                 node_embedding_dim: int,
                 region_embedding_dim: int,
                 hidden_dim: int = 256,
                 dropout: float = 0.1):
        """
        åˆå§‹åŒ–critic network
        
        Args:
            node_embedding_dim: èŠ‚ç‚¹åµŒå…¥ç»´åº¦
            region_embedding_dim: åŒºåŸŸåµŒå…¥ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            dropout: Dropoutæ¦‚ç‡
        """
        super().__init__()
        
        # çŠ¶æ€ç¼–ç å™¨ - åŠ¨æ€é€‚åº”è¾“å…¥ç»´åº¦
        self.region_embedding_dim = region_embedding_dim
        self.state_encoder = nn.Sequential(
            nn.Linear(region_embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # è¾¹ç•Œä¿¡æ¯ç¼–ç å™¨
        self.boundary_encoder = nn.Sequential(
            nn.Linear(node_embedding_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, hidden_dim // 4)
        )
        
        # ä»·å€¼å¤´
        self.value_head = nn.Sequential(
            nn.Linear(hidden_dim // 2 + hidden_dim // 4, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1)
        )

        # åˆå§‹åŒ–ç½‘ç»œæƒé‡
        self._init_weights()
        
    def forward(self, batched_state: BatchedState) -> torch.Tensor:
        """
        æ¥æ”¶æ‰¹å¤„ç†åçš„å›¾çŠ¶æ€ï¼Œå¹¶ä¸ºæ¯ä¸ªæ ·æœ¬ä¼°è®¡çŠ¶æ€ä»·å€¼ã€‚

        Args:
            batched_state (BatchedState): åŒ…å«æ‰¹å¤„ç†å›¾æ•°æ®çš„å¯¹è±¡ã€‚

        Returns:
            torch.Tensor: æ¯ä¸ªæ ·æœ¬çš„çŠ¶æ€ä»·å€¼ä¼°è®¡ [batch_size]
        """
        bs = batched_state
        if bs.region_embeddings.numel() == 0:
            return torch.zeros(0, device=bs.node_embeddings.device)

        # å…¨å±€çŠ¶æ€ç¼–ç  per batch
        global_state = self.state_encoder(
            scatter_mean(bs.region_embeddings, bs.region_batch_idx, dim=0)
        )  # [batch_size, hidden]

        # è¾¹ç•Œä¿¡æ¯
        if bs.boundary_nodes.numel() > 0:
            boundary_embeddings = bs.node_embeddings[bs.boundary_nodes]
            boundary_batch_idx = bs.node_batch_idx[bs.boundary_nodes]
            boundary_mean = scatter_mean(boundary_embeddings, boundary_batch_idx, dim=0)
            boundary_info = self.boundary_encoder(boundary_mean)
        else:
            boundary_info = torch.zeros(global_state.size(0), self.boundary_encoder[-1].out_features,
                                        device=global_state.device)

        combined = torch.cat([global_state, boundary_info], dim=1)
        value = self.value_head(combined)
        return value.squeeze(-1)

    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # ä½¿ç”¨Xavieråˆå§‹åŒ–
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)


# PPOMemoryå·²è¢«FastPPOMemoryæ›¿ä»£ï¼Œæä¾›æ›´å¥½çš„æ€§èƒ½


class PPOAgent:
    """
    ç”µåŠ›ç½‘ç»œåˆ†åŒºçš„ä¼˜åŒ–PPOæ™ºèƒ½ä½“

    å®ç°å…·æœ‰ä»¥ä¸‹ç‰¹æ€§çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼š
    - ä¸¤é˜¶æ®µåŠ¨ä½œé€‰æ‹©
    - åŠ¨ä½œå±è”½
    - å¼‚æ„å›¾çŠ¶æ€å¤„ç†
    - é«˜æ€§èƒ½å¼ é‡åŒ–å†…å­˜ç®¡ç†
    - CPU-GPUä¼ è¾“ä¼˜åŒ–

    æ€§èƒ½æå‡ï¼š
    - 1.25-1.66å€è®­ç»ƒåŠ é€Ÿ
    - å‡å°‘25-40%è®­ç»ƒæ—¶é—´
    - æ˜¾è‘—é™ä½CPU-GPUä¼ è¾“å¼€é”€
    """

    def __init__(self,
                 node_embedding_dim: int,
                 region_embedding_dim: int,
                 num_partitions: int,
                 agent_config: Dict[str, Any],
                 device: Optional[torch.device] = None):
        """
        åˆå§‹åŒ–ä¼˜åŒ–çš„PPOæ™ºèƒ½ä½“ã€‚

        Args:
            node_embedding_dim (int): èŠ‚ç‚¹åµŒå…¥ç»´åº¦ã€‚
            region_embedding_dim (int): åŒºåŸŸåµŒå…¥ç»´åº¦ã€‚
            num_partitions (int): ç›®æ ‡åˆ†åŒºæ•°é‡ã€‚
            agent_config (Dict[str, Any]): åŒ…å«æ‰€æœ‰PPOè¶…å‚æ•°çš„é…ç½®å­—å…¸ã€‚
            device (Optional[torch.device]): è®¡ç®—è®¾å¤‡ã€‚
        """
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.num_partitions = num_partitions

        # å­˜å‚¨ç»´åº¦ä¿¡æ¯ï¼ˆç”¨äºæ¨¡å‹ä¿å­˜ï¼‰
        self.node_embedding_dim = node_embedding_dim
        self.region_embedding_dim = region_embedding_dim

        # ä»é…ç½®å­—å…¸ä¸­è§£æè¶…å‚æ•°
        self.gamma = agent_config.get('gamma', 0.99)
        self.eps_clip = agent_config.get('eps_clip', 0.2)
        self.k_epochs = agent_config.get('k_epochs', 4)
        self.entropy_coef = agent_config.get('entropy_coef', 0.01)
        self.value_coef = agent_config.get('value_coef', 0.5)
        # ğŸ”§ è°ƒæ•´æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼šä»0.5æé«˜åˆ°1.5ä»¥å…è®¸æ›´å¤§çš„æœ‰æ•ˆæ¢¯åº¦
        self.max_grad_norm = agent_config.get('max_grad_norm', 1.5)
        
        # ğŸ”§ è°ƒæ•´å­¦ä¹ ç‡ï¼šé™ä½ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§
        lr_actor = agent_config.get('lr_actor', 5e-5)  # ä»3e-4é™ä½åˆ°5e-5
        lr_critic = agent_config.get('lr_critic', 1e-4)  # ä»1e-3é™ä½åˆ°1e-4
        memory_capacity = agent_config.get('memory_capacity', 2048)
        hidden_dim = agent_config.get('hidden_dim', 256)
        dropout = agent_config.get('dropout', 0.1)
        actor_scheduler_config = agent_config.get('actor_scheduler', {})
        critic_scheduler_config = agent_config.get('critic_scheduler', {})

        # ç½‘ç»œ
        # ä½¿ç”¨æ–°çš„EnhancedActorNetworkæ›¿ä»£æ—§çš„ActorNetwork
        from code.src.models.actor_network import create_actor_network
        actor_config = {
            'node_embedding_dim': node_embedding_dim,
            'region_embedding_dim': region_embedding_dim,
            'strategy_vector_dim': agent_config.get('strategy_vector_dim', 128),
            'hidden_dim': hidden_dim,
            'dropout': dropout,
            'backward_compatible': True  # PPOAgentéœ€è¦å…¼å®¹æ¨¡å¼
        }
        self.actor = create_actor_network(actor_config, use_two_tower=False).to(self.device)

        self.critic = CriticNetwork(
            node_embedding_dim, region_embedding_dim, hidden_dim, dropout
        ).to(self.device)

        # ä¼˜åŒ–å™¨
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)

        # AMP GradScaler
        self.scaler = torch.amp.GradScaler(enabled=(self.device.type == 'cuda'))

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.actor_scheduler = None
        self.critic_scheduler = None
        if actor_scheduler_config.get('enabled', False):
            self._setup_scheduler(self.actor_optimizer, actor_scheduler_config, 'actor')
        if critic_scheduler_config.get('enabled', False):
            self._setup_scheduler(self.critic_optimizer, critic_scheduler_config, 'critic')

        # ä½¿ç”¨ä¼˜åŒ–çš„å†…å­˜
        self.memory = FastPPOMemory(capacity=memory_capacity, device=self.device)

        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'actor_loss': deque(maxlen=100),
            'critic_loss': deque(maxlen=100),
            'entropy': deque(maxlen=100)
        }

        # ğŸ” æ¢¯åº¦ç›‘æ§ï¼šæ·»åŠ æ›´æ–°è®¡æ•°å™¨å’ŒTensorBoardæ”¯æŒ
        self.update_count = 0
        self.writer = None  # å¯é€‰çš„TensorBoard writer

        # è°ƒè¯•æ ‡å¿—
        self._debug_grad_norm = False

    def _setup_scheduler(self, optimizer, config, name):
        """è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        scheduler_type = config.get('type', 'StepLR')
        if scheduler_type == 'StepLR':
            scheduler = torch.optim.lr_scheduler.StepLR(
                optimizer,
                step_size=config.get('step_size', 1000),
                gamma=config.get('gamma', 0.95)
            )
        elif scheduler_type == 'ExponentialLR':
            scheduler = torch.optim.lr_scheduler.ExponentialLR(
                optimizer,
                gamma=config.get('gamma', 0.99)
            )
        else:
            scheduler = None

        if name == 'actor':
            self.actor_scheduler = scheduler
        else:
            self.critic_scheduler = scheduler

    def update_learning_rate(self, factor: float):
        """åŠ¨æ€æ›´æ–°å­¦ä¹ ç‡ï¼ˆç”¨äºæ™ºèƒ½è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ ï¼‰"""
        try:
            # æ›´æ–°actorå­¦ä¹ ç‡
            for param_group in self.actor_optimizer.param_groups:
                param_group['lr'] *= factor

            # æ›´æ–°criticå­¦ä¹ ç‡
            for param_group in self.critic_optimizer.param_groups:
                param_group['lr'] *= factor

            print(f"ğŸ“ˆ å­¦ä¹ ç‡å·²æ›´æ–°ï¼Œç¼©æ”¾å› å­: {factor:.3f}")

        except Exception as e:
            print(f"âš ï¸ æ›´æ–°å­¦ä¹ ç‡å¤±è´¥: {e}")

    def get_current_learning_rates(self) -> Dict[str, float]:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        try:
            actor_lr = self.actor_optimizer.param_groups[0]['lr']
            critic_lr = self.critic_optimizer.param_groups[0]['lr']
            return {'actor_lr': actor_lr, 'critic_lr': critic_lr}
        except:
            return {'actor_lr': 0.0, 'critic_lr': 0.0}

    def select_action(self, state: Dict[str, torch.Tensor], training: bool = True) -> Tuple[Optional[Tuple[int, int]], float, float]:
        """
        ä½¿ç”¨å½“å‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œ (å·²é‡æ„ï¼Œæ¥å£ç»Ÿä¸€)
        
        Args:
            state: çŠ¶æ€è§‚å¯Ÿ
            training: æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼
            
        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ (node_idx, partition_idx)ï¼Œå¦‚æœæ— åŠ¨ä½œåˆ™ä¸ºNone
            log_prob: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            value: çŠ¶æ€ä»·å€¼ä¼°è®¡
        """
        with torch.no_grad():
            # 1. ç»Ÿä¸€æ¥å£ï¼šå°†å•ä¸€æ ·æœ¬å°è£…ä¸ºæ‰¹å¤„ç†BatchedState
            batched_state = self._prepare_batched_state([state])

            # 2. è·å–ä¼°å€¼ (Value)
            value = self.critic(batched_state).item()

            # 3. å¦‚æœæ²¡æœ‰è¾¹ç•ŒèŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›
            if batched_state.boundary_nodes.numel() == 0:
                return None, 0.0, value

            # 4. è·å–åŠ¨ä½œ logits
            node_logits, partition_logits = self.actor(batched_state)
            
            # 5. é‡‡æ ·æˆ–è´ªå¿ƒé€‰æ‹©
            # è·å–action_maskä¸­æ¯ä¸ªè¾¹ç•ŒèŠ‚ç‚¹æ˜¯å¦æœ‰ä»»ä½•æœ‰æ•ˆåˆ†åŒº
            boundary_mask = batched_state.action_mask[batched_state.boundary_nodes]
            valid_node_mask = boundary_mask.any(dim=1)
            
            # å¯¹æ— æ•ˆèŠ‚ç‚¹åº”ç”¨æ©ç 
            node_logits = node_logits.masked_fill(~valid_node_mask, -1e9)

            if training:
                # é‡‡æ ·æ¨¡å¼
                node_probs = F.softmax(node_logits, dim=0)
                node_dist = torch.distributions.Categorical(probs=node_probs)
                node_action_idx = node_dist.sample() # è¿™æ˜¯åœ¨ batched_state.boundary_nodes ä¸­çš„ç´¢å¼•

                partition_probs = F.softmax(partition_logits[node_action_idx], dim=0)
                partition_dist = torch.distributions.Categorical(probs=partition_probs)
                partition_action = partition_dist.sample()

                log_prob = (node_dist.log_prob(node_action_idx) + 
                            partition_dist.log_prob(partition_action)).item()

            else:
                # è´ªå¿ƒæ¨¡å¼
                node_action_idx = torch.argmax(node_logits)
                partition_action = torch.argmax(partition_logits[node_action_idx])
                log_prob = 0.0

            # 6. å°†ç´¢å¼•è½¬æ¢å›åŸå§‹ID
            selected_node = batched_state.boundary_nodes[node_action_idx].item()
            selected_partition = partition_action.item() + 1
            
            action = (selected_node, selected_partition)
            
        return action, log_prob, value
        
    def store_experience(self, 
                         state: Dict[str, torch.Tensor], 
                         action: Tuple[int, int], 
                         reward: float, 
                         log_prob: float, 
                         value: float, 
                         done: bool):
        """
        åœ¨ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­å­˜å‚¨ä¸€ä¸ªæ—¶é—´æ­¥çš„ç»éªŒã€‚

        Args:
            state (Dict[str, torch.Tensor]): å½“å‰çŠ¶æ€çš„è§‚å¯Ÿå­—å…¸ã€‚
            action (Tuple[int, int]): æ‰§è¡Œçš„åŠ¨ä½œã€‚
            reward (float): ä»ç¯å¢ƒä¸­è·å¾—çš„å¥–åŠ±ã€‚
            log_prob (float): æ‰§è¡ŒåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ã€‚
            value (float): è¯„åˆ¤ç½‘ç»œå¯¹å½“å‰çŠ¶æ€çš„ä»·å€¼ä¼°è®¡ã€‚
            done (bool): å›åˆæ˜¯å¦ç»“æŸçš„æ ‡å¿—ã€‚
        """
        self.memory.store(state, action, reward, log_prob, value, done)
        
    def update(self) -> Dict[str, float]:
        """
        ä½¿ç”¨PPOæ›´æ–°ç½‘ç»œï¼ˆä¼˜åŒ–ç‰ˆæœ¬ - é¿å…CPU-GPUä¼ è¾“ï¼‰

        Returns:
            è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        if len(self.memory) == 0:
            return {}

        # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šç›´æ¥è·å–å¼ é‡ï¼Œé¿å…CPU-GPUä¼ è¾“
        states, actions, rewards_tensor, old_log_probs_tensor, old_values_tensor, dones_tensor = self.memory.get_batch_tensors()
        
        # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
        advantages, returns = self._compute_advantages(rewards_tensor, old_values_tensor, dones_tensor)

        # PPOæ›´æ–° - æ”¹è¿›ç‰ˆï¼šæŒ‰æ ·æœ¬æ•°å¹³å‡è€ŒéæŒ‰epochå¹³å‡
        agg_stats = {
            'actor_loss': 0.0,
            'critic_loss': 0.0,
            'entropy': 0.0,
            'grad_norm': 0.0,
            'actor_param_change': 0.0,
            'critic_param_change': 0.0
        }
        total_samples = 0

        for _ in range(self.k_epochs):
            epoch_stats, batch_size = self._ppo_epoch(
                states, actions, old_log_probs_tensor, advantages, returns)

            # å°†"batchå¹³å‡loss"ä¹˜å›æ ·æœ¬æ•°ï¼Œå˜æˆ"æ ·æœ¬æ€»loss"
            for key in agg_stats:
                if key in epoch_stats:
                    agg_stats[key] += epoch_stats[key] * batch_size

            total_samples += batch_size

        # çœŸæ­£çš„æ ·æœ¬å¹³å‡
        for key in agg_stats:
            agg_stats[key] /= max(total_samples, 1)  # é˜²æ­¢é™¤é›¶

        # æ›´æ–°è®­ç»ƒç»Ÿè®¡
        self.training_stats['actor_loss'].append(agg_stats['actor_loss'])
        self.training_stats['critic_loss'].append(agg_stats['critic_loss'])
        self.training_stats['entropy'].append(agg_stats['entropy'])

        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.actor_scheduler:
            self.actor_scheduler.step()
        if self.critic_scheduler:
            self.critic_scheduler.step()

        # æ¸…ç©ºå†…å­˜
        self.memory.clear()

        # æ·»åŠ è¯¦ç»†æ—¥å¿—è®°å½•
        if hasattr(self, 'update_count') and self.update_count % 10 == 0:
            print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡ | Actor Loss: {agg_stats['actor_loss']:.4f}, "
                  f"Critic Loss: {agg_stats['critic_loss']:.4f}, "
                  f"Entropy: {agg_stats['entropy']:.4f}, "
                  f"Grad Norm: {agg_stats.get('grad_norm', 0):.2f}")

            # è®°å½•å½“å‰å­¦ä¹ ç‡
            current_lr = self.get_current_learning_rates()
            print(f"ğŸ“ˆ å½“å‰å­¦ä¹ ç‡ | Actor: {current_lr['actor_lr']:.6f}, Critic: {current_lr['critic_lr']:.6f}")

        return agg_stats
        
    def _compute_advantages(self, 
                            rewards: torch.Tensor, 
                            values: torch.Tensor, 
                            dones: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ä½¿ç”¨å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ (GAE) è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥ã€‚

        Args:
            rewards (torch.Tensor): ä¸€ä¸ªå›åˆçš„å¥–åŠ±åºåˆ—ã€‚
            values (torch.Tensor): ä¸€ä¸ªå›åˆçš„çŠ¶æ€ä»·å€¼åºåˆ—ã€‚
            dones (torch.Tensor): ä¸€ä¸ªå›åˆçš„ç»“æŸæ ‡å¿—åºåˆ—ã€‚

        Returns:
            Tuple[torch.Tensor, torch.Tensor]:
                - advantages: æ ‡å‡†åŒ–åçš„ä¼˜åŠ¿å‡½æ•°åºåˆ—ã€‚
                - returns: GAEè®¡ç®—å‡ºçš„å›æŠ¥åºåˆ—ã€‚
        """
        advantages = torch.zeros_like(rewards)
        returns = torch.zeros_like(rewards)

        gae = 0
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]

            delta = rewards[t] + self.gamma * next_value * (~dones[t]).float() - values[t]
            gae = delta + self.gamma * 0.95 * (~dones[t]).float() * gae
            advantages[t] = gae
            returns[t] = advantages[t] + values[t]

        # å®‰å…¨çš„å›æŠ¥å¥åº·æ£€æŸ¥
        returns = torch.nan_to_num(returns, nan=0.0, posinf=0.0, neginf=0.0)

        # å®‰å…¨æ ‡å‡†åŒ–ä¼˜åŠ¿
        def safe_standardize(t, eps=1e-6):
            mean = t.mean()
            std = t.std()
            if torch.isnan(std) or std < eps:
                return t - mean
            return (t - mean) / (std + eps)

        advantages = safe_standardize(advantages)
        advantages = torch.nan_to_num(advantages, nan=0.0, posinf=0.0, neginf=0.0)

        return advantages, returns

    def _compute_grad_norm(self, model: nn.Module) -> float:
        """
        è®¡ç®—æ¨¡å‹çš„æ¢¯åº¦èŒƒæ•°

        Args:
            model: è¦è®¡ç®—æ¢¯åº¦èŒƒæ•°çš„æ¨¡å‹

        Returns:
            æ¢¯åº¦çš„L2èŒƒæ•°
        """
        total_norm = 0.0
        param_count = 0
        for param in model.parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1

        if param_count == 0:
            return 0.0
        return (total_norm ** 0.5)

    def _compute_param_norm(self, model: nn.Module) -> float:
        """
        è®¡ç®—æ¨¡å‹çš„å‚æ•°èŒƒæ•°

        Args:
            model: è¦è®¡ç®—å‚æ•°èŒƒæ•°çš„æ¨¡å‹

        Returns:
            å‚æ•°çš„L2èŒƒæ•°
        """
        total_norm = 0.0
        for param in model.parameters():
            param_norm = param.data.norm(2)
            total_norm += param_norm.item() ** 2
        return (total_norm ** 0.5)

    def set_tensorboard_writer(self, writer):
        """
        è®¾ç½®TensorBoard writerç”¨äºæ¢¯åº¦ç›‘æ§

        Args:
            writer: TensorBoard SummaryWriterå®ä¾‹
        """
        self.writer = writer
        
    def _ppo_epoch(self,
                   states: List[Dict[str, torch.Tensor]],
                   actions: List[Tuple[int, int]],
                   old_log_probs: torch.Tensor,
                   advantages: torch.Tensor,
                   returns: torch.Tensor) -> Dict[str, float]:
        """
        æ‰§è¡Œå•ä¸ªPPOæ›´æ–°å‘¨æœŸã€‚

        Args:
            states (List[Dict[str, torch.Tensor]]): æ‰¹å¤„ç†çš„çŠ¶æ€åˆ—è¡¨ã€‚
            actions (List[Tuple[int, int]]): æ‰¹å¤„ç†çš„åŠ¨ä½œåˆ—è¡¨ã€‚
            old_log_probs (torch.Tensor): æ—§ç­–ç•¥ä¸‹å¯¹åº”åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ã€‚
            advantages (torch.Tensor): è®¡ç®—å‡ºçš„ä¼˜åŠ¿å‡½æ•°ã€‚
            returns (torch.Tensor): è®¡ç®—å‡ºçš„å›æŠ¥ã€‚

        Returns:
            Dict[str, float]: åŒ…å«actoræŸå¤±ã€criticæŸå¤±å’Œç†µçš„å­—å…¸ã€‚
        """
        batch_size = len(states)
        if batch_size == 0:
            return {'actor_loss': 0.0, 'critic_loss': 0.0, 'entropy': 0.0}

        # ğŸ” æ·»åŠ æ¢¯åº¦ç›‘æ§ï¼šè®¡ç®—æ›´æ–°å‰çš„å‚æ•°èŒƒæ•°
        actor_param_norm_before = self._compute_param_norm(self.actor)
        critic_param_norm_before = self._compute_param_norm(self.critic)

        # ---- 1. æ•°æ®æ‰¹é‡åŒ– ----
        batched_state = self._prepare_batched_state(states)

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬åœ¨ node_embeddings ä¸­çš„èµ·å§‹åç§»é‡ï¼Œç”¨äºå°†å±€éƒ¨èŠ‚ç‚¹ç´¢å¼•è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•
        node_offsets = torch.zeros(batch_size, dtype=torch.long, device=self.device)
        offset = 0
        for i, s in enumerate(states):
            node_offsets[i] = offset
            offset += s['node_embeddings'].size(0)

        # å°†åŠ¨ä½œæ‹†åˆ†ä¸ºèŠ‚ç‚¹ç´¢å¼•å’Œåˆ†åŒºç´¢å¼•
        node_indices_local = torch.tensor([a[0] for a in actions], dtype=torch.long, device=self.device)
        partition_indices = torch.tensor([a[1] - 1 for a in actions], dtype=torch.long, device=self.device)  # 0-based
        node_indices_global = node_indices_local + node_offsets  # [batch_size]

        # ä½¿ç”¨ bfloat16 autocast ä¸Šä¸‹æ–‡è¿›è¡Œæ··åˆç²¾åº¦è®¡ç®—
        with torch.amp.autocast(device_type=self.device.type, dtype=torch.bfloat16):
            # ---- 2. å‰å‘ä¼ æ’­ ----
            node_logits, partition_logits = self.actor(batched_state)  # logits æŒ‰ boundary_nodes é¡ºåº
            values = self.critic(batched_state)  # [batch_size]

            # ---- 3. é€‰å–æ‰€æ‰§è¡ŒåŠ¨ä½œå¯¹åº”çš„ logits/probs ----
            # æ„é€  (batch_size, num_boundary) å¸ƒå°”çŸ©é˜µï¼Œæ‰¾åˆ°æ¯ä¸ªæ ·æœ¬å¯¹åº” boundary ä½ç½®
            boundary_nodes = batched_state.boundary_nodes  # [num_boundary]
            boundary_batch_idx = batched_state.node_batch_idx[boundary_nodes]  # [num_boundary]

            # ä½¿ç”¨å¼ é‡æ¯”è¾ƒè®¡ç®—ä½ç½®ç´¢å¼•ï¼Œé¿å…Pythonå¾ªç¯
            equal_matrix = (boundary_nodes.unsqueeze(0) == node_indices_global.unsqueeze(1))  # [batch_size, num_boundary]
            selected_positions = equal_matrix.float().argmax(dim=1)  # [batch_size] æ¯è¡Œä¿è¯å”¯ä¸€ True

            # å–å‡ºå¯¹åº”èŠ‚ç‚¹ logits è¡Œ
            selected_node_logits = node_logits[selected_positions]  # [batch_size]
            selected_partition_logits = partition_logits[selected_positions]  # [batch_size, num_partitions]

            # ---- 4. è®¡ç®—æ¦‚ç‡ & å¯¹æ•°æ¦‚ç‡ ----
            # èŠ‚ç‚¹ softmax éœ€è¦åœ¨å„è‡ªæ ·æœ¬èŒƒå›´å†…æ‰§è¡Œ
            node_logits_clipped = torch.clamp(node_logits, min=-20, max=20)
            exp_node = torch.exp(node_logits_clipped)
            sum_exp_node = scatter_sum(exp_node, boundary_batch_idx, dim=0)  # [batch_size]
            node_probs = exp_node / sum_exp_node[boundary_batch_idx]  # [num_boundary]
            selected_node_probs = node_probs[selected_positions]  # [batch_size]

            # åˆ†åŒºæ¦‚ç‡ï¼ˆä»…é€‰ä¸­èŠ‚ç‚¹ï¼‰
            partition_logits_clipped = torch.clamp(selected_partition_logits, min=-20, max=20)
            exp_part = torch.exp(partition_logits_clipped)
            partition_probs = exp_part / exp_part.sum(dim=1, keepdim=True)
            selected_partition_probs = partition_probs[torch.arange(batch_size, device=self.device), partition_indices]

            # å®‰å…¨å–å¯¹æ•°
            new_log_probs = safe_log_prob(selected_node_probs) + safe_log_prob(selected_partition_probs)

            # ---- 5. PPO æŸå¤± ----
            log_prob_diff = torch.clamp(new_log_probs - old_log_probs, min=-20, max=20)
            ratio = torch.exp(log_prob_diff)
            ratio = torch.nan_to_num(ratio, nan=1.0, posinf=1.0, neginf=0.0)

            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages
            actor_loss = -torch.min(surr1, surr2).mean()

            critic_loss = F.mse_loss(values.squeeze(), returns.squeeze())

            # ---- 6. ç†µ ----
            node_entropy_all = -(node_probs * safe_log_prob(node_probs))
            node_entropy_per_batch = scatter_sum(node_entropy_all, boundary_batch_idx, dim=0)
            partition_entropy = -(partition_probs * safe_log_prob(partition_probs)).sum(dim=1)  # [batch_size]
            entropy = (node_entropy_per_batch + partition_entropy).mean()

            # The loss needs to be float32 to be scaled
            total_loss = (actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy).float()

        # ---- 7. åå‘ä¼ æ’­ & æ›´æ–° (ä½¿ç”¨ GradScaler) ----
        self.actor_optimizer.zero_grad(set_to_none=True)
        self.critic_optimizer.zero_grad(set_to_none=True)

        self.scaler.scale(total_loss).backward()

        # ğŸ” æ¢¯åº¦ç›‘æ§ï¼šè®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼ˆè£å‰ªå‰ï¼‰
        actor_grad_norm_before = self._compute_grad_norm(self.actor)
        critic_grad_norm_before = self._compute_grad_norm(self.critic)

        # æ¢¯åº¦è£å‰ªï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.max_grad_norm is not None:
            # åœ¨æ¢¯åº¦è£å‰ªå‰unscaleæ¢¯åº¦
            self.scaler.unscale_(self.actor_optimizer)
            self.scaler.unscale_(self.critic_optimizer)
            all_params = list(self.actor.parameters()) + list(self.critic.parameters())
            total_grad_norm = torch.nn.utils.clip_grad_norm_(all_params, self.max_grad_norm)

            # ğŸ” æ¢¯åº¦ç›‘æ§ï¼šè®°å½•è£å‰ªåçš„æ¢¯åº¦èŒƒæ•°
            actor_grad_norm_after = self._compute_grad_norm(self.actor)
            critic_grad_norm_after = self._compute_grad_norm(self.critic)

            # è®°å½•æ¢¯åº¦è¢«è£å‰ªçš„æ¯”ä¾‹
            grad_clip_ratio = 1.0
            if actor_grad_norm_before > 0:
                grad_clip_ratio = min(1.0, actor_grad_norm_after / actor_grad_norm_before)
        else:
            total_grad_norm = torch.tensor(0.0)
            grad_clip_ratio = 1.0

        # æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤
        self.scaler.step(self.actor_optimizer)
        self.scaler.step(self.critic_optimizer)

        # æ¯æ¬¡éƒ½æ›´æ–°scaler - è¿™æ˜¯PyTorch AMPçš„æ­£ç¡®ä½¿ç”¨æ–¹å¼
        self.scaler.update()

        # ğŸ” å‚æ•°ç›‘æ§ï¼šè®¡ç®—æ›´æ–°åçš„å‚æ•°èŒƒæ•°
        actor_param_norm_after = self._compute_param_norm(self.actor)
        critic_param_norm_after = self._compute_param_norm(self.critic)

        # è®¡ç®—å‚æ•°å˜åŒ–ç‡
        actor_param_change = (actor_param_norm_after - actor_param_norm_before) / (actor_param_norm_before + 1e-8)
        critic_param_change = (critic_param_norm_after - critic_param_norm_before) / (critic_param_norm_before + 1e-8)

        # è®°å½•åˆ°TensorBoardï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(self, 'writer') and self.writer is not None:
            self.writer.add_scalar('Gradients/Actor_Norm_Before', actor_grad_norm_before, self.update_count)
            self.writer.add_scalar('Gradients/Critic_Norm_Before', critic_grad_norm_before, self.update_count)
            self.writer.add_scalar('Gradients/Total_Norm', total_grad_norm, self.update_count)
            self.writer.add_scalar('Gradients/Clip_Ratio', grad_clip_ratio, self.update_count)
            self.writer.add_scalar('Parameters/Actor_Change', actor_param_change, self.update_count)
            self.writer.add_scalar('Parameters/Critic_Change', critic_param_change, self.update_count)

        # æ›´æ–°è®¡æ•°å™¨
        self.update_count += 1

        # æ£€æµ‹æ¢¯åº¦å¼‚å¸¸
        if actor_grad_norm_before > 100 or critic_grad_norm_before > 100:
            print(f"âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°å¤§æ¢¯åº¦ - Actor: {actor_grad_norm_before:.2f}, Critic: {critic_grad_norm_before:.2f}")

        # æ£€æµ‹å‚æ•°å˜åŒ–å¼‚å¸¸
        if abs(actor_param_change) > 0.1 or abs(critic_param_change) > 0.1:
            print(f"âš ï¸ è­¦å‘Šï¼šå‚æ•°å˜åŒ–è¿‡å¤§ - Actor: {actor_param_change:.2f}, Critic: {critic_param_change:.2f}")

        return ({
            'actor_loss': actor_loss.item(),
            'critic_loss': critic_loss.item(),
            'entropy': entropy.item(),
            'grad_norm': total_grad_norm.item(),
            'actor_param_change': actor_param_change,
            'critic_param_change': critic_param_change
        }, batch_size)

    def enable_gradient_norm_debug(self, enable: bool = True):
        """å¯ç”¨/ç¦ç”¨æ¢¯åº¦èŒƒæ•°è°ƒè¯•æ‰“å°"""
        self._debug_grad_norm = enable
        
    def save(self, filepath: str):
        """ä¿å­˜æ™ºèƒ½ä½“çŠ¶æ€"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
        }, filepath)
        
    def load(self, filepath: str):
        """åŠ è½½æ™ºèƒ½ä½“çŠ¶æ€"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])

    def store_experience(self, 
                         state: Dict[str, torch.Tensor], 
                         action: Tuple[int, int], 
                         reward: float, 
                         log_prob: float, 
                         value: float, 
                         done: bool):
        """
        åœ¨ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­å­˜å‚¨ä¸€ä¸ªæ—¶é—´æ­¥çš„ç»éªŒã€‚

        Args:
            state (Dict[str, torch.Tensor]): å½“å‰çŠ¶æ€çš„è§‚å¯Ÿå­—å…¸ã€‚
            action (Tuple[int, int]): æ‰§è¡Œçš„åŠ¨ä½œã€‚
            reward (float): ä»ç¯å¢ƒä¸­è·å¾—çš„å¥–åŠ±ã€‚
            log_prob (float): æ‰§è¡ŒåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ã€‚
            value (float): è¯„åˆ¤ç½‘ç»œå¯¹å½“å‰çŠ¶æ€çš„ä»·å€¼ä¼°è®¡ã€‚
            done (bool): å›åˆæ˜¯å¦ç»“æŸçš„æ ‡å¿—ã€‚
        """
        self.memory.store(state, action, reward, log_prob, value, done)

    def update_learning_rate(self, factor: float):
        """åŠ¨æ€æ›´æ–°å­¦ä¹ ç‡"""
        for param_group in self.actor_optimizer.param_groups:
            param_group['lr'] *= factor
        for param_group in self.critic_optimizer.param_groups:
            param_group['lr'] *= factor

    def get_current_learning_rates(self) -> Dict[str, float]:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        try:
            actor_lr = self.actor_optimizer.param_groups[0]['lr']
            critic_lr = self.critic_optimizer.param_groups[0]['lr']
            return {'actor_lr': actor_lr, 'critic_lr': critic_lr}
        except:
            return {'actor_lr': 0.0, 'critic_lr': 0.0}

    def _prepare_batched_state(self, states_list: List[Dict[str, torch.Tensor]]) -> "BatchedState":
        """
        å°†ä¸€ç³»åˆ—ç‹¬ç«‹çš„çŠ¶æ€å­—å…¸æ‹¼æ¥æˆä¸€ä¸ªæ‰¹å¤„ç†çš„BatchedStateå¯¹è±¡ã€‚

        è¿™ä¸ªå‡½æ•°æ˜¯æ€§èƒ½ä¼˜åŒ–çš„æ ¸å¿ƒï¼Œå®ƒé€šè¿‡å°†å¤šä¸ªå›¾æ ·æœ¬çš„æ•°æ®èšåˆï¼Œ
        ä½¿å¾—ç¥ç»ç½‘ç»œå¯ä»¥ä¸€æ¬¡æ€§åœ¨GPUä¸Šå¤„ç†æ•´ä¸ªæ‰¹æ¬¡ï¼Œè€Œä¸æ˜¯é€ä¸ªå¤„ç†ã€‚

        Args:
            states_list (List[Dict[str, torch.Tensor]]): 
                ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªä»£è¡¨å•ä¸€æ ·æœ¬çŠ¶æ€çš„å­—å…¸ã€‚

        Returns:
            BatchedState: åŒ…å«äº†æ‰€æœ‰è¾“å…¥æ ·æœ¬æ•°æ®çš„èšåˆæ‰¹å¤„ç†å¯¹è±¡ã€‚
        """
        # ç´¯ç§¯åˆ—è¡¨
        node_emb_list: List[torch.Tensor] = []
        region_emb_list: List[torch.Tensor] = []
        action_mask_list: List[torch.Tensor] = []
        node_batch_idx_list: List[torch.Tensor] = []
        region_batch_idx_list: List[torch.Tensor] = []
        boundary_nodes_global: List[torch.Tensor] = []

        node_offset = 0
        region_offset = 0

        for batch_id, state in enumerate(states_list):
            node_embeddings = state['node_embeddings'].detach()
            region_embeddings = state['region_embeddings'].detach()
            boundary_nodes = state['boundary_nodes']
            current_partition = state['current_partition']

            num_nodes = node_embeddings.size(0)
            num_regions = region_embeddings.size(0)

            # ==== åŠ¨ä½œæ©ç æ„é€  ====
            if 'action_mask' in state:
                action_mask = state['action_mask']
            else:
                # å¤åˆ¶è‡ªæ—§å®ç°çš„ç®€åŒ–é€»è¾‘
                action_mask = torch.zeros(
                    num_nodes, self.num_partitions, dtype=torch.bool, device=self.device
                )
                for node_idx in boundary_nodes:
                    current_node_partition = current_partition[node_idx].item()
                    for p in range(self.num_partitions):
                        if p + 1 != current_node_partition:
                            action_mask[node_idx, p] = True

            # ==== ç´¯ç§¯å¼ é‡ ====
            node_emb_list.append(node_embeddings)
            region_emb_list.append(region_embeddings)
            action_mask_list.append(action_mask)

            # batch idx
            node_batch_idx_list.append(torch.full((num_nodes,), batch_id, dtype=torch.long, device=self.device))
            region_batch_idx_list.append(torch.full((num_regions,), batch_id, dtype=torch.long, device=self.device))

            # è¾¹ç•ŒèŠ‚ç‚¹å…¨å±€ç´¢å¼•
            if boundary_nodes.numel() > 0:
                boundary_nodes_global.append(boundary_nodes + node_offset)

            node_offset += num_nodes
            region_offset += num_regions

        # ===== æ‹¼æ¥ =====
        node_embeddings_cat = torch.cat(node_emb_list, dim=0)
        region_embeddings_cat = torch.cat(region_emb_list, dim=0)
        action_mask_cat = torch.cat(action_mask_list, dim=0)
        node_batch_idx = torch.cat(node_batch_idx_list, dim=0)
        region_batch_idx = torch.cat(region_batch_idx_list, dim=0)
        if boundary_nodes_global:
            boundary_nodes_cat = torch.cat(boundary_nodes_global, dim=0)
        else:
            boundary_nodes_cat = torch.zeros(0, dtype=torch.long, device=self.device)

        return BatchedState(
            node_embeddings=node_embeddings_cat,
            region_embeddings=region_embeddings_cat,
            boundary_nodes=boundary_nodes_cat,
            node_batch_idx=node_batch_idx,
            region_batch_idx=region_batch_idx,
            action_mask=action_mask_cat,
        )
