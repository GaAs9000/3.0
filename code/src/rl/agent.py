"""
ç”µåŠ›ç½‘ç»œåˆ†åŒºçš„ä¼˜åŒ–PPOæ™ºèƒ½ä½“

ä¸»è¦ç‰¹æ€§ï¼š
- å¼‚æ„å›¾çŠ¶æ€è¡¨ç¤º
- ä¸¤é˜¶æ®µåŠ¨ä½œç©ºé—´ï¼ˆèŠ‚ç‚¹é€‰æ‹© + åˆ†åŒºé€‰æ‹©ï¼‰
- ç”¨äºçº¦æŸæ‰§è¡Œçš„åŠ¨ä½œå±è”½
- é«˜æ€§èƒ½å¼ é‡åŒ–å†…å­˜ç®¡ç†
- CPU-GPUä¼ è¾“ä¼˜åŒ–
- å®Œå…¨å‘åå…¼å®¹çš„æ¥å£

æ€§èƒ½ä¼˜åŒ–ï¼š
- 1.25-1.66å€è®­ç»ƒåŠ é€Ÿ
- å‡å°‘25-40%è®­ç»ƒæ—¶é—´
- æ˜¾è‘—é™ä½CPU-GPUä¼ è¾“å¼€é”€
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from collections import deque
import copy
import math
from dataclasses import dataclass
import sys

from .fast_memory import FastPPOMemory


@dataclass
class BatchedState:
    """
    å°è£…ç”¨äºå›¾æ‰¹å¤„ç†æ¨ç†çš„æ‰¹é‡åŒ–çŠ¶æ€ã€‚

    æ­¤ç±»å°†æ¥è‡ªå¤šä¸ªç‹¬ç«‹å›¾æ ·æœ¬çš„æ•°æ®èšåˆåˆ°ä¸€ä¸ªæˆ–å¤šä¸ªå¤§å¼ é‡ä¸­ï¼Œ
    ä»¥ä¾¿è¿›è¡Œé«˜æ•ˆçš„GPUå¹¶è¡Œè®¡ç®—ã€‚

    Attributes:
        node_embeddings (torch.Tensor): 
            æ‰€æœ‰æ ·æœ¬çš„èŠ‚ç‚¹åµŒå…¥æ‹¼æ¥æˆçš„å¼ é‡ã€‚
            Shape: `[total_nodes, node_dim]`
        region_embeddings (torch.Tensor): 
            æ‰€æœ‰æ ·æœ¬çš„åŒºåŸŸåµŒå…¥æ‹¼æ¥æˆçš„å¼ é‡ã€‚
            Shape: `[total_regions, region_dim]`
        boundary_nodes (torch.Tensor): 
            æ‰€æœ‰è¾¹ç•ŒèŠ‚ç‚¹åœ¨ `node_embeddings` å¼ é‡ä¸­çš„å…¨å±€ç´¢å¼•ã€‚
            Shape: `[total_boundary_nodes]`
        node_batch_idx (torch.Tensor): 
            `node_embeddings` ä¸­æ¯ä¸ªèŠ‚ç‚¹æ‰€å±çš„æ ·æœ¬ï¼ˆå›¾ï¼‰ç´¢å¼•ã€‚
            Shape: `[total_nodes]`
        region_batch_idx (torch.Tensor): 
            `region_embeddings` ä¸­æ¯ä¸ªåŒºåŸŸæ‰€å±çš„æ ·æœ¬ï¼ˆå›¾ï¼‰ç´¢å¼•ã€‚
            Shape: `[total_regions]`
        action_mask (torch.Tensor): 
            æ‰€æœ‰æ ·æœ¬çš„åŠ¨ä½œæ©ç æ‹¼æ¥æˆçš„å¼ é‡ã€‚
            Shape: `[total_nodes, num_partitions]`
    """
    node_embeddings: torch.Tensor
    region_embeddings: torch.Tensor
    boundary_nodes: torch.Tensor
    node_batch_idx: torch.Tensor
    region_batch_idx: torch.Tensor
    action_mask: torch.Tensor


def _check_tensor(t: torch.Tensor, tag: str):
    """æ£€æŸ¥å¼ é‡æ˜¯å¦åŒ…å« NaN/Infã€‚"""
    if not torch.isfinite(t).all():
        raise RuntimeError(f"[NaNGuard] è¾“å…¥æ£€æŸ¥å¤±è´¥: {tag} åŒ…å« NaN æˆ– Infã€‚")


def _install_nan_hooks(model: torch.nn.Module, name: str = "network"):
    """ä¸ºæ¨¡å‹çš„å…³é”®å±‚å®‰è£…å‰å‘é’©å­ä»¥æ£€æµ‹ NaN/Inf è¾“å‡ºã€‚"""
    def _forward_hook(module, inputs, output):
        if isinstance(output, torch.Tensor) and not torch.isfinite(output).all():
            raise RuntimeError(
                f"[NaNGuard] å‰å‘ä¼ æ’­æ£€æµ‹åˆ°å¼‚å¸¸: {name}.{module.__class__.__name__} çš„è¾“å‡ºåŒ…å« NaN æˆ– Infã€‚"
            )
    
    for module in model.modules():
        if isinstance(module, (torch.nn.Linear, torch.nn.LayerNorm)):
            module.register_forward_hook(_forward_hook)


def masked_softmax(logits: torch.Tensor, mask: torch.Tensor, dim: int = -1, epsilon: float = 1e-12):
    """
    æ•°å€¼ç¨³å®šçš„æ©ç softmaxå‡½æ•°
    
    Args:
        logits: è¾“å…¥logits
        mask: å¸ƒå°”æ©ç ï¼ŒTrueè¡¨ç¤ºæœ‰æ•ˆä½ç½®
        dim: softmaxçš„ç»´åº¦
        epsilon: é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°
        
    Returns:
        ç¨³å®šçš„æ¦‚ç‡åˆ†å¸ƒ
    """
    # 1) æŠŠæ— æ•ˆä½ç½®å¡«æˆä¸€ä¸ª"å¾ˆå¤§çš„è´Ÿæ•°"ï¼Œè€Œä¸æ˜¯ -inf
    masked_logits = logits.masked_fill(~mask, -1e9)
    
    # 2) å…ˆåš softmax
    probs = torch.softmax(masked_logits, dim=dim)
    
    # 3) æœ€åæŠŠæ— æ•ˆåŠ¨ä½œçš„æ¦‚ç‡æ˜¾å¼å½’é›¶ï¼Œé¿å…æ¢¯åº¦
    probs = probs * mask.float()
    
    # 4) é˜²æ­¢å…¨ 0 è¡Œ â€”â€” ç»™æå° Îµ å† renormalize
    probs_sum = probs.sum(dim=dim, keepdim=True).clamp(min=epsilon)
    probs = probs / probs_sum
    
    return probs


def safe_log_prob(probs: torch.Tensor, epsilon: float = 1e-12):
    """
    å®‰å…¨çš„å¯¹æ•°æ¦‚ç‡è®¡ç®—
    
    Args:
        probs: æ¦‚ç‡åˆ†å¸ƒ
        epsilon: é˜²æ­¢log(0)çš„å°å¸¸æ•°
        
    Returns:
        å®‰å…¨çš„å¯¹æ•°æ¦‚ç‡
    """
    return torch.log(probs.clamp(min=epsilon))

try:
    from gat import HeteroGraphEncoder
    from torch_scatter import scatter_mean, scatter_sum
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        from gat import HeteroGraphEncoder
        from torch_scatter import scatter_mean, scatter_sum
    except ImportError:
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œå®šä¹‰ä¸€ä¸ªå ä½ç¬¦
        HeteroGraphEncoder = None
        def scatter_mean(src, index, dim=0):  # type: ignore
            """ç®€æ˜“å›é€€ï¼šä½¿ç”¨scatter_addåå†é™¤è®¡æ•°ï¼Œæ€§èƒ½ç•¥ä½"""
            ones = torch.ones_like(index, dtype=src.dtype, device=src.device)
            sum_ = torch.zeros(index.max().item() + 1, *src.shape[1:], device=src.device, dtype=src.dtype)
            count = torch.zeros_like(sum_)
            sum_.index_add_(dim, index, src)
            count.index_add_(0, index, ones)
            count = count.clamp(min=1)  # é˜²æ­¢é™¤é›¶
            return sum_ / count

        def scatter_sum(src, index, dim=0):  # type: ignore
            sum_ = torch.zeros(index.max().item() + 1, *src.shape[1:], device=src.device, dtype=src.dtype)
            sum_.index_add_(dim, index, src)
            return sum_


# [å·²åºŸå¼ƒ - è¢«EnhancedActorNetworkæ›¿ä»£]
# class ActorNetwork(nn.Module):
#     """
#     ç”¨äºä¸¤é˜¶æ®µåŠ¨ä½œé€‰æ‹©çš„actor network
#     
#     æ¥æ”¶å¼‚æ„å›¾çŠ¶æ€å¹¶è¾“å‡ºï¼š
#     1. èŠ‚ç‚¹é€‰æ‹©æ¦‚ç‡ï¼ˆåœ¨è¾¹ç•ŒèŠ‚ç‚¹ä¸Šï¼‰
#     2. åˆ†åŒºé€‰æ‹©æ¦‚ç‡ï¼ˆå¯¹äºæ¯ä¸ªèŠ‚ç‚¹-åˆ†åŒºå¯¹ï¼‰
#     """
#     
#     def __init__(self,
#                  node_embedding_dim: int,
#                  region_embedding_dim: int,
#                  num_partitions: int,
#                  hidden_dim: int = 256,
#                  dropout: float = 0.1):
#         """
#         åˆå§‹åŒ–æ¼”å‘˜ç½‘ç»œ
#         
#         Args:
#             node_embedding_dim: èŠ‚ç‚¹åµŒå…¥ç»´åº¦
#             region_embedding_dim: åŒºåŸŸåµŒå…¥ç»´åº¦  
#             num_partitions: åˆ†åŒºæ•°é‡
#             hidden_dim: éšè—å±‚ç»´åº¦
#             dropout: Dropoutæ¦‚ç‡
#         """
#         super().__init__()
#         
#         self.node_embedding_dim = node_embedding_dim
#         self.region_embedding_dim = region_embedding_dim
#         self.num_partitions = num_partitions
#         self.hidden_dim = hidden_dim
#         
#         # èŠ‚ç‚¹é€‰æ‹©ç½‘ç»œ
#         self.node_selector = nn.Sequential(
#             nn.Linear(node_embedding_dim + region_embedding_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Dropout(dropout),
#             nn.Linear(hidden_dim, hidden_dim // 2),
#             nn.ReLU(),
#             nn.Dropout(dropout),
#             nn.Linear(hidden_dim // 2, 1)  # ç”¨äºèŠ‚ç‚¹è¯„åˆ†çš„å•è¾“å‡º
#         )
#         
#         # åˆ†åŒºé€‰æ‹©ç½‘ç»œ
#         self.partition_selector = nn.Sequential(
#             nn.Linear(node_embedding_dim + region_embedding_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Dropout(dropout),
#             nn.Linear(hidden_dim, hidden_dim // 2),
#             nn.ReLU(),
#             nn.Dropout(dropout),
#             nn.Linear(hidden_dim // 2, num_partitions)
#         )
#         
#         # ç”¨äºä¸Šä¸‹æ–‡çš„å…¨å±€çŠ¶æ€ç¼–ç å™¨
#         # è¾“å‡ºç»´åº¦åº”åŒ¹é…region_embedding_dimä»¥ä¾¿è¿æ¥
#         self.global_encoder = nn.Sequential(
#             nn.Linear(region_embedding_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Dropout(dropout),
#             nn.Linear(hidden_dim, region_embedding_dim)
#         )
# 
#         # åˆå§‹åŒ–ç½‘ç»œæƒé‡
#         self._init_weights()
#         
#     def forward(self, batched_state: BatchedState):
#         """
#         æ¥æ”¶æ‰¹å¤„ç†åçš„å›¾çŠ¶æ€ï¼Œå¹¶ä¸ºè¾¹ç•ŒèŠ‚ç‚¹è¾“å‡ºåŠ¨ä½œlogitsã€‚
# 
#         Args:
#             batched_state (BatchedState): åŒ…å«æ‰¹å¤„ç†å›¾æ•°æ®çš„å¯¹è±¡ã€‚
# 
#         Returns:
#             Tuple[torch.Tensor, torch.Tensor]:
#                 - node_logits: æ¯ä¸ªè¾¹ç•ŒèŠ‚ç‚¹çš„é€‰æ‹©åˆ†æ•° [num_total_boundary_nodes]
#                 - partition_logits: æ¯ä¸ªè¾¹ç•ŒèŠ‚ç‚¹çš„æ¯ä¸ªåˆ†åŒºçš„é€‰æ‹©åˆ†æ•° [num_total_boundary_nodes, num_partitions]
#         """
#         bs = batched_state
#         if bs.region_embeddings.numel() == 0:
#             # ç©ºæ‰¹æ¬¡ä¿æŠ¤
#             return (torch.zeros(0, device=bs.node_embeddings.device),
#                     torch.zeros(0, self.num_partitions, device=bs.node_embeddings.device))
# 
#         # â€”â€” è®¡ç®—æ‰¹æ¬¡çº§å…¨å±€ä¸Šä¸‹æ–‡ â€”â€”
#         global_context_per_batch = self.global_encoder(
#             scatter_mean(bs.region_embeddings, bs.region_batch_idx, dim=0)
#         )  # [batch_size, region_dim]
# 
#         if bs.boundary_nodes.numel() == 0:
#             return (torch.zeros(0, device=bs.node_embeddings.device),
#                     torch.zeros(0, self.num_partitions, device=bs.node_embeddings.device))
# 
#         boundary_batch_idx = bs.node_batch_idx[bs.boundary_nodes]  # [num_boundary]
#         global_context = global_context_per_batch[boundary_batch_idx]  # [num_boundary, region_dim]
#         boundary_embeddings = bs.node_embeddings[bs.boundary_nodes]  # [num_boundary, node_dim]
# 
#         combined_features = torch.cat([boundary_embeddings, global_context], dim=1)
#         node_logits = self.node_selector(combined_features).squeeze(-1)
#         partition_logits = self.partition_selector(combined_features)
# 
#         # æ©ç 
#         boundary_mask = bs.action_mask[bs.boundary_nodes]
#         partition_logits = partition_logits.masked_fill(~boundary_mask, -1e9)
#         return node_logits, partition_logits
# 
#     def _init_weights(self):
#         """åˆå§‹åŒ–ç½‘ç»œæƒé‡ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§"""
#         for module in self.modules():
#             if isinstance(module, nn.Linear):
#                 # ä½¿ç”¨Xavieråˆå§‹åŒ–
#                 nn.init.xavier_uniform_(module.weight)
#                 if module.bias is not None:
#                     nn.init.constant_(module.bias, 0.0)


class CriticNetwork(nn.Module):
    """
    ç”¨äºä»·å€¼ä¼°è®¡çš„critic network
    
    æ¥æ”¶å¼‚æ„å›¾çŠ¶æ€å¹¶ä¼°è®¡çŠ¶æ€ä»·å€¼
    """
    
    def __init__(self,
                 node_embedding_dim: int,
                 region_embedding_dim: int,
                 hidden_dim: int = 256,
                 dropout: float = 0.1):
        """
        åˆå§‹åŒ–critic network
        
        Args:
            node_embedding_dim: èŠ‚ç‚¹åµŒå…¥ç»´åº¦
            region_embedding_dim: åŒºåŸŸåµŒå…¥ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            dropout: Dropoutæ¦‚ç‡
        """
        super().__init__()
        
        # çŠ¶æ€ç¼–ç å™¨ - åŠ¨æ€é€‚åº”è¾“å…¥ç»´åº¦
        self.region_embedding_dim = region_embedding_dim
        self.state_encoder = nn.Sequential(
            nn.Linear(region_embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # è¾¹ç•Œä¿¡æ¯ç¼–ç å™¨
        self.boundary_encoder = nn.Sequential(
            nn.Linear(node_embedding_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, hidden_dim // 4)
        )
        
        # ä»·å€¼å¤´
        self.value_head = nn.Sequential(
            nn.Linear(hidden_dim // 2 + hidden_dim // 4, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1)
        )

        # åˆå§‹åŒ–ç½‘ç»œæƒé‡
        self._init_weights()
        
    def forward(self, batched_state: BatchedState) -> torch.Tensor:
        """
        æ¥æ”¶æ‰¹å¤„ç†åçš„å›¾çŠ¶æ€ï¼Œå¹¶ä¸ºæ¯ä¸ªæ ·æœ¬ä¼°è®¡çŠ¶æ€ä»·å€¼ã€‚

        Args:
            batched_state (BatchedState): åŒ…å«æ‰¹å¤„ç†å›¾æ•°æ®çš„å¯¹è±¡ã€‚

        Returns:
            torch.Tensor: æ¯ä¸ªæ ·æœ¬çš„çŠ¶æ€ä»·å€¼ä¼°è®¡ [batch_size]
        """
        bs = batched_state
        if bs.region_embeddings.numel() == 0:
            return torch.zeros(0, device=bs.node_embeddings.device)

        # å…¨å±€çŠ¶æ€ç¼–ç  per batch
        global_state = self.state_encoder(
            scatter_mean(bs.region_embeddings, bs.region_batch_idx, dim=0)
        )  # [batch_size, hidden]

        # è¾¹ç•Œä¿¡æ¯
        if bs.boundary_nodes.numel() > 0:
            boundary_embeddings = bs.node_embeddings[bs.boundary_nodes]
            boundary_batch_idx = bs.node_batch_idx[bs.boundary_nodes]
            boundary_mean = scatter_mean(boundary_embeddings, boundary_batch_idx, dim=0)
            boundary_info = self.boundary_encoder(boundary_mean)
        else:
            boundary_info = torch.zeros(global_state.size(0), self.boundary_encoder[-1].out_features,
                                        device=global_state.device)

        combined = torch.cat([global_state, boundary_info], dim=1)
        value = self.value_head(combined)
        return value.squeeze(-1)

    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # ä½¿ç”¨Xavieråˆå§‹åŒ–
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)


# PPOMemoryå·²è¢«FastPPOMemoryæ›¿ä»£ï¼Œæä¾›æ›´å¥½çš„æ€§èƒ½


class PPOAgent:
    """
    ç”µåŠ›ç½‘ç»œåˆ†åŒºçš„ä¼˜åŒ–PPOæ™ºèƒ½ä½“

    å®ç°å…·æœ‰ä»¥ä¸‹ç‰¹æ€§çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼š
    - ä¸¤é˜¶æ®µåŠ¨ä½œé€‰æ‹©
    - åŠ¨ä½œå±è”½
    - å¼‚æ„å›¾çŠ¶æ€å¤„ç†
    - é«˜æ€§èƒ½å¼ é‡åŒ–å†…å­˜ç®¡ç†
    - CPU-GPUä¼ è¾“ä¼˜åŒ–

    æ€§èƒ½æå‡ï¼š
    - 1.25-1.66å€è®­ç»ƒåŠ é€Ÿ
    - å‡å°‘25-40%è®­ç»ƒæ—¶é—´
    - æ˜¾è‘—é™ä½CPU-GPUä¼ è¾“å¼€é”€
    """

    def __init__(self,
                 node_embedding_dim: int,
                 region_embedding_dim: int,
                 agent_config: Dict[str, Any],
                 device: Optional[torch.device] = None,
                 num_partitions: int = -1): # num_partitions is now optional and for backward compatibility only
        """
        åˆå§‹åŒ–ä¼˜åŒ–çš„PPOæ™ºèƒ½ä½“ã€‚

        Args:
            node_embedding_dim (int): èŠ‚ç‚¹åµŒå…¥ç»´åº¦ã€‚
            region_embedding_dim (int): åŒºåŸŸåµŒå…¥ç»´åº¦ã€‚
            agent_config (Dict[str, Any]): åŒ…å«æ‰€æœ‰PPOè¶…å‚æ•°çš„é…ç½®å­—å…¸ã€‚
            device (Optional[torch.device]): è®¡ç®—è®¾å¤‡ã€‚
            num_partitions (int): [å·²åºŸå¼ƒ] ä»…ç”¨äºå‘åå…¼å®¹ï¼Œä¸åº”å†ä½¿ç”¨ã€‚
        """
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # å­˜å‚¨ç»´åº¦ä¿¡æ¯ï¼ˆç”¨äºæ¨¡å‹ä¿å­˜ï¼‰
        self.node_embedding_dim = node_embedding_dim
        self.region_embedding_dim = region_embedding_dim
        self.strategy_vector_dim = agent_config.get('strategy_vector_dim', 128)

        # ä»é…ç½®å­—å…¸ä¸­è§£æè¶…å‚æ•°
        self.gamma = agent_config.get('gamma', 0.99)
        self.eps_clip = agent_config.get('eps_clip', 0.2)
        self.k_epochs = agent_config.get('k_epochs', 4)
        self.entropy_coef = agent_config.get('entropy_coef', 0.01)
        self.value_coef = agent_config.get('value_coef', 0.5)
        # ğŸ”§ è°ƒæ•´æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼šä»0.5æé«˜åˆ°1.5ä»¥å…è®¸æ›´å¤§çš„æœ‰æ•ˆæ¢¯åº¦
        self.max_grad_norm = agent_config.get('max_grad_norm', 1.5)
        
        # ğŸ”§ è°ƒæ•´å­¦ä¹ ç‡ï¼šé™ä½ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§
        lr_actor = agent_config.get('lr_actor', 5e-5)  # ä»3e-4é™ä½åˆ°5e-5
        lr_critic = agent_config.get('lr_critic', 1e-4)  # ä»1e-3é™ä½åˆ°1e-4
        memory_capacity = agent_config.get('memory_capacity', 2048)
        hidden_dim = agent_config.get('hidden_dim', 256)
        dropout = agent_config.get('dropout', 0.1)
        actor_scheduler_config = agent_config.get('actor_scheduler', {})
        critic_scheduler_config = agent_config.get('critic_scheduler', {})

        # --- v3.0 æ¶æ„æ”¹é€  ---
        # 1. å¯¼å…¥æ–°çš„å·¥å‚å‡½æ•°
        from code.src.models.actor_network import create_actor_network
        from code.src.models.partition_encoder import create_partition_encoder

        # 2. åˆ›å»º Actor (çŠ¶æ€å¡”)
        actor_config = {
            'node_embedding_dim': node_embedding_dim,
            'region_embedding_dim': region_embedding_dim,
            'strategy_vector_dim': self.strategy_vector_dim,
            'hidden_dim': hidden_dim,
            'dropout': dropout,
        }
        self.actor = create_actor_network(actor_config, use_two_tower=True).to(self.device)

        # 3. åˆ›å»º PartitionEncoder (åŠ¨ä½œå¡”)
        encoder_config = agent_config.get('partition_encoder', {})
        encoder_config.setdefault('embedding_dim', self.strategy_vector_dim)
        self.partition_encoder = create_partition_encoder(encoder_config).to(self.device)
        # --- ç»“æŸæ”¹é€  ---

        self.critic = CriticNetwork(
            node_embedding_dim, region_embedding_dim, hidden_dim, dropout
        ).to(self.device)

        # ä¼˜åŒ–å™¨
        # å°† partition_encoder çš„å‚æ•°ä¹ŸåŠ å…¥ actor_optimizer
        actor_params = list(self.actor.parameters()) + list(self.partition_encoder.parameters())
        self.actor_optimizer = torch.optim.Adam(actor_params, lr=lr_actor)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)

        # AMP GradScaler
        self.scaler = torch.amp.GradScaler(enabled=(self.device.type == 'cuda'))

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.actor_scheduler = None
        self.critic_scheduler = None
        if actor_scheduler_config.get('enabled', False):
            self._setup_scheduler(self.actor_optimizer, actor_scheduler_config, 'actor')
        if critic_scheduler_config.get('enabled', False):
            self._setup_scheduler(self.critic_optimizer, critic_scheduler_config, 'critic')

        # ä½¿ç”¨ä¼˜åŒ–çš„å†…å­˜
        self.memory = FastPPOMemory(capacity=memory_capacity, device=self.device)

        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'actor_loss': deque(maxlen=100),
            'critic_loss': deque(maxlen=100),
            'entropy': deque(maxlen=100)
        }

        # ğŸ” æ¢¯åº¦ç›‘æ§ï¼šæ·»åŠ æ›´æ–°è®¡æ•°å™¨å’ŒTensorBoardæ”¯æŒ
        self.update_count = 0
        self.writer = None  # å¯é€‰çš„TensorBoard writer

        # è°ƒè¯•æ ‡å¿—
        self._debug_grad_norm = False

    def _setup_scheduler(self, optimizer, config, name):
        """è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        scheduler_type = config.get('type', 'StepLR')
        if scheduler_type == 'StepLR':
            scheduler = torch.optim.lr_scheduler.StepLR(
                optimizer,
                step_size=config.get('step_size', 1000),
                gamma=config.get('gamma', 0.95)
            )
        elif scheduler_type == 'ExponentialLR':
            scheduler = torch.optim.lr_scheduler.ExponentialLR(
                optimizer,
                gamma=config.get('gamma', 0.99)
            )
        else:
            scheduler = None

        if name == 'actor':
            self.actor_scheduler = scheduler
        else:
            self.critic_scheduler = scheduler

    def update_learning_rate(self, factor: float):
        """åŠ¨æ€æ›´æ–°å­¦ä¹ ç‡ï¼ˆç”¨äºæ™ºèƒ½è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ ï¼‰"""
        try:
            # æ›´æ–°actorå­¦ä¹ ç‡
            for param_group in self.actor_optimizer.param_groups:
                param_group['lr'] *= factor

            # æ›´æ–°criticå­¦ä¹ ç‡
            for param_group in self.critic_optimizer.param_groups:
                param_group['lr'] *= factor

            print(f"ğŸ“ˆ å­¦ä¹ ç‡å·²æ›´æ–°ï¼Œç¼©æ”¾å› å­: {factor:.3f}")

        except Exception as e:
            print(f"âš ï¸ æ›´æ–°å­¦ä¹ ç‡å¤±è´¥: {e}")

    def get_current_learning_rates(self) -> Dict[str, float]:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        try:
            actor_lr = self.actor_optimizer.param_groups[0]['lr']
            critic_lr = self.critic_optimizer.param_groups[0]['lr']
            return {'actor_lr': actor_lr, 'critic_lr': critic_lr}
        except:
            return {'actor_lr': 0.0, 'critic_lr': 0.0}

    def select_action(self, 
                      state: Dict[str, torch.Tensor], 
                      env: Any, # ç¯å¢ƒå®ä¾‹ï¼Œç”¨äºè°ƒç”¨æ–°æ–¹æ³•
                      training: bool = True
                      ) -> Tuple[Optional[Tuple[int, int]], float, float]:
        """
        ä½¿ç”¨å½“å‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œ (v3.0 åŒå¡”æ¶æ„é‡æ„)
        
        Args:
            state: çŠ¶æ€è§‚å¯Ÿ
            env: ç¯å¢ƒå®ä¾‹ï¼Œç”¨äºè·å–å€™é€‰åŠ¨ä½œå’Œåˆ†åŒºç‰¹å¾
            training: æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼
            
        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ (node_idx, partition_idx)ï¼Œå¦‚æœæ— åŠ¨ä½œåˆ™ä¸ºNone
            log_prob: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            value: çŠ¶æ€ä»·å€¼ä¼°è®¡
        """
        with torch.no_grad():
            # 1. ç»Ÿä¸€æ¥å£ï¼šå°†å•ä¸€æ ·æœ¬å°è£…ä¸ºæ‰¹å¤„ç†BatchedState
            batched_state = self._prepare_batched_state([state])

            # 2. è·å–ä¼°å€¼ (Value)
            value = self.critic(batched_state).item()

            # 3. å¦‚æœæ²¡æœ‰è¾¹ç•ŒèŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›
            if batched_state.boundary_nodes.numel() == 0:
                return None, 0.0, value

            # --- v3.0 ä¸¤é˜¶æ®µå†³ç­–æµç¨‹ ---
            # 4. è·å–ç­–ç•¥å‘é‡
            node_logits, strategy_vectors = self.actor(batched_state)

            # 5. é˜¶æ®µä¸€ï¼šèŠ‚ç‚¹è¿‡æ»¤ä¸æ©ç 
            # ç¡®å®šæ¯ä¸ªè¾¹ç•ŒèŠ‚ç‚¹æ˜¯å¦æœ‰ä»»ä½•æœ‰æ•ˆçš„ç§»åŠ¨
            valid_node_mask_list = [
                bool(env.get_connectivity_safe_partitions(node_id.item()))
                for node_id in batched_state.boundary_nodes
            ]
            valid_node_mask = torch.tensor(valid_node_mask_list, device=self.device, dtype=torch.bool)
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•èŠ‚ç‚¹æœ‰æœ‰æ•ˆç§»åŠ¨ï¼Œåˆ™æå‰é€€å‡º
            if not valid_node_mask.any():
                return None, 0.0, value

            # åº”ç”¨æ©ç ï¼Œå±è”½æ‰æ²¡æœ‰æœ‰æ•ˆç§»åŠ¨çš„èŠ‚ç‚¹
            masked_node_logits = node_logits.masked_fill(~valid_node_mask, -1e9)

            # 6. é˜¶æ®µäºŒï¼šå®‰å…¨é‡‡æ ·
            # ä»æœ‰æ•ˆèŠ‚ç‚¹ä¸­é‡‡æ ·æˆ–è´ªå¿ƒé€‰æ‹©
            if training:
                node_probs = F.softmax(masked_node_logits, dim=0)
                node_dist = torch.distributions.Categorical(probs=node_probs)
                node_action_idx = node_dist.sample() # boundary_nodes ä¸­çš„ç´¢å¼•
            else: # è´ªå¿ƒæ¨¡å¼
                node_action_idx = torch.argmax(masked_node_logits)

            selected_node_global_id = batched_state.boundary_nodes[node_action_idx].item()
            available_partitions = env.get_connectivity_safe_partitions(selected_node_global_id)

            # 7. è·å–é€‰å®šèŠ‚ç‚¹çš„ç­–ç•¥å‘é‡
            strategy_vector = strategy_vectors[node_action_idx]

            # 8. ç¼–ç å¯ç”¨åˆ†åŒº
            partition_features = env.get_partition_features(available_partitions)
            partition_embeddings = self.partition_encoder(partition_features)

            # 9. è®¡ç®—ç›¸ä¼¼åº¦å¹¶é‡‡æ ·åˆ†åŒº
            similarities = self.partition_encoder.compute_similarity(strategy_vector, partition_embeddings)
            
            if training:
                partition_probs = F.softmax(similarities, dim=0)
                partition_dist = torch.distributions.Categorical(probs=partition_probs)
                selected_partition_local_idx = partition_dist.sample()
                
                # è®¡ç®—ç»„åˆå¯¹æ•°æ¦‚ç‡
                log_prob_node = node_dist.log_prob(node_action_idx)
                log_prob_partition = partition_dist.log_prob(selected_partition_local_idx)
                log_prob = (log_prob_node + log_prob_partition).item()
            else: # è´ªå¿ƒæ¨¡å¼
                selected_partition_local_idx = torch.argmax(similarities)
                log_prob = 0.0

            # 10. è½¬æ¢å›åˆ†åŒºID
            selected_partition_id = available_partitions[selected_partition_local_idx.item()]
            action = (selected_node_global_id, selected_partition_id)
            
        return action, log_prob, value
        
    def store_experience(self, 
                         state: Dict[str, torch.Tensor], 
                         action: Tuple[int, int], 
                         reward: float, 
                         log_prob: float, 
                         value: float, 
                         done: bool):
        """
        åœ¨ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­å­˜å‚¨ä¸€ä¸ªæ—¶é—´æ­¥çš„ç»éªŒã€‚

        Args:
            state (Dict[str, torch.Tensor]): å½“å‰çŠ¶æ€çš„è§‚å¯Ÿå­—å…¸ã€‚
            action (Tuple[int, int]): æ‰§è¡Œçš„åŠ¨ä½œã€‚
            reward (float): ä»ç¯å¢ƒä¸­è·å¾—çš„å¥–åŠ±ã€‚
            log_prob (float): æ‰§è¡ŒåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ã€‚
            value (float): è¯„åˆ¤ç½‘ç»œå¯¹å½“å‰çŠ¶æ€çš„ä»·å€¼ä¼°è®¡ã€‚
            done (bool): å›åˆæ˜¯å¦ç»“æŸçš„æ ‡å¿—ã€‚
        """
        self.memory.store(state, action, reward, log_prob, value, done)
        
    def update(self, **kwargs) -> Dict[str, float]:
        """
        ä½¿ç”¨PPOæ›´æ–°ç½‘ç»œï¼ˆv3.0é‡æ„ï¼Œéœ€è¦envå®ä¾‹ï¼‰

        Returns:
            è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        if len(self.memory) == 0:
            return {}
            
        # v3.0: å¿…é¡»å°† env å®ä¾‹ä¼ å…¥ _ppo_epoch
        if 'env' not in kwargs:
             raise ValueError("PPOAgent.update() now requires 'env' keyword argument for v3.0 architecture.")
        env = kwargs['env']

        # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šç›´æ¥è·å–å¼ é‡ï¼Œé¿å…CPU-GPUä¼ è¾“
        states, actions, rewards_tensor, old_log_probs_tensor, old_values_tensor, dones_tensor = self.memory.get_batch_tensors()
        
        # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
        advantages, returns = self._compute_advantages(rewards_tensor, old_values_tensor, dones_tensor)

        # PPOæ›´æ–°
        agg_stats = {
            'actor_loss': 0.0,
            'critic_loss': 0.0,
            'entropy': 0.0,
            'grad_norm': 0.0,
        }
        total_samples = 0

        for _ in range(self.k_epochs):
            epoch_stats, batch_size = self._ppo_epoch(
                states, actions, old_log_probs_tensor, advantages, returns, env=env)

            for key in agg_stats:
                if key in epoch_stats:
                    agg_stats[key] += epoch_stats[key] * batch_size

            total_samples += batch_size

        # æ ·æœ¬å¹³å‡
        for key in agg_stats:
            agg_stats[key] /= max(total_samples, 1)

        # æ›´æ–°è®­ç»ƒç»Ÿè®¡
        self.training_stats['actor_loss'].append(agg_stats['actor_loss'])
        self.training_stats['critic_loss'].append(agg_stats['critic_loss'])
        self.training_stats['entropy'].append(agg_stats['entropy'])

        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.actor_scheduler:
            self.actor_scheduler.step()
        if self.critic_scheduler:
            self.critic_scheduler.step()

        # æ¸…ç©ºå†…å­˜
        self.memory.clear()

        return agg_stats
        
    def _compute_advantages(self, 
                            rewards: torch.Tensor, 
                            values: torch.Tensor, 
                            dones: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ä½¿ç”¨å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ (GAE) è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥ã€‚

        Args:
            rewards (torch.Tensor): ä¸€ä¸ªå›åˆçš„å¥–åŠ±åºåˆ—ã€‚
            values (torch.Tensor): ä¸€ä¸ªå›åˆçš„çŠ¶æ€ä»·å€¼åºåˆ—ã€‚
            dones (torch.Tensor): ä¸€ä¸ªå›åˆçš„ç»“æŸæ ‡å¿—åºåˆ—ã€‚

        Returns:
            Tuple[torch.Tensor, torch.Tensor]:
                - advantages: æ ‡å‡†åŒ–åçš„ä¼˜åŠ¿å‡½æ•°åºåˆ—ã€‚
                - returns: GAEè®¡ç®—å‡ºçš„å›æŠ¥åºåˆ—ã€‚
        """
        advantages = torch.zeros_like(rewards)
        returns = torch.zeros_like(rewards)

        gae = 0
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]

            delta = rewards[t] + self.gamma * next_value * (~dones[t]).float() - values[t]
            gae = delta + self.gamma * 0.95 * (~dones[t]).float() * gae
            advantages[t] = gae
            returns[t] = advantages[t] + values[t]

        # å®‰å…¨çš„å›æŠ¥å¥åº·æ£€æŸ¥
        returns = torch.nan_to_num(returns, nan=0.0, posinf=0.0, neginf=0.0)

        # å®‰å…¨æ ‡å‡†åŒ–ä¼˜åŠ¿
        def safe_standardize(t, eps=1e-6):
            mean = t.mean()
            std = t.std()
            if torch.isnan(std) or std < eps:
                return t - mean
            return (t - mean) / (std + eps)

        advantages = safe_standardize(advantages)
        advantages = torch.nan_to_num(advantages, nan=0.0, posinf=0.0, neginf=0.0)

        return advantages, returns

    def _compute_grad_norm(self, model: nn.Module) -> float:
        """
        è®¡ç®—æ¨¡å‹çš„æ¢¯åº¦èŒƒæ•°

        Args:
            model: è¦è®¡ç®—æ¢¯åº¦èŒƒæ•°çš„æ¨¡å‹

        Returns:
            æ¢¯åº¦çš„L2èŒƒæ•°
        """
        total_norm = 0.0
        param_count = 0
        for param in model.parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1

        if param_count == 0:
            return 0.0
        return (total_norm ** 0.5)

    def _compute_param_norm(self, model: nn.Module) -> float:
        """
        è®¡ç®—æ¨¡å‹çš„å‚æ•°èŒƒæ•°

        Args:
            model: è¦è®¡ç®—å‚æ•°èŒƒæ•°çš„æ¨¡å‹

        Returns:
            å‚æ•°çš„L2èŒƒæ•°
        """
        total_norm = 0.0
        for param in model.parameters():
            param_norm = param.data.norm(2)
            total_norm += param_norm.item() ** 2
        return (total_norm ** 0.5)

    def set_tensorboard_writer(self, writer):
        """
        è®¾ç½®TensorBoard writerç”¨äºæ¢¯åº¦ç›‘æ§

        Args:
            writer: TensorBoard SummaryWriterå®ä¾‹
        """
        self.writer = writer
        
    def _ppo_epoch(self,
                   states: List[Dict[str, torch.Tensor]],
                   actions: List[Tuple[int, int]],
                   old_log_probs: torch.Tensor,
                   advantages: torch.Tensor,
                   returns: torch.Tensor,
                   env: Any) -> Tuple[Dict[str, float], int]:
        """
        æ‰§è¡Œå•ä¸ªPPOæ›´æ–°å‘¨æœŸ (v3.0 åŒå¡”æ¶æ„é‡æ„)ã€‚
        """
        batch_size = len(states)
        if batch_size == 0:
            return {'actor_loss': 0.0, 'critic_loss': 0.0, 'entropy': 0.0}, 0

        # ---- 1. æ•°æ®æ‰¹é‡åŒ– å’Œ ä»·å€¼ä¼°è®¡ ----
        batched_state = self._prepare_batched_state(states)
        values = self.critic(batched_state).squeeze()

        # ---- 2. æ ¸å¿ƒï¼šä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªåŠ¨ä½œé‡æ–°è®¡ç®— log_prob å’Œç†µ ----
        node_logits, strategy_vectors = self.actor(batched_state)
        boundary_batch_idx = batched_state.node_batch_idx[batched_state.boundary_nodes]
        
        # âš¡ ç§»é™¤æ·±æ‹·è´ä»¥é¿å…ä¸å¯å“ˆå¸Œé”®é”™è¯¯å¹¶æå‡æ€§èƒ½
        orig_partition = env.state_manager.current_partition.clone()
        try:
            log_probs_list = []
            entropy_list = []

            # è¿™æ˜¯ä¸€ä¸ªæ€§èƒ½ç“¶é¢ˆï¼Œç†æƒ³æƒ…å†µä¸‹åº”æ‰¹é‡åŒ–ã€‚ä½†ä¸ºäº†é€»è¾‘æ¸…æ™°ï¼Œå…ˆä½¿ç”¨å¾ªç¯å®ç°ã€‚
            for i in range(batch_size):
                env.state_manager.current_partition = states[i]['current_partition']
                
                sample_mask = (boundary_batch_idx == i)
                sample_boundary_nodes = batched_state.boundary_nodes[sample_mask]
                
                if sample_boundary_nodes.numel() == 0: continue

                # a) èŠ‚ç‚¹è¿‡æ»¤ä¸æ©ç 
                valid_node_mask = torch.tensor([
                    bool(env.get_connectivity_safe_partitions(node_id.item()))
                    for node_id in sample_boundary_nodes
                ], device=self.device, dtype=torch.bool)
                
                if not valid_node_mask.any(): continue

                masked_node_logits = node_logits[sample_mask].masked_fill(~valid_node_mask, -1e9)
                node_probs = F.softmax(masked_node_logits, dim=0)
                node_dist = torch.distributions.Categorical(probs=node_probs)
                
                # b) è·å–åŠ¨ä½œ
                node_id, partition_id = actions[i]
                
                # c) ç¼–ç åˆ†åŒº
                available_partitions = env.get_connectivity_safe_partitions(node_id)
                if not available_partitions: continue

                partition_features = env.get_partition_features(available_partitions)
                partition_embeddings = self.partition_encoder(partition_features)
                
                # d) è®¡ç®—åˆ†åŒºæ¦‚ç‡
                node_action_idx_in_sample = (sample_boundary_nodes == node_id).nonzero(as_tuple=True)[0]
                strategy_vector = strategy_vectors[sample_mask][node_action_idx_in_sample].squeeze(0)
                similarities = self.partition_encoder.compute_similarity(strategy_vector, partition_embeddings)
                partition_probs = F.softmax(similarities, dim=0)
                partition_dist = torch.distributions.Categorical(probs=partition_probs)
                
                # e) è®¡ç®—LogProbå’Œç†µ
                try:
                    # ğŸ”§ DEBUG: æ‰“å°å…³é”®æ•°æ®ç±»å‹
                    print(f"\n=== DEBUG Episode {i} ===")
                    print(f"actions[i] = {actions[i]}")
                    print(f"type(actions[i]) = {type(actions[i])}")
                    if isinstance(actions[i], (list, tuple)) and len(actions[i]) >= 2:
                        print(f"partition_id = {actions[i][1]}")
                        print(f"type(partition_id) = {type(actions[i][1])}")
                    print(f"available_partitions = {available_partitions}")
                    print(f"type(available_partitions) = {type(available_partitions)}")
                    if available_partitions:
                        print(f"available_partitions[0] type = {type(available_partitions[0])}")
                    print("=== END DEBUG ===\n")
                    
                    part_action_idx_in_list = available_partitions.index(partition_id)
                    log_prob = node_dist.log_prob(node_action_idx_in_sample) + \
                               partition_dist.log_prob(torch.tensor(part_action_idx_in_list, device=self.device))
                    log_probs_list.append(log_prob)
                    entropy_list.append(node_dist.entropy() + partition_dist.entropy())
                except ValueError:
                    continue

        # æ¢å¤ç¯å¢ƒåˆ†åŒºæ˜ å°„
        finally:
            env.state_manager.current_partition = orig_partition

        if not log_probs_list:
            return {'actor_loss': 0.0, 'critic_loss': 0.0, 'entropy': 0.0}, 0
            
        new_log_probs = torch.stack(log_probs_list)
        entropy = torch.stack(entropy_list).mean()

        # ---- 3. PPO æŸå¤±è®¡ç®— ----
        with torch.amp.autocast(device_type=self.device.type, dtype=torch.bfloat16):
            ratio = torch.exp(new_log_probs - old_log_probs)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages
            actor_loss = -torch.min(surr1, surr2).mean()
            critic_loss = F.mse_loss(values, returns)
            total_loss = (actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy).float()

        # ---- 4. åå‘ä¼ æ’­ & æ›´æ–° ----
        self.actor_optimizer.zero_grad(set_to_none=True)
        self.critic_optimizer.zero_grad(set_to_none=True)

        self.scaler.scale(total_loss).backward()

        total_grad_norm = torch.tensor(0.0)
        if self.max_grad_norm is not None:
            self.scaler.unscale_(self.actor_optimizer)
            self.scaler.unscale_(self.critic_optimizer)
            all_params = list(self.actor.parameters()) + list(self.partition_encoder.parameters()) + list(self.critic.parameters())
            total_grad_norm = torch.nn.utils.clip_grad_norm_(all_params, self.max_grad_norm)

        self.scaler.step(self.actor_optimizer)
        self.scaler.step(self.critic_optimizer)
        self.scaler.update()

        return ({
            'actor_loss': actor_loss.item(),
            'critic_loss': critic_loss.item(),
            'entropy': entropy.item(),
            'grad_norm': total_grad_norm.item(),
        }, batch_size)

    def enable_gradient_norm_debug(self, enable: bool = True):
        """å¯ç”¨/ç¦ç”¨æ¢¯åº¦èŒƒæ•°è°ƒè¯•æ‰“å°"""
        self._debug_grad_norm = enable
        
    def save(self, filepath: str):
        """ä¿å­˜æ™ºèƒ½ä½“çŠ¶æ€"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
        }, filepath)
        
    def load(self, filepath: str):
        """åŠ è½½æ™ºèƒ½ä½“çŠ¶æ€"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])

    def store_experience(self, 
                         state: Dict[str, torch.Tensor], 
                         action: Tuple[int, int], 
                         reward: float, 
                         log_prob: float, 
                         value: float, 
                         done: bool):
        """
        åœ¨ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­å­˜å‚¨ä¸€ä¸ªæ—¶é—´æ­¥çš„ç»éªŒã€‚

        Args:
            state (Dict[str, torch.Tensor]): å½“å‰çŠ¶æ€çš„è§‚å¯Ÿå­—å…¸ã€‚
            action (Tuple[int, int]): æ‰§è¡Œçš„åŠ¨ä½œã€‚
            reward (float): ä»ç¯å¢ƒä¸­è·å¾—çš„å¥–åŠ±ã€‚
            log_prob (float): æ‰§è¡ŒåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ã€‚
            value (float): è¯„åˆ¤ç½‘ç»œå¯¹å½“å‰çŠ¶æ€çš„ä»·å€¼ä¼°è®¡ã€‚
            done (bool): å›åˆæ˜¯å¦ç»“æŸçš„æ ‡å¿—ã€‚
        """
        self.memory.store(state, action, reward, log_prob, value, done)

    def update_learning_rate(self, factor: float):
        """åŠ¨æ€æ›´æ–°å­¦ä¹ ç‡"""
        for param_group in self.actor_optimizer.param_groups:
            param_group['lr'] *= factor
        for param_group in self.critic_optimizer.param_groups:
            param_group['lr'] *= factor

    def get_current_learning_rates(self) -> Dict[str, float]:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        try:
            actor_lr = self.actor_optimizer.param_groups[0]['lr']
            critic_lr = self.critic_optimizer.param_groups[0]['lr']
            return {'actor_lr': actor_lr, 'critic_lr': critic_lr}
        except:
            return {'actor_lr': 0.0, 'critic_lr': 0.0}

    def _prepare_batched_state(self, states_list: List[Dict[str, torch.Tensor]]) -> "BatchedState":
        """
        å°†ä¸€ç³»åˆ—ç‹¬ç«‹çš„çŠ¶æ€å­—å…¸æ‹¼æ¥æˆä¸€ä¸ªæ‰¹å¤„ç†çš„BatchedStateå¯¹è±¡ã€‚

        è¿™ä¸ªå‡½æ•°æ˜¯æ€§èƒ½ä¼˜åŒ–çš„æ ¸å¿ƒï¼Œå®ƒé€šè¿‡å°†å¤šä¸ªå›¾æ ·æœ¬çš„æ•°æ®èšåˆï¼Œ
        ä½¿å¾—ç¥ç»ç½‘ç»œå¯ä»¥ä¸€æ¬¡æ€§åœ¨GPUä¸Šå¤„ç†æ•´ä¸ªæ‰¹æ¬¡ï¼Œè€Œä¸æ˜¯é€ä¸ªå¤„ç†ã€‚

        Args:
            states_list (List[Dict[str, torch.Tensor]]): 
                ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªä»£è¡¨å•ä¸€æ ·æœ¬çŠ¶æ€çš„å­—å…¸ã€‚

        Returns:
            BatchedState: åŒ…å«äº†æ‰€æœ‰è¾“å…¥æ ·æœ¬æ•°æ®çš„èšåˆæ‰¹å¤„ç†å¯¹è±¡ã€‚
        """
        # ç´¯ç§¯åˆ—è¡¨
        node_emb_list: List[torch.Tensor] = []
        region_emb_list: List[torch.Tensor] = []
        action_mask_list: List[torch.Tensor] = []
        node_batch_idx_list: List[torch.Tensor] = []
        region_batch_idx_list: List[torch.Tensor] = []
        boundary_nodes_global: List[torch.Tensor] = []

        node_offset = 0
        region_offset = 0

        for batch_id, state in enumerate(states_list):
            node_embeddings = state['node_embeddings'].detach()
            region_embeddings = state['region_embeddings'].detach()
            boundary_nodes = state['boundary_nodes']
            current_partition = state['current_partition']

            num_nodes = node_embeddings.size(0)
            num_regions = region_embeddings.size(0)

            # ==== åŠ¨ä½œæ©ç æ„é€  ====
            if 'action_mask' in state:
                action_mask = state['action_mask']
            else:
                # å¤åˆ¶è‡ªæ—§å®ç°çš„ç®€åŒ–é€»è¾‘
                action_mask = torch.zeros(
                    num_nodes, self.num_partitions, dtype=torch.bool, device=self.device
                )
                for node_idx in boundary_nodes:
                    current_node_partition = current_partition[node_idx].item()
                    for p in range(self.num_partitions):
                        if p + 1 != current_node_partition:
                            action_mask[node_idx, p] = True

            # ==== ç´¯ç§¯å¼ é‡ ====
            node_emb_list.append(node_embeddings)
            region_emb_list.append(region_embeddings)
            action_mask_list.append(action_mask)

            # batch idx
            node_batch_idx_list.append(torch.full((num_nodes,), batch_id, dtype=torch.long, device=self.device))
            region_batch_idx_list.append(torch.full((num_regions,), batch_id, dtype=torch.long, device=self.device))

            # è¾¹ç•ŒèŠ‚ç‚¹å…¨å±€ç´¢å¼•
            if boundary_nodes.numel() > 0:
                boundary_nodes_global.append(boundary_nodes + node_offset)

            node_offset += num_nodes
            region_offset += num_regions

        # ===== æ‹¼æ¥ =====
        node_embeddings_cat = torch.cat(node_emb_list, dim=0)
        region_embeddings_cat = torch.cat(region_emb_list, dim=0)
        action_mask_cat = torch.cat(action_mask_list, dim=0)
        node_batch_idx = torch.cat(node_batch_idx_list, dim=0)
        region_batch_idx = torch.cat(region_batch_idx_list, dim=0)
        if boundary_nodes_global:
            boundary_nodes_cat = torch.cat(boundary_nodes_global, dim=0)
        else:
            boundary_nodes_cat = torch.zeros(0, dtype=torch.long, device=self.device)

        return BatchedState(
            node_embeddings=node_embeddings_cat,
            region_embeddings=region_embeddings_cat,
            boundary_nodes=boundary_nodes_cat,
            node_batch_idx=node_batch_idx,
            region_batch_idx=region_batch_idx,
            action_mask=action_mask_cat,
        )
