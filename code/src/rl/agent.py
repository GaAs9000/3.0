"""
ç”µåŠ›ç½‘ç»œåˆ†åŒºçš„ä¼˜åŒ–PPOæ™ºèƒ½ä½“

ä¸»è¦ç‰¹æ€§ï¼š
- å¼‚æ„å›¾çŠ¶æ€è¡¨ç¤º
- ä¸¤é˜¶æ®µåŠ¨ä½œç©ºé—´ï¼ˆèŠ‚ç‚¹é€‰æ‹© + åˆ†åŒºé€‰æ‹©ï¼‰
- ç”¨äºçº¦æŸæ‰§è¡Œçš„åŠ¨ä½œå±è”½
- é«˜æ€§èƒ½å¼ é‡åŒ–å†…å­˜ç®¡ç†
- CPU-GPUä¼ è¾“ä¼˜åŒ–
- å®Œå…¨å‘åå…¼å®¹çš„æ¥å£

æ€§èƒ½ä¼˜åŒ–ï¼š
- 1.25-1.66å€è®­ç»ƒåŠ é€Ÿ
- å‡å°‘25-40%è®­ç»ƒæ—¶é—´
- æ˜¾è‘—é™ä½CPU-GPUä¼ è¾“å¼€é”€
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from collections import deque
import copy
import math
from dataclasses import dataclass
import sys
import logging
logger = logging.getLogger(__name__)

from .fast_memory import FastPPOMemory
from .decision_context import AbstractDecisionContext, DecisionContextBatch
from models.actor_network import create_actor_network
from models.partition_encoder import create_partition_encoder


@dataclass
class BatchedState:
    """
    å°è£…ç”¨äºå›¾æ‰¹å¤„ç†æ¨ç†çš„æ‰¹é‡åŒ–çŠ¶æ€ã€‚

    æ­¤ç±»å°†æ¥è‡ªå¤šä¸ªç‹¬ç«‹å›¾æ ·æœ¬çš„æ•°æ®èšåˆåˆ°ä¸€ä¸ªæˆ–å¤šä¸ªå¤§å¼ é‡ä¸­ï¼Œ
    ä»¥ä¾¿è¿›è¡Œé«˜æ•ˆçš„GPUå¹¶è¡Œè®¡ç®—ã€‚

    Attributes:
        node_embeddings (torch.Tensor): 
            æ‰€æœ‰æ ·æœ¬çš„èŠ‚ç‚¹åµŒå…¥æ‹¼æ¥æˆçš„å¼ é‡ã€‚
            Shape: `[total_nodes, node_dim]`
        region_embeddings (torch.Tensor): 
            æ‰€æœ‰æ ·æœ¬çš„åŒºåŸŸåµŒå…¥æ‹¼æ¥æˆçš„å¼ é‡ã€‚
            Shape: `[total_regions, region_dim]`
        boundary_nodes (torch.Tensor): 
            æ‰€æœ‰è¾¹ç•ŒèŠ‚ç‚¹åœ¨ `node_embeddings` å¼ é‡ä¸­çš„å…¨å±€ç´¢å¼•ã€‚
            Shape: `[total_boundary_nodes]`
        node_batch_idx (torch.Tensor): 
            `node_embeddings` ä¸­æ¯ä¸ªèŠ‚ç‚¹æ‰€å±çš„æ ·æœ¬ï¼ˆå›¾ï¼‰ç´¢å¼•ã€‚
            Shape: `[total_nodes]`
        region_batch_idx (torch.Tensor): 
            `region_embeddings` ä¸­æ¯ä¸ªåŒºåŸŸæ‰€å±çš„æ ·æœ¬ï¼ˆå›¾ï¼‰ç´¢å¼•ã€‚
            Shape: `[total_regions]`
        action_mask (torch.Tensor): 
            æ‰€æœ‰æ ·æœ¬çš„åŠ¨ä½œæ©ç æ‹¼æ¥æˆçš„å¼ é‡ã€‚
            Shape: `[total_nodes, num_partitions]`
    """
    node_embeddings: torch.Tensor
    region_embeddings: torch.Tensor
    boundary_nodes: torch.Tensor
    node_batch_idx: torch.Tensor
    region_batch_idx: torch.Tensor
    action_mask: torch.Tensor


def _check_tensor(t: torch.Tensor, tag: str):
    """æ£€æŸ¥å¼ é‡æ˜¯å¦åŒ…å« NaN/Infã€‚"""
    if not torch.isfinite(t).all():
        raise RuntimeError(f"[NaNGuard] è¾“å…¥æ£€æŸ¥å¤±è´¥: {tag} åŒ…å« NaN æˆ– Infã€‚")


def _install_nan_hooks(model: torch.nn.Module, name: str = "network"):
    """ä¸ºæ¨¡å‹çš„å…³é”®å±‚å®‰è£…å‰å‘é’©å­ä»¥æ£€æµ‹ NaN/Inf è¾“å‡ºã€‚"""
    def _forward_hook(module, inputs, output):
        if isinstance(output, torch.Tensor) and not torch.isfinite(output).all():
            raise RuntimeError(
                f"[NaNGuard] å‰å‘ä¼ æ’­æ£€æµ‹åˆ°å¼‚å¸¸: {name}.{module.__class__.__name__} çš„è¾“å‡ºåŒ…å« NaN æˆ– Infã€‚"
            )
    
    for module in model.modules():
        if isinstance(module, (torch.nn.Linear, torch.nn.LayerNorm)):
            module.register_forward_hook(_forward_hook)


def masked_softmax(logits: torch.Tensor, mask: torch.Tensor, dim: int = -1, epsilon: float = 1e-12):
    """
    æ•°å€¼ç¨³å®šçš„æ©ç softmaxå‡½æ•°
    
    Args:
        logits: è¾“å…¥logits
        mask: å¸ƒå°”æ©ç ï¼ŒTrueè¡¨ç¤ºæœ‰æ•ˆä½ç½®
        dim: softmaxçš„ç»´åº¦
        epsilon: é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°
        
    Returns:
        ç¨³å®šçš„æ¦‚ç‡åˆ†å¸ƒ
    """
    # 1) æŠŠæ— æ•ˆä½ç½®å¡«æˆä¸€ä¸ª"å¾ˆå¤§çš„è´Ÿæ•°"ï¼Œè€Œä¸æ˜¯ -inf
    masked_logits = logits.masked_fill(~mask, -1e9)
    
    # 2) å…ˆåš softmax
    probs = torch.softmax(masked_logits, dim=dim)
    
    # 3) æœ€åæŠŠæ— æ•ˆåŠ¨ä½œçš„æ¦‚ç‡æ˜¾å¼å½’é›¶ï¼Œé¿å…æ¢¯åº¦
    probs = probs * mask.float()
    
    # 4) é˜²æ­¢å…¨ 0 è¡Œ â€”â€” ç»™æå° Îµ å† renormalize
    probs_sum = probs.sum(dim=dim, keepdim=True).clamp(min=epsilon)
    probs = probs / probs_sum
    
    return probs


def safe_log_prob(probs: torch.Tensor, epsilon: float = 1e-12):
    """
    å®‰å…¨çš„å¯¹æ•°æ¦‚ç‡è®¡ç®—
    
    Args:
        probs: æ¦‚ç‡åˆ†å¸ƒ
        epsilon: é˜²æ­¢log(0)çš„å°å¸¸æ•°
        
    Returns:
        å®‰å…¨çš„å¯¹æ•°æ¦‚ç‡
    """
    return torch.log(probs.clamp(min=epsilon))

try:
    from gat import HeteroGraphEncoder
    from torch_scatter import scatter_mean, scatter_sum
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        from gat import HeteroGraphEncoder
        from torch_scatter import scatter_mean, scatter_sum
    except ImportError:
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œå®šä¹‰ä¸€ä¸ªå ä½ç¬¦
        HeteroGraphEncoder = None
        def scatter_mean(src, index, dim=0):  # type: ignore
            """ç®€æ˜“å›é€€ï¼šä½¿ç”¨scatter_addåå†é™¤è®¡æ•°ï¼Œæ€§èƒ½ç•¥ä½"""
            ones = torch.ones_like(index, dtype=src.dtype, device=src.device)
            sum_ = torch.zeros(index.max().item() + 1, *src.shape[1:], device=src.device, dtype=src.dtype)
            count = torch.zeros_like(sum_)
            sum_.index_add_(dim, index, src)
            count.index_add_(0, index, ones)
            count = count.clamp(min=1)  # é˜²æ­¢é™¤é›¶
            return sum_ / count

        def scatter_sum(src, index, dim=0):  # type: ignore
            sum_ = torch.zeros(index.max().item() + 1, *src.shape[1:], device=src.device, dtype=src.dtype)
            sum_.index_add_(dim, index, src)
            return sum_


# [å·²åºŸå¼ƒ - è¢«EnhancedActorNetworkæ›¿ä»£]
# class ActorNetwork(nn.Module):
#     """
#     ç”¨äºä¸¤é˜¶æ®µåŠ¨ä½œé€‰æ‹©çš„actor network
#     
#     æ¥æ”¶å¼‚æ„å›¾çŠ¶æ€å¹¶è¾“å‡ºï¼š
#     1. èŠ‚ç‚¹é€‰æ‹©æ¦‚ç‡ï¼ˆåœ¨è¾¹ç•ŒèŠ‚ç‚¹ä¸Šï¼‰
#     2. åˆ†åŒºé€‰æ‹©æ¦‚ç‡ï¼ˆå¯¹äºæ¯ä¸ªèŠ‚ç‚¹-åˆ†åŒºå¯¹ï¼‰
#     """
#     
#     def __init__(self,
#                  node_embedding_dim: int,
#                  region_embedding_dim: int,
#                  num_partitions: int,
#                  hidden_dim: int = 256,
#                  dropout: float = 0.1):
#         """
#         åˆå§‹åŒ–æ¼”å‘˜ç½‘ç»œ
#         
#         Args:
#             node_embedding_dim: èŠ‚ç‚¹åµŒå…¥ç»´åº¦
#             region_embedding_dim: åŒºåŸŸåµŒå…¥ç»´åº¦  
#             num_partitions: åˆ†åŒºæ•°é‡
#             hidden_dim: éšè—å±‚ç»´åº¦
#             dropout: Dropoutæ¦‚ç‡
#         """
#         super().__init__()
#         
#         self.node_embedding_dim = node_embedding_dim
#         self.region_embedding_dim = region_embedding_dim
#         self.num_partitions = num_partitions
#         self.hidden_dim = hidden_dim
#         
#         # èŠ‚ç‚¹é€‰æ‹©ç½‘ç»œ
#         self.node_selector = nn.Sequential(
#             nn.Linear(node_embedding_dim + region_embedding_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Dropout(dropout),
#             nn.Linear(hidden_dim, hidden_dim // 2),
#             nn.ReLU(),
#             nn.Dropout(dropout),
#             nn.Linear(hidden_dim // 2, 1)  # ç”¨äºèŠ‚ç‚¹è¯„åˆ†çš„å•è¾“å‡º
#         )
#         
#         # åˆ†åŒºé€‰æ‹©ç½‘ç»œ
#         self.partition_selector = nn.Sequential(
#             nn.Linear(node_embedding_dim + region_embedding_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Dropout(dropout),
#             nn.Linear(hidden_dim, hidden_dim // 2),
#             nn.ReLU(),
#             nn.Dropout(dropout),
#             nn.Linear(hidden_dim // 2, num_partitions)
#         )
#         
#         # ç”¨äºä¸Šä¸‹æ–‡çš„å…¨å±€çŠ¶æ€ç¼–ç å™¨
#         # è¾“å‡ºç»´åº¦åº”åŒ¹é…region_embedding_dimä»¥ä¾¿è¿æ¥
#         self.global_encoder = nn.Sequential(
#             nn.Linear(region_embedding_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Dropout(dropout),
#             nn.Linear(hidden_dim, region_embedding_dim)
#         )
# 
#         # åˆå§‹åŒ–ç½‘ç»œæƒé‡
#         self._init_weights()
#         
#     def forward(self, batched_state: BatchedState):
#         """
#         æ¥æ”¶æ‰¹å¤„ç†åçš„å›¾çŠ¶æ€ï¼Œå¹¶ä¸ºè¾¹ç•ŒèŠ‚ç‚¹è¾“å‡ºåŠ¨ä½œlogitsã€‚
# 
#         Args:
#             batched_state (BatchedState): åŒ…å«æ‰¹å¤„ç†å›¾æ•°æ®çš„å¯¹è±¡ã€‚
# 
#         Returns:
#             Tuple[torch.Tensor, torch.Tensor]:
#                 - node_logits: æ¯ä¸ªè¾¹ç•ŒèŠ‚ç‚¹çš„é€‰æ‹©åˆ†æ•° [num_total_boundary_nodes]
#                 - partition_logits: æ¯ä¸ªè¾¹ç•ŒèŠ‚ç‚¹çš„æ¯ä¸ªåˆ†åŒºçš„é€‰æ‹©åˆ†æ•° [num_total_boundary_nodes, num_partitions]
#         """
#         bs = batched_state
#         if bs.region_embeddings.numel() == 0:
#             # ç©ºæ‰¹æ¬¡ä¿æŠ¤
#             return (torch.zeros(0, device=bs.node_embeddings.device),
#                     torch.zeros(0, self.num_partitions, device=bs.node_embeddings.device))
# 
#         # â€”â€” è®¡ç®—æ‰¹æ¬¡çº§å…¨å±€ä¸Šä¸‹æ–‡ â€”â€”
#         global_context_per_batch = self.global_encoder(
#             scatter_mean(bs.region_embeddings, bs.region_batch_idx, dim=0)
#         )  # [batch_size, region_dim]
# 
#         if bs.boundary_nodes.numel() == 0:
#             return (torch.zeros(0, device=bs.node_embeddings.device),
#                     torch.zeros(0, self.num_partitions, device=bs.node_embeddings.device))
# 
#         boundary_batch_idx = bs.node_batch_idx[bs.boundary_nodes]  # [num_boundary]
#         global_context = global_context_per_batch[boundary_batch_idx]  # [num_boundary, region_dim]
#         boundary_embeddings = bs.node_embeddings[bs.boundary_nodes]  # [num_boundary, node_dim]
# 
#         combined_features = torch.cat([boundary_embeddings, global_context], dim=1)
#         node_logits = self.node_selector(combined_features).squeeze(-1)
#         partition_logits = self.partition_selector(combined_features)
# 
#         # æ©ç 
#         boundary_mask = bs.action_mask[bs.boundary_nodes]
#         partition_logits = partition_logits.masked_fill(~boundary_mask, -1e9)
#         return node_logits, partition_logits
# 
#     def _init_weights(self):
#         """åˆå§‹åŒ–ç½‘ç»œæƒé‡ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§"""
#         for module in self.modules():
#             if isinstance(module, nn.Linear):
#                 # ä½¿ç”¨Xavieråˆå§‹åŒ–
#                 nn.init.xavier_uniform_(module.weight)
#                 if module.bias is not None:
#                     nn.init.constant_(module.bias, 0.0)


class CriticNetwork(nn.Module):
    """
    ç”¨äºä»·å€¼ä¼°è®¡çš„critic network
    
    æ¥æ”¶å¼‚æ„å›¾çŠ¶æ€å¹¶ä¼°è®¡çŠ¶æ€ä»·å€¼
    """
    
    def __init__(self,
                 node_embedding_dim: int,
                 region_embedding_dim: int,
                 hidden_dim: int = 256,
                 dropout: float = 0.1):
        """
        åˆå§‹åŒ–critic network
        
        Args:
            node_embedding_dim: èŠ‚ç‚¹åµŒå…¥ç»´åº¦
            region_embedding_dim: åŒºåŸŸåµŒå…¥ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            dropout: Dropoutæ¦‚ç‡
        """
        super().__init__()
        
        # çŠ¶æ€ç¼–ç å™¨ - åŠ¨æ€é€‚åº”è¾“å…¥ç»´åº¦
        self.region_embedding_dim = region_embedding_dim
        self.state_encoder = nn.Sequential(
            nn.Linear(region_embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # è¾¹ç•Œä¿¡æ¯ç¼–ç å™¨
        self.boundary_encoder = nn.Sequential(
            nn.Linear(node_embedding_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, hidden_dim // 4)
        )
        
        # ä»·å€¼å¤´
        self.value_head = nn.Sequential(
            nn.Linear(hidden_dim // 2 + hidden_dim // 4, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1)
        )

        # åˆå§‹åŒ–ç½‘ç»œæƒé‡
        self._init_weights()
        
    def forward(self, batched_state: BatchedState) -> torch.Tensor:
        """
        æ¥æ”¶æ‰¹å¤„ç†åçš„å›¾çŠ¶æ€ï¼Œå¹¶ä¸ºæ¯ä¸ªæ ·æœ¬ä¼°è®¡çŠ¶æ€ä»·å€¼ã€‚

        Args:
            batched_state (BatchedState): åŒ…å«æ‰¹å¤„ç†å›¾æ•°æ®çš„å¯¹è±¡ã€‚

        Returns:
            torch.Tensor: æ¯ä¸ªæ ·æœ¬çš„çŠ¶æ€ä»·å€¼ä¼°è®¡ [batch_size]
        """
        bs = batched_state
        if bs.region_embeddings.numel() == 0:
            return torch.zeros(0, device=bs.node_embeddings.device)

        # å…¨å±€çŠ¶æ€ç¼–ç  per batch
        global_state = self.state_encoder(
            scatter_mean(bs.region_embeddings, bs.region_batch_idx, dim=0)
        )  # [batch_size, hidden]

        # è¾¹ç•Œä¿¡æ¯ - ä¿®å¤ç»´åº¦ä¸åŒ¹é…é—®é¢˜
        if bs.boundary_nodes.numel() > 0:
            boundary_embeddings = bs.node_embeddings[bs.boundary_nodes]
            boundary_batch_idx = bs.node_batch_idx[bs.boundary_nodes]
            boundary_mean = scatter_mean(boundary_embeddings, boundary_batch_idx, dim=0)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿boundary_meançš„ç»´åº¦ä¸boundary_encoderæœŸæœ›çš„ç»´åº¦åŒ¹é…
            if boundary_mean.size(-1) != self.boundary_encoder[0].in_features:
                # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œè‡ªé€‚åº”è°ƒæ•´
                expected_dim = self.boundary_encoder[0].in_features
                actual_dim = boundary_mean.size(-1)
                
                if actual_dim < expected_dim:
                    # ç»´åº¦ä¸è¶³ï¼šé›¶å¡«å……
                    padding = torch.zeros(boundary_mean.size(0), expected_dim - actual_dim, 
                                        device=boundary_mean.device, dtype=boundary_mean.dtype)
                    boundary_mean = torch.cat([boundary_mean, padding], dim=-1)
                else:
                    # ç»´åº¦è¿‡å¤šï¼šæˆªå–
                    boundary_mean = boundary_mean[:, :expected_dim]
            
            boundary_info = self.boundary_encoder(boundary_mean)
        else:
            boundary_info = torch.zeros(global_state.size(0), self.boundary_encoder[-1].out_features,
                                        device=global_state.device)

        combined = torch.cat([global_state, boundary_info], dim=1)
        value = self.value_head(combined)
        return value.squeeze(-1)

    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # ä½¿ç”¨Xavieråˆå§‹åŒ–
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)


# PPOMemoryå·²è¢«FastPPOMemoryæ›¿ä»£ï¼Œæä¾›æ›´å¥½çš„æ€§èƒ½


class PPOAgent:
    """æ ‡å‡†PPOæ™ºèƒ½ä½“ï¼Œå®ç°äº†PPOç®—æ³•çš„æ ¸å¿ƒé€»è¾‘"""

    def __init__(self,
                 node_embedding_dim: int,
                 region_embedding_dim: int,
                 num_partitions: int,
                 agent_config: dict,
                 device: torch.device,
                 use_mixed_precision: bool = False
                ):
        """
        åˆå§‹åŒ–PPOæ™ºèƒ½ä½“

        Args:
            node_embedding_dim: èŠ‚ç‚¹åµŒå…¥ç»´åº¦
            region_embedding_dim: åŒºåŸŸåµŒå…¥ç»´åº¦
            num_partitions: åˆ†åŒºæ•°
            agent_config: æ™ºèƒ½ä½“é…ç½®å­—å…¸
            device: è®¡ç®—è®¾å¤‡
            use_mixed_precision: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        """
        self.config = agent_config
        self.gamma = agent_config.get('gamma', 0.99)
        self.gae_lambda = agent_config.get('gae_lambda', 0.95)
        self.clip_epsilon = agent_config.get('clip_epsilon', 0.2)
        self.eps_clip = agent_config.get('eps_clip', 0.2)  # æ·»åŠ eps_clipåˆ«å
        self.k_epochs = agent_config.get('k_epochs', 4)  # æ·»åŠ ç¼ºå¤±çš„k_epochs
        self.entropy_coef = agent_config.get('entropy_coef', 0.01)
        self.value_loss_coef = agent_config.get('value_loss_coef', 0.5)
        self.value_coef = agent_config.get('value_coef', 0.5)  # æ·»åŠ value_coefåˆ«å
        self.max_grad_norm = agent_config.get('max_grad_norm', 1.5)  # æ·»åŠ æ¢¯åº¦è£å‰ªé˜ˆå€¼
        self.device = device
        self.use_mixed_precision = use_mixed_precision
        self.grad_scaler = torch.cuda.amp.GradScaler(enabled=use_mixed_precision)
        self.scaler = self.grad_scaler  # æ·»åŠ scaleråˆ«å

        self.node_embedding_dim = node_embedding_dim
        self.region_embedding_dim = region_embedding_dim
        self.num_partitions = num_partitions
        self.strategy_vector_dim = agent_config.get('strategy_vector_dim', 128)

        actor_config = agent_config.get('actor', {})
        critic_config = agent_config.get('critic', {})
        partition_enc_cfg = agent_config.get('partition_encoder', {})
        
        # ç¡®ä¿ç­–ç•¥å‘é‡ç»´åº¦ä¸åˆ†åŒºç¼–ç å™¨åµŒå…¥ç»´åº¦ä¸€è‡´
        self.strategy_vector_dim = agent_config.get('strategy_vector_dim', 128)
        
        actor_config['strategy_vector_dim'] = self.strategy_vector_dim
        partition_enc_cfg['embedding_dim'] = self.strategy_vector_dim  # ä½¿ç”¨ç›¸åŒçš„ç»´åº¦
        partition_enc_cfg['output_dim'] = self.strategy_vector_dim

        # å°†é…ç½®å‚æ•°åˆå¹¶åˆ°actor_configä¸­
        actor_config['node_embedding_dim'] = self.node_embedding_dim
        actor_config['region_embedding_dim'] = self.region_embedding_dim
        actor_config['num_partitions'] = self.num_partitions
        
        self.actor = create_actor_network(
            config=actor_config,
            use_two_tower=True
        ).to(device)

        self.critic = CriticNetwork(
            node_embedding_dim=self.node_embedding_dim,
            region_embedding_dim=self.region_embedding_dim,
            hidden_dim=critic_config.get('hidden_dim', 256),
            dropout=critic_config.get('dropout', 0.1)
        ).to(device)

        # å°†num_partitionsæ·»åŠ åˆ°é…ç½®ä¸­
        partition_enc_cfg['num_partitions'] = self.num_partitions
        
        self.partition_encoder = create_partition_encoder(
            config=partition_enc_cfg
        ).to(device)

        # åˆ†ç¦»çš„ä¼˜åŒ–å™¨ - ä¿®å¤ç¼ºå¤±çš„actor_optimizerå’Œcritic_optimizer
        lr_actor = agent_config.get('lr_actor', 5e-5)
        lr_critic = agent_config.get('lr_critic', 1e-4)

        # å°†partition_encoderçš„å‚æ•°ä¹ŸåŠ å…¥actor_optimizer
        actor_params = list(self.actor.parameters()) + list(self.partition_encoder.parameters())
        self.actor_optimizer = torch.optim.Adam(actor_params, lr=lr_actor)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)

        # ä¿æŒå‘åå…¼å®¹çš„ç»Ÿä¸€ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(
            list(self.actor.parameters()) + list(self.critic.parameters()) + list(self.partition_encoder.parameters()),
            lr=agent_config.get('learning_rate', 3e-4),
            eps=agent_config.get('adam_eps', 1e-5)
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.actor_scheduler = None
        self.critic_scheduler = None
        actor_scheduler_config = agent_config.get('actor_scheduler', {})
        critic_scheduler_config = agent_config.get('critic_scheduler', {})

        if actor_scheduler_config.get('enabled', False):
            self.actor_scheduler = torch.optim.lr_scheduler.StepLR(
                self.actor_optimizer,
                step_size=actor_scheduler_config.get('step_size', 50),
                gamma=actor_scheduler_config.get('gamma', 0.9)
            )
        if critic_scheduler_config.get('enabled', False):
            self.critic_scheduler = torch.optim.lr_scheduler.StepLR(
                self.critic_optimizer,
                step_size=critic_scheduler_config.get('step_size', 50),
                gamma=critic_scheduler_config.get('gamma', 0.9)
            )

        # ä¿æŒå‘åå…¼å®¹çš„ç»Ÿä¸€è°ƒåº¦å™¨
        self.scheduler = None
        if agent_config.get('use_lr_scheduler', False):
            self.scheduler = torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=agent_config.get('lr_step_size', 50),
                gamma=agent_config.get('lr_gamma', 0.9)
            )

        self.memory = FastPPOMemory()

        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'actor_loss': deque(maxlen=100),
            'critic_loss': deque(maxlen=100),
            'entropy': deque(maxlen=100)
        }

    @classmethod
    def from_config(cls, env, config, device, use_mixed_precision=False):
        """ä»é…ç½®åˆ›å»ºAgentå®ä¾‹çš„å·¥å‚æ–¹æ³•"""
        return cls(
            node_embedding_dim=env.state_manager.embedding_dim,
            region_embedding_dim=env.state_manager.embedding_dim * 2,
            num_partitions=env.num_partitions,
            agent_config=config,
            device=device,
            use_mixed_precision=use_mixed_precision
        )
        
    def select_action(self, 
                      state: Dict[str, torch.Tensor], 
                      env: Any, # ç¯å¢ƒå®ä¾‹ï¼Œç”¨äºè°ƒç”¨æ–°æ–¹æ³•
                      training: bool = True
                      ) -> Tuple[Optional[Tuple[int, int]], float, float]:
        """
        ä½¿ç”¨å½“å‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œ (v3.0 åŒå¡”æ¶æ„é‡æ„)
        
        Args:
            state: çŠ¶æ€è§‚å¯Ÿ
            env: ç¯å¢ƒå®ä¾‹ï¼Œç”¨äºè·å–å€™é€‰åŠ¨ä½œå’Œåˆ†åŒºç‰¹å¾
            training: æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼
            
        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ (node_idx, partition_idx)ï¼Œå¦‚æœæ— åŠ¨ä½œåˆ™ä¸ºNone
            log_prob: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            value: çŠ¶æ€ä»·å€¼ä¼°è®¡
        """
        with torch.no_grad():
            # 1. ç»Ÿä¸€æ¥å£ï¼šå°†å•ä¸€æ ·æœ¬å°è£…ä¸ºæ‰¹å¤„ç†BatchedState
            batched_state = self._prepare_batched_state([state])

            # 2. è·å–ä¼°å€¼ (Value)
            value = self.critic(batched_state).item()

            # 3. å¦‚æœæ²¡æœ‰è¾¹ç•ŒèŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›
            if batched_state.boundary_nodes.numel() == 0:
                return None, 0.0, value

            # --- v3.0 ä¸¤é˜¶æ®µå†³ç­–æµç¨‹ ---
            # 4. è·å–ç­–ç•¥å‘é‡
            node_logits, strategy_vectors = self.actor(batched_state)

            # 5. é˜¶æ®µä¸€ï¼šèŠ‚ç‚¹è¿‡æ»¤ä¸æ©ç 
            # ç¡®å®šæ¯ä¸ªè¾¹ç•ŒèŠ‚ç‚¹æ˜¯å¦æœ‰ä»»ä½•æœ‰æ•ˆçš„ç§»åŠ¨
            valid_node_mask_list = [
                bool(env.get_connectivity_safe_partitions(node_id.item()))
                for node_id in batched_state.boundary_nodes
            ]
            valid_node_mask = torch.tensor(valid_node_mask_list, device=self.device, dtype=torch.bool)
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•èŠ‚ç‚¹æœ‰æœ‰æ•ˆç§»åŠ¨ï¼Œåˆ™æå‰é€€å‡º
            if not valid_node_mask.any():
                return None, 0.0, value

            # åº”ç”¨æ©ç ï¼Œå±è”½æ‰æ²¡æœ‰æœ‰æ•ˆç§»åŠ¨çš„èŠ‚ç‚¹
            masked_node_logits = node_logits.masked_fill(~valid_node_mask, -1e9)

            # 6. é˜¶æ®µäºŒï¼šå®‰å…¨é‡‡æ ·
            # ä»æœ‰æ•ˆèŠ‚ç‚¹ä¸­é‡‡æ ·æˆ–è´ªå¿ƒé€‰æ‹©
            if training:
                node_probs = F.softmax(masked_node_logits, dim=0)
                node_dist = torch.distributions.Categorical(probs=node_probs)
                node_action_idx = node_dist.sample() # boundary_nodes ä¸­çš„ç´¢å¼•
            else: # è´ªå¿ƒæ¨¡å¼
                node_action_idx = torch.argmax(masked_node_logits)

            selected_node_global_id = batched_state.boundary_nodes[node_action_idx].item()
            available_partitions = env.get_connectivity_safe_partitions(selected_node_global_id)

            # 7. è·å–é€‰å®šèŠ‚ç‚¹çš„ç­–ç•¥å‘é‡
            strategy_vector = strategy_vectors[node_action_idx]

            # 8. ç¼–ç å¯ç”¨åˆ†åŒº
            partition_features = env.get_partition_features(available_partitions)
            partition_embeddings = self.partition_encoder(partition_features)

            # 9. è®¡ç®—ç›¸ä¼¼åº¦å¹¶é‡‡æ ·åˆ†åŒº
            similarities = self.partition_encoder.compute_similarity(strategy_vector, partition_embeddings)
            
            if training:
                partition_probs = F.softmax(similarities, dim=0)
                partition_dist = torch.distributions.Categorical(probs=partition_probs)
                selected_partition_local_idx = partition_dist.sample()
                
                # è®¡ç®—ç»„åˆå¯¹æ•°æ¦‚ç‡
                log_prob_node = node_dist.log_prob(node_action_idx)
                log_prob_partition = partition_dist.log_prob(selected_partition_local_idx)
                log_prob = (log_prob_node + log_prob_partition).item()
            else: # è´ªå¿ƒæ¨¡å¼
                selected_partition_local_idx = torch.argmax(similarities)
                log_prob = 0.0

            # 10. è½¬æ¢å›åˆ†åŒºID
            selected_partition_id = available_partitions[selected_partition_local_idx.item()]
            action = (selected_node_global_id, selected_partition_id)
            
        return action, log_prob, value
        
    def store_experience(self, 
                         state: Dict[str, torch.Tensor], 
                         action: Optional[Tuple[int, int]] = None,
                         decision_context: Optional[AbstractDecisionContext] = None,
                         reward: float = 0.0, 
                         log_prob: float = 0.0, 
                         value: float = 0.0, 
                         done: bool = False):
        """
        åœ¨ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­å­˜å‚¨ä¸€ä¸ªæ—¶é—´æ­¥çš„ç»éªŒ (æ”¯æŒv3.0 DecisionContext)ã€‚

        Args:
            state (Dict[str, torch.Tensor]): å½“å‰çŠ¶æ€çš„è§‚å¯Ÿå­—å…¸ã€‚
            action (Optional[Tuple[int, int]]): æ‰§è¡Œçš„åŠ¨ä½œï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
            decision_context (Optional[AbstractDecisionContext]): v3.0å†³ç­–ä¸Šä¸‹æ–‡ã€‚
            reward (float): ä»ç¯å¢ƒä¸­è·å¾—çš„å¥–åŠ±ã€‚
            log_prob (float): æ‰§è¡ŒåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ï¼ˆv3.0ä¸­ä¼šè¢«é‡æ–°è®¡ç®—ï¼‰ã€‚
            value (float): è¯„åˆ¤ç½‘ç»œå¯¹å½“å‰çŠ¶æ€çš„ä»·å€¼ä¼°è®¡ã€‚
            done (bool): å›åˆæ˜¯å¦ç»“æŸçš„æ ‡å¿—ã€‚
        """
        # ä¼ é€’ç»™å†…å­˜ç®¡ç†å™¨ï¼Œå®ƒä¼šå¤„ç†æ–°æ—§æ ¼å¼
        self.memory.store(
            state=state,
            action=action,
            decision_context=decision_context,
            reward=reward,
            log_prob=log_prob,
            value=value,
            done=done
        )
        
    def update(self, **kwargs) -> Dict[str, float]:
        """
        ä½¿ç”¨PPOæ›´æ–°ç½‘ç»œï¼ˆv3.0é‡æ„ï¼Œéœ€è¦envå®ä¾‹ï¼‰

        Returns:
            è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        try:
            if len(self.memory) == 0:
                return {}

            # v3.0: å¿…é¡»å°† env å®ä¾‹ä¼ å…¥ _ppo_epoch
            if 'env' not in kwargs:
                 raise ValueError("PPOAgent.update() now requires 'env' keyword argument for v3.0 architecture.")
            env = kwargs['env']

            # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šç›´æ¥è·å–å¼ é‡ï¼Œé¿å…CPU-GPUä¼ è¾“
            states, actions, rewards_tensor, old_log_probs_tensor, old_values_tensor, dones_tensor = self.memory.get_batch_tensors()

            # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
            advantages, returns = self._compute_advantages(rewards_tensor, old_values_tensor, dones_tensor)

            # PPOæ›´æ–°
            agg_stats = {
                'actor_loss': 0.0,
                'critic_loss': 0.0,
                'entropy': 0.0,
                'grad_norm': 0.0,
            }
            total_samples = 0

            for _ in range(self.k_epochs):
                epoch_stats, batch_size = self._ppo_epoch(
                    states, actions, old_log_probs_tensor, advantages, returns, env=env)

                for key in agg_stats:
                    if key in epoch_stats:
                        agg_stats[key] += epoch_stats[key] * batch_size

                total_samples += batch_size

            # æ ·æœ¬å¹³å‡
            for key in agg_stats:
                agg_stats[key] /= max(total_samples, 1)

            # æ›´æ–°è®­ç»ƒç»Ÿè®¡
            self.training_stats['actor_loss'].append(agg_stats['actor_loss'])
            self.training_stats['critic_loss'].append(agg_stats['critic_loss'])
            self.training_stats['entropy'].append(agg_stats['entropy'])

            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            if self.actor_scheduler:
                self.actor_scheduler.step()
            if self.critic_scheduler:
                self.critic_scheduler.step()

            # æ¸…ç©ºå†…å­˜
            self.memory.clear()

            return agg_stats

        except Exception as e:
            # ğŸ”§ DEBUG: æ•è·æ‰€æœ‰å¼‚å¸¸å¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯
            print(f"\n=== AGENT UPDATE ERROR ===")
            print(f"Error: {e}")
            print(f"Error type: {type(e)}")
            import traceback
            print(f"Full traceback:\n{traceback.format_exc()}")
            print("=== END AGENT UPDATE ERROR ===\n")
            raise e
        
    def _compute_advantages(self, 
                            rewards: torch.Tensor, 
                            values: torch.Tensor, 
                            dones: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ä½¿ç”¨å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ (GAE) è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥ã€‚

        Args:
            rewards (torch.Tensor): ä¸€ä¸ªå›åˆçš„å¥–åŠ±åºåˆ—ã€‚
            values (torch.Tensor): ä¸€ä¸ªå›åˆçš„çŠ¶æ€ä»·å€¼åºåˆ—ã€‚
            dones (torch.Tensor): ä¸€ä¸ªå›åˆçš„ç»“æŸæ ‡å¿—åºåˆ—ã€‚

        Returns:
            Tuple[torch.Tensor, torch.Tensor]:
                - advantages: æ ‡å‡†åŒ–åçš„ä¼˜åŠ¿å‡½æ•°åºåˆ—ã€‚
                - returns: GAEè®¡ç®—å‡ºçš„å›æŠ¥åºåˆ—ã€‚
        """
        advantages = torch.zeros_like(rewards)
        returns = torch.zeros_like(rewards)

        gae = 0
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]

            delta = rewards[t] + self.gamma * next_value * (~dones[t]).float() - values[t]
            gae = delta + self.gamma * 0.95 * (~dones[t]).float() * gae
            advantages[t] = gae
            returns[t] = advantages[t] + values[t]

        # å®‰å…¨çš„å›æŠ¥å¥åº·æ£€æŸ¥
        returns = torch.nan_to_num(returns, nan=0.0, posinf=0.0, neginf=0.0)

        # å®‰å…¨æ ‡å‡†åŒ–ä¼˜åŠ¿
        def safe_standardize(t, eps=1e-6):
            mean = t.mean()
            std = t.std()
            if torch.isnan(std) or std < eps:
                return t - mean
            return (t - mean) / (std + eps)

        advantages = safe_standardize(advantages)
        advantages = torch.nan_to_num(advantages, nan=0.0, posinf=0.0, neginf=0.0)

        return advantages, returns

    def _compute_grad_norm(self, model: nn.Module) -> float:
        """
        è®¡ç®—æ¨¡å‹çš„æ¢¯åº¦èŒƒæ•°

        Args:
            model: è¦è®¡ç®—æ¢¯åº¦èŒƒæ•°çš„æ¨¡å‹

        Returns:
            æ¢¯åº¦çš„L2èŒƒæ•°
        """
        total_norm = 0.0
        param_count = 0
        for param in model.parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1

        if param_count == 0:
            return 0.0
        return (total_norm ** 0.5)

    def _compute_param_norm(self, model: nn.Module) -> float:
        """
        è®¡ç®—æ¨¡å‹çš„å‚æ•°èŒƒæ•°

        Args:
            model: è¦è®¡ç®—å‚æ•°èŒƒæ•°çš„æ¨¡å‹

        Returns:
            å‚æ•°çš„L2èŒƒæ•°
        """
        total_norm = 0.0
        for param in model.parameters():
            param_norm = param.data.norm(2)
            total_norm += param_norm.item() ** 2
        return (total_norm ** 0.5)

    def set_tensorboard_writer(self, writer):
        """
        è®¾ç½®TensorBoard writerç”¨äºæ¢¯åº¦ç›‘æ§

        Args:
            writer: TensorBoard SummaryWriterå®ä¾‹
        """
        self.writer = writer
        
    def _ppo_epoch(self,
                   states: List[Dict[str, torch.Tensor]],
                   actions_or_contexts: List,  # æ”¯æŒä¼ ç»Ÿactionsæˆ–AbstractDecisionContext
                   old_log_probs: torch.Tensor,
                   advantages: torch.Tensor,
                   returns: torch.Tensor,
                   env: Any) -> Tuple[Dict[str, float], int]:
        """
        æ‰§è¡Œå•ä¸ªPPOæ›´æ–°å‘¨æœŸ (v3.0 åŒå¡”æ¶æ„é‡æ„)ã€‚
        
        æ ¸å¿ƒæ”¹è¿›ï¼šæ”¯æŒAbstractDecisionContextçš„ç›´æ¥log_probé‡å»ºï¼Œ
        é¿å…å¤æ‚çš„ç¯å¢ƒé‡è®¾å’Œåˆ†åŒºé‡ç¼–ç ï¼Œæ˜¾è‘—æå‡æ€§èƒ½å’Œç¨³å®šæ€§ã€‚
        """
        batch_size = len(states)
        if batch_size == 0:
            return {'actor_loss': 0.0, 'critic_loss': 0.0, 'entropy': 0.0}, 0

        # ---- 0. æ£€æµ‹æ•°æ®ç±»å‹å¹¶é€‰æ‹©å¤„ç†åˆ†æ”¯ ----
        # æ£€æŸ¥æ˜¯å¦ä¸ºv3.0 AbstractDecisionContextæ ¼å¼
        has_decision_contexts = (
            batch_size > 0 and 
            isinstance(actions_or_contexts[0], AbstractDecisionContext)
        )
        
        if has_decision_contexts:
            return self._ppo_epoch_v3_contexts(
                states, actions_or_contexts, old_log_probs, advantages, returns, env
            )
        else:
            # å›é€€åˆ°ä¼ ç»Ÿçš„å…·ä½“actionå¤„ç†
            return self._ppo_epoch_legacy_actions(
                states, actions_or_contexts, old_log_probs, advantages, returns, env
            )
    
    def _ppo_epoch_v3_contexts(self,
                               states: List[Dict[str, torch.Tensor]],
                               decision_contexts: List[AbstractDecisionContext],
                               old_log_probs: torch.Tensor,
                               advantages: torch.Tensor,
                               returns: torch.Tensor,
                               env: Any) -> Tuple[Dict[str, float], int]:
        """
        v3.0æ¶æ„çš„PPO epochå®ç°ï¼šä½¿ç”¨AbstractDecisionContext
        
        æ ¸å¿ƒä¼˜åŠ¿ï¼š
        - ç›´æ¥ä»ä¿å­˜çš„ä¸Šä¸‹æ–‡é‡å»ºlog_probï¼Œæ— éœ€ç¯å¢ƒé‡è®¾
        - é¿å…åˆ†åŒºç‰¹å¾é‡æ–°ç¼–ç ï¼Œæå‡æ€§èƒ½
        - æ”¯æŒè·¨æ‹“æ‰‘æ³›åŒ–ï¼ŒcontextsåŒ…å«å®Œæ•´ä¿¡æ¯
        """
        batch_size = len(states)
        
        # ---- 1. æ‰¹é‡åŒ–çŠ¶æ€å’Œä»·å€¼ä¼°è®¡ ----
        batched_state = self._prepare_batched_state(states)
        values = self.critic(batched_state).squeeze()
        
        # ---- 2. ä»å†³ç­–ä¸Šä¸‹æ–‡é‡å»ºlog_probå’Œç†µ ----
        context_batch = DecisionContextBatch(
            contexts=decision_contexts,
            batch_size=batch_size
        )
        
        # ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
        context_batch = context_batch.to_device(self.device)
        
        # æ‰¹é‡è®¡ç®—logæ¦‚ç‡ - è¿™æ˜¯v3.0çš„æ ¸å¿ƒä¼˜åŠ¿ï¼
        new_log_probs = context_batch.compute_batch_log_probs(temperature=1.0)
        
        # è®¡ç®—ç†µï¼ˆä½¿ç”¨ä¿å­˜çš„ç›¸ä¼¼åº¦é‡å»ºåˆ†å¸ƒï¼‰
        entropy_list = []
        for ctx in context_batch.contexts:
            probs = ctx.compute_probability_distribution(temperature=1.0)
            entropy = -torch.sum(probs * torch.log(probs + 1e-8))
            entropy_list.append(entropy)
        
        entropy = torch.stack(entropy_list).mean()
        
        # ---- 3. PPOæŸå¤±è®¡ç®— ----
        if isinstance(returns, list):
            returns = torch.tensor(returns, dtype=torch.float32, device=self.device)
        if isinstance(values, list):
            values = torch.tensor(values, dtype=torch.float32, device=self.device)

        with torch.amp.autocast(device_type=self.device.type, dtype=torch.bfloat16):
            ratio = torch.exp(new_log_probs - old_log_probs)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages
            actor_loss = -torch.min(surr1, surr2).mean()
            critic_loss = F.mse_loss(values, returns)
            total_loss = (actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy).float()

        # ---- 4. åå‘ä¼ æ’­å’Œæ›´æ–° ----
        self.actor_optimizer.zero_grad(set_to_none=True)
        self.critic_optimizer.zero_grad(set_to_none=True)

        self.scaler.scale(total_loss).backward()

        total_grad_norm = torch.tensor(0.0)
        if self.max_grad_norm is not None:
            self.scaler.unscale_(self.actor_optimizer)
            self.scaler.unscale_(self.critic_optimizer)
            all_params = list(self.actor.parameters()) + list(self.partition_encoder.parameters()) + list(self.critic.parameters())
            total_grad_norm = torch.nn.utils.clip_grad_norm_(all_params, self.max_grad_norm)

        self.scaler.step(self.actor_optimizer)
        self.scaler.step(self.critic_optimizer)
        self.scaler.update()

        return ({
            'actor_loss': actor_loss.item(),
            'critic_loss': critic_loss.item(),
            'entropy': entropy.item(),
            'grad_norm': total_grad_norm.item(),
        }, batch_size)
    
    def _ppo_epoch_legacy_actions(self,
                                  states: List[Dict[str, torch.Tensor]],
                                  actions: List[Tuple[int, int]],
                                  old_log_probs: torch.Tensor,
                                  advantages: torch.Tensor,
                                  returns: torch.Tensor,
                                  env: Any) -> Tuple[Dict[str, float], int]:
        """
        ä¼ ç»ŸPPO epochå®ç°ï¼šå‘åå…¼å®¹å…·ä½“actions
        """
        batch_size = len(states)
        
        # ---- 1. æ•°æ®æ‰¹é‡åŒ– å’Œ ä»·å€¼ä¼°è®¡ ----
        batched_state = self._prepare_batched_state(states)
        values = self.critic(batched_state).squeeze()

        # ---- 2. æ ¸å¿ƒï¼šä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªåŠ¨ä½œé‡æ–°è®¡ç®— log_prob å’Œç†µ ----
        node_logits, strategy_vectors = self.actor(batched_state)
        boundary_batch_idx = batched_state.node_batch_idx[batched_state.boundary_nodes]
        
        # âš¡ ç§»é™¤æ·±æ‹·è´ä»¥é¿å…ä¸å¯å“ˆå¸Œé”®é”™è¯¯å¹¶æå‡æ€§èƒ½
        orig_partition = env.state_manager.current_partition.clone()
        try:
            log_probs_list = []
            entropy_list = []

            # è¿™æ˜¯ä¸€ä¸ªæ€§èƒ½ç“¶é¢ˆï¼Œç†æƒ³æƒ…å†µä¸‹åº”æ‰¹é‡åŒ–ã€‚ä½†ä¸ºäº†é€»è¾‘æ¸…æ™°ï¼Œå…ˆä½¿ç”¨å¾ªç¯å®ç°ã€‚
            for i in range(batch_size):
                env.state_manager.current_partition = states[i]['current_partition']
                
                sample_mask = (boundary_batch_idx == i)
                sample_boundary_nodes = batched_state.boundary_nodes[sample_mask]
                
                if sample_boundary_nodes.numel() == 0: continue

                # a) èŠ‚ç‚¹è¿‡æ»¤ä¸æ©ç 
                valid_node_mask = torch.tensor([
                    bool(env.get_connectivity_safe_partitions(node_id.item()))
                    for node_id in sample_boundary_nodes
                ], device=self.device, dtype=torch.bool)
                
                if not valid_node_mask.any(): continue

                masked_node_logits = node_logits[sample_mask].masked_fill(~valid_node_mask, -1e9)
                node_probs = F.softmax(masked_node_logits, dim=0)
                node_dist = torch.distributions.Categorical(probs=node_probs)
                
                # b) è·å–åŠ¨ä½œå¹¶ç¡®ä¿ç±»å‹æ­£ç¡®
                action = actions[i]

                # è®°å½•å…³é”®åŠ¨ä½œä¿¡æ¯ï¼ˆç”Ÿäº§çº§åˆ«ï¼‰
                if i == 0:  # ä»…è®°å½•ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å…³é”®ä¿¡æ¯
                    logger.debug(f"åŠ¨ä½œç±»å‹: {type(action)}, å†…å®¹: {action}")

                # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿actionæ˜¯æ­£ç¡®çš„æ ¼å¼
                if isinstance(action, (list, tuple)) and len(action) >= 2:
                    node_id = action[0]
                    partition_id = action[1]

                    # ç¡®ä¿node_idæ˜¯æ•´æ•°
                    if isinstance(node_id, torch.Tensor):
                        node_id = node_id.item()
                    elif not isinstance(node_id, int):
                        node_id = int(node_id)

                    # ç¡®ä¿partition_idæ˜¯æ•´æ•°
                    if isinstance(partition_id, (list, tuple)):
                        partition_id = partition_id[0] if len(partition_id) > 0 else 1
                    elif isinstance(partition_id, torch.Tensor):
                        partition_id = partition_id.item()
                    elif not isinstance(partition_id, int):
                        partition_id = int(partition_id)
                else:
                    # å¦‚æœactionæ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡è¿™ä¸ªæ ·æœ¬
                    continue
                
                # c) ç¼–ç åˆ†åŒº
                available_partitions = env.get_connectivity_safe_partitions(node_id)
                if not available_partitions: continue

                # ğŸ”§ ä¿®å¤ï¼šä¸ºæ¯ä¸ªåˆ†åŒºåˆ†åˆ«è·å–ç‰¹å¾å¹¶è½¬æ¢ä¸ºå¼ é‡
                partition_features_list = []
                for pid in available_partitions:
                    features = env.get_partition_features(pid)
                    partition_features_list.append(features)

                # å°†ç‰¹å¾åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡ï¼ˆåªé€‰æ‹©æ•°å€¼ç‰¹å¾ï¼Œç¡®ä¿ç»´åº¦ä¸º6ï¼‰
                if partition_features_list:
                    # å®šä¹‰partition_encoderæœŸæœ›çš„6ä¸ªç‰¹å¾ï¼ˆæŒ‰é¡ºåºï¼‰
                    expected_features = [
                        'size_ratio', 'load_ratio', 'generation_ratio',
                        'internal_connectivity', 'boundary_ratio', 'power_imbalance'
                    ]

                    feature_tensors = []
                    for features in partition_features_list:
                        # æå–6ä¸ªæ ‡å‡†ç‰¹å¾
                        values = []
                        for key in expected_features:
                            val = features.get(key, 0.0)  # å¦‚æœç¼ºå°‘ç‰¹å¾ï¼Œä½¿ç”¨0.0
                            if isinstance(val, (int, float)):
                                values.append(float(val))
                            elif isinstance(val, torch.Tensor):
                                values.append(val.item() if val.numel() == 1 else val.mean().item())
                            else:
                                values.append(0.0)  # å…¶ä»–ç±»å‹è½¬ä¸º0.0

                        # ç¡®ä¿æ­£å¥½6ä¸ªç‰¹å¾
                        if len(values) != 6:
                            values = values[:6] + [0.0] * (6 - len(values))

                        feature_tensors.append(torch.tensor(values, dtype=torch.float32, device=self.device))

                    partition_features_tensor = torch.stack(feature_tensors)
                    partition_embeddings = self.partition_encoder(partition_features_tensor)
                else:
                    # å¦‚æœæ²¡æœ‰å¯ç”¨åˆ†åŒºï¼Œåˆ›å»ºç©ºå¼ é‡
                    partition_embeddings = torch.empty(0, self.partition_encoder.embedding_dim, device=self.device)
                
                # d) è®¡ç®—åˆ†åŒºæ¦‚ç‡
                node_action_idx_in_sample = (sample_boundary_nodes == node_id).nonzero(as_tuple=True)[0]
                strategy_vector = strategy_vectors[sample_mask][node_action_idx_in_sample].squeeze(0)
                similarities = self.partition_encoder.compute_similarity(strategy_vector, partition_embeddings)
                partition_probs = F.softmax(similarities, dim=0)
                partition_dist = torch.distributions.Categorical(probs=partition_probs)
                
                # e) è®¡ç®—LogProbå’Œç†µ
                try:
                    # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿partition_idæ˜¯æ•´æ•°ç±»å‹
                    if isinstance(partition_id, (list, tuple)):
                        # å¦‚æœpartition_idæ˜¯listæˆ–tupleï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                        partition_id = partition_id[0] if len(partition_id) > 0 else 1
                    elif isinstance(partition_id, torch.Tensor):
                        # å¦‚æœæ˜¯å¼ é‡ï¼Œè½¬æ¢ä¸ºæ•´æ•°
                        partition_id = partition_id.item()
                    elif not isinstance(partition_id, int):
                        # å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºæ•´æ•°
                        partition_id = int(partition_id)

                    # ç¡®ä¿available_partitionsä¸­çš„å…ƒç´ ä¹Ÿæ˜¯æ•´æ•°
                    available_partitions = [int(p) if isinstance(p, torch.Tensor) else int(p)
                                          for p in available_partitions]

                    # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥partition_idæ˜¯å¦åœ¨å¯ç”¨åˆ†åŒºåˆ—è¡¨ä¸­
                    if partition_id not in available_partitions:
                        # è·³è¿‡æ— æ•ˆçš„æ ·æœ¬
                        continue

                    part_action_idx_in_list = available_partitions.index(partition_id)
                except Exception as debug_e:
                    # ç”Ÿäº§çº§é”™è¯¯å¤„ç†å’Œè®°å½•
                    logger.error(f"åˆ†åŒºç´¢å¼•æŸ¥æ‰¾å¤±è´¥: {debug_e}")
                    logger.error(f"åˆ†åŒºID: {partition_id} (ç±»å‹: {type(partition_id)})")
                    logger.error(f"å¯ç”¨åˆ†åŒº: {available_partitions}")
                    import traceback
                    logger.debug(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                    # è·³è¿‡æœ‰é—®é¢˜çš„æ ·æœ¬è€Œä¸æ˜¯å´©æºƒ
                    continue

                try:
                    log_prob = node_dist.log_prob(node_action_idx_in_sample) + \
                               partition_dist.log_prob(torch.tensor(part_action_idx_in_list, device=self.device))
                    log_probs_list.append(log_prob)
                    entropy_list.append(node_dist.entropy() + partition_dist.entropy())
                except ValueError:
                    continue

        # æ¢å¤ç¯å¢ƒåˆ†åŒºæ˜ å°„
        finally:
            env.state_manager.current_partition = orig_partition

        if not log_probs_list:
            return {'actor_loss': 0.0, 'critic_loss': 0.0, 'entropy': 0.0}, 0
            
        new_log_probs = torch.stack(log_probs_list)
        entropy = torch.stack(entropy_list).mean()

        # ---- 3. PPO æŸå¤±è®¡ç®— ----
        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿returnså’Œvalueséƒ½æ˜¯å¼ é‡
        if isinstance(returns, list):
            returns = torch.tensor(returns, dtype=torch.float32, device=self.device)
        if isinstance(values, list):
            values = torch.tensor(values, dtype=torch.float32, device=self.device)

        with torch.amp.autocast(device_type=self.device.type, dtype=torch.bfloat16):
            ratio = torch.exp(new_log_probs - old_log_probs)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages
            actor_loss = -torch.min(surr1, surr2).mean()
            critic_loss = F.mse_loss(values, returns)
            total_loss = (actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy).float()

        # ---- 4. åå‘ä¼ æ’­ & æ›´æ–° ----
        self.actor_optimizer.zero_grad(set_to_none=True)
        self.critic_optimizer.zero_grad(set_to_none=True)

        self.scaler.scale(total_loss).backward()

        total_grad_norm = torch.tensor(0.0)
        if self.max_grad_norm is not None:
            self.scaler.unscale_(self.actor_optimizer)
            self.scaler.unscale_(self.critic_optimizer)
            all_params = list(self.actor.parameters()) + list(self.partition_encoder.parameters()) + list(self.critic.parameters())
            total_grad_norm = torch.nn.utils.clip_grad_norm_(all_params, self.max_grad_norm)

        self.scaler.step(self.actor_optimizer)
        self.scaler.step(self.critic_optimizer)
        self.scaler.update()

        return ({
            'actor_loss': actor_loss.item(),
            'critic_loss': critic_loss.item(),
            'entropy': entropy.item(),
            'grad_norm': total_grad_norm.item(),
        }, batch_size)

    def enable_gradient_norm_debug(self, enable: bool = True):
        """å¯ç”¨/ç¦ç”¨æ¢¯åº¦èŒƒæ•°è°ƒè¯•æ‰“å°"""
        self._debug_grad_norm = enable
        
    def save(self, filepath: str):
        """ä¿å­˜æ™ºèƒ½ä½“çŠ¶æ€"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
        }, filepath)
        
    def load(self, filepath: str):
        """åŠ è½½æ™ºèƒ½ä½“çŠ¶æ€"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])

    def store_experience(self, 
                         state: Dict[str, torch.Tensor], 
                         action: Tuple[int, int], 
                         reward: float, 
                         log_prob: float, 
                         value: float, 
                         done: bool):
        """
        åœ¨ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­å­˜å‚¨ä¸€ä¸ªæ—¶é—´æ­¥çš„ç»éªŒã€‚

        Args:
            state (Dict[str, torch.Tensor]): å½“å‰çŠ¶æ€çš„è§‚å¯Ÿå­—å…¸ã€‚
            action (Tuple[int, int]): æ‰§è¡Œçš„åŠ¨ä½œã€‚
            reward (float): ä»ç¯å¢ƒä¸­è·å¾—çš„å¥–åŠ±ã€‚
            log_prob (float): æ‰§è¡ŒåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ã€‚
            value (float): è¯„åˆ¤ç½‘ç»œå¯¹å½“å‰çŠ¶æ€çš„ä»·å€¼ä¼°è®¡ã€‚
            done (bool): å›åˆæ˜¯å¦ç»“æŸçš„æ ‡å¿—ã€‚
        """
        # v3.0æ¶æ„æ”¯æŒï¼šæ£€æŸ¥actionæ˜¯å¦ä¸ºAbstractDecisionContext
        from .decision_context import AbstractDecisionContext
        if isinstance(action, AbstractDecisionContext):
            self.memory.store(state, action=None, decision_context=action, 
                            reward=reward, log_prob=log_prob, value=value, done=done)
        else:
            self.memory.store(state, action, reward, log_prob, value, done)

    def update_learning_rate(self, factor: float):
        """åŠ¨æ€æ›´æ–°å­¦ä¹ ç‡"""
        for param_group in self.actor_optimizer.param_groups:
            param_group['lr'] *= factor
        for param_group in self.critic_optimizer.param_groups:
            param_group['lr'] *= factor

    def get_current_learning_rates(self) -> Dict[str, float]:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        try:
            actor_lr = self.actor_optimizer.param_groups[0]['lr']
            critic_lr = self.critic_optimizer.param_groups[0]['lr']
            return {'actor_lr': actor_lr, 'critic_lr': critic_lr}
        except:
            return {'actor_lr': 0.0, 'critic_lr': 0.0}

    def _prepare_batched_state(self, states_list: List[Dict[str, torch.Tensor]]) -> "BatchedState":
        """
        å°†ä¸€ç³»åˆ—ç‹¬ç«‹çš„çŠ¶æ€å­—å…¸æ‹¼æ¥æˆä¸€ä¸ªæ‰¹å¤„ç†çš„BatchedStateå¯¹è±¡ã€‚

        è¿™ä¸ªå‡½æ•°æ˜¯æ€§èƒ½ä¼˜åŒ–çš„æ ¸å¿ƒï¼Œå®ƒé€šè¿‡å°†å¤šä¸ªå›¾æ ·æœ¬çš„æ•°æ®èšåˆï¼Œ
        ä½¿å¾—ç¥ç»ç½‘ç»œå¯ä»¥ä¸€æ¬¡æ€§åœ¨GPUä¸Šå¤„ç†æ•´ä¸ªæ‰¹æ¬¡ï¼Œè€Œä¸æ˜¯é€ä¸ªå¤„ç†ã€‚

        Args:
            states_list (List[Dict[str, torch.Tensor]]): 
                ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªä»£è¡¨å•ä¸€æ ·æœ¬çŠ¶æ€çš„å­—å…¸ã€‚

        Returns:
            BatchedState: åŒ…å«äº†æ‰€æœ‰è¾“å…¥æ ·æœ¬æ•°æ®çš„èšåˆæ‰¹å¤„ç†å¯¹è±¡ã€‚
        """
        # ç´¯ç§¯åˆ—è¡¨
        node_emb_list: List[torch.Tensor] = []
        region_emb_list: List[torch.Tensor] = []
        action_mask_list: List[torch.Tensor] = []
        node_batch_idx_list: List[torch.Tensor] = []
        region_batch_idx_list: List[torch.Tensor] = []
        boundary_nodes_global: List[torch.Tensor] = []

        node_offset = 0
        region_offset = 0

        for batch_id, state in enumerate(states_list):
            node_embeddings = state['node_embeddings'].detach()
            region_embeddings = state['region_embeddings'].detach()
            boundary_nodes = state['boundary_nodes']
            current_partition = state['current_partition']

            num_nodes = node_embeddings.size(0)
            num_regions = region_embeddings.size(0)

            # ==== åŠ¨ä½œæ©ç æ„é€  ====
            if 'action_mask' in state:
                action_mask = state['action_mask']
            else:
                # å¤åˆ¶è‡ªæ—§å®ç°çš„ç®€åŒ–é€»è¾‘
                action_mask = torch.zeros(
                    num_nodes, self.num_partitions, dtype=torch.bool, device=self.device
                )
                for node_idx in boundary_nodes:
                    current_node_partition = current_partition[node_idx].item()
                    for p in range(self.num_partitions):
                        if p + 1 != current_node_partition:
                            action_mask[node_idx, p] = True

            # ==== ç´¯ç§¯å¼ é‡ ====
            node_emb_list.append(node_embeddings)
            region_emb_list.append(region_embeddings)
            action_mask_list.append(action_mask)

            # batch idx
            node_batch_idx_list.append(torch.full((num_nodes,), batch_id, dtype=torch.long, device=self.device))
            region_batch_idx_list.append(torch.full((num_regions,), batch_id, dtype=torch.long, device=self.device))

            # è¾¹ç•ŒèŠ‚ç‚¹å…¨å±€ç´¢å¼•
            if boundary_nodes.numel() > 0:
                boundary_nodes_global.append(boundary_nodes + node_offset)

            node_offset += num_nodes
            region_offset += num_regions

        # ===== æ‹¼æ¥ =====
        node_embeddings_cat = torch.cat(node_emb_list, dim=0)
        region_embeddings_cat = torch.cat(region_emb_list, dim=0)
        action_mask_cat = torch.cat(action_mask_list, dim=0)
        node_batch_idx = torch.cat(node_batch_idx_list, dim=0)
        region_batch_idx = torch.cat(region_batch_idx_list, dim=0)
        if boundary_nodes_global:
            boundary_nodes_cat = torch.cat(boundary_nodes_global, dim=0)
        else:
            boundary_nodes_cat = torch.zeros(0, dtype=torch.long, device=self.device)

        return BatchedState(
            node_embeddings=node_embeddings_cat,
            region_embeddings=region_embeddings_cat,
            boundary_nodes=boundary_nodes_cat,
            node_batch_idx=node_batch_idx,
            region_batch_idx=region_batch_idx,
            action_mask=action_mask_cat,
        )
