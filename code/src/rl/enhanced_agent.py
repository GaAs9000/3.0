#!/usr/bin/env python3
"""
Enhanced PPO Agent with Action Embedding Support

åŸºäºåŠ¨ä½œåµŒå…¥çš„PPOæ™ºèƒ½ä½“å®ç°ï¼Œæ”¯æŒï¼š
- åŒå¡”æ¶æ„ï¼ˆçŠ¶æ€å¡”+åŠ¨ä½œå¡”ï¼‰
- åŠ¨æ€Kå€¼æ”¯æŒ
- è¿é€šæ€§ç¡¬çº¦æŸ
- é«˜æ•ˆçš„åŠ¨ä½œé€‰æ‹©æµç¨‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
from dataclasses import dataclass
from collections import defaultdict
import time

from .agent import PPOAgent, BatchedState
from models.partition_encoder import PartitionEncoder, create_partition_encoder
from models.actor_network import EnhancedActorNetwork, create_actor_network

logger = logging.getLogger(__name__)


@dataclass
class RiskAssessmentResult:
    """é£é™©è¯„ä¼°ç»“æœ"""
    risk_level: str  # 'safe', 'low', 'medium', 'high'
    risk_score: float  # 0.0-1.0
    available_partitions: List[int]
    fallback_reason: Optional[str] = None
    cache_hit: bool = False


class ConnectivityCache:
    """è¿é€šæ€§æ£€æŸ¥ç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self, max_size: int = 1000, ttl_seconds: int = 300):
        self.cache = {}
        self.access_times = {}
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds

    def _get_cache_key(self, partition_state: torch.Tensor, node_id: int, target_partition: int) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # ä½¿ç”¨åˆ†åŒºçŠ¶æ€çš„å“ˆå¸Œå’ŒåŠ¨ä½œä¿¡æ¯ä½œä¸ºé”®
        partition_hash = hash(tuple(partition_state.cpu().numpy()))
        return f"{partition_hash}_{node_id}_{target_partition}"

    def get(self, partition_state: torch.Tensor, node_id: int, target_partition: int) -> Optional[Dict]:
        """è·å–ç¼“å­˜çš„è¿é€šæ€§æ£€æŸ¥ç»“æœ"""
        key = self._get_cache_key(partition_state, node_id, target_partition)
        current_time = time.time()

        if key in self.cache:
            cached_time, result = self.cache[key]
            if current_time - cached_time < self.ttl_seconds:
                self.access_times[key] = current_time
                return result
            else:
                # è¿‡æœŸï¼Œåˆ é™¤
                del self.cache[key]
                if key in self.access_times:
                    del self.access_times[key]

        return None

    def put(self, partition_state: torch.Tensor, node_id: int, target_partition: int, result: Dict):
        """ç¼“å­˜è¿é€šæ€§æ£€æŸ¥ç»“æœ"""
        key = self._get_cache_key(partition_state, node_id, target_partition)
        current_time = time.time()

        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®
        if len(self.cache) >= self.max_size:
            oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
            del self.cache[oldest_key]
            del self.access_times[oldest_key]

        self.cache[key] = (current_time, result)
        self.access_times[key] = current_time


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨ - ç”¨äºè‡ªé€‚åº”çº¦æŸè°ƒèŠ‚"""

    def __init__(self, window_size: int = 50):
        self.window_size = window_size
        self.episode_data = []
        self.current_episode = 0

    def record_episode(self, episode_info: Dict[str, Any]):
        """è®°å½•å•ä¸ªepisodeçš„æ€§èƒ½æ•°æ®"""
        self.current_episode += 1

        # æå–å…³é”®æŒ‡æ ‡
        performance_data = {
            'episode': self.current_episode,
            'reward': episode_info.get('reward', -float('inf')),
            'success': episode_info.get('success', False),
            'connectivity_score': episode_info.get('connectivity_score', 0.0),
            'quality_score': episode_info.get('quality_score', 0.0),
            'episode_length': episode_info.get('episode_length', 0),
            'termination_reason': episode_info.get('termination_reason', 'unknown')
        }

        # ä»metricsä¸­æå–æ›´å¤šæŒ‡æ ‡
        if 'metrics' in episode_info:
            metrics = episode_info['metrics']
            performance_data.update({
                'cv': metrics.get('cv', 1.0),
                'coupling_ratio': metrics.get('coupling_ratio', 1.0),
                'power_imbalance': metrics.get('power_imbalance_normalized', 1.0)
            })

        self.episode_data.append(performance_data)

        # ä¿æŒçª—å£å¤§å°
        if len(self.episode_data) > self.window_size:
            self.episode_data = self.episode_data[-self.window_size:]

    def get_performance_metrics(self) -> Dict[str, float]:
        """è®¡ç®—å½“å‰æ€§èƒ½æŒ‡æ ‡"""
        if not self.episode_data:
            return {
                'success_rate': 0.0,
                'avg_connectivity': 0.0,
                'avg_reward': -float('inf'),
                'avg_quality': 0.0,
                'constraint_violation_rate': 1.0
            }

        recent_data = self.episode_data[-min(20, len(self.episode_data)):]  # æœ€è¿‘20ä¸ªepisode

        # è®¡ç®—æˆåŠŸç‡
        success_rate = sum(1 for d in recent_data if d['success']) / len(recent_data)

        # è®¡ç®—å¹³å‡è¿é€šæ€§åˆ†æ•°
        avg_connectivity = sum(d['connectivity_score'] for d in recent_data) / len(recent_data)

        # è®¡ç®—å¹³å‡å¥–åŠ±
        avg_reward = sum(d['reward'] for d in recent_data) / len(recent_data)

        # è®¡ç®—å¹³å‡è´¨é‡åˆ†æ•°
        avg_quality = sum(d['quality_score'] for d in recent_data) / len(recent_data)

        # è®¡ç®—çº¦æŸè¿è§„ç‡ï¼ˆåŸºäºç»ˆæ­¢åŸå› ï¼‰
        violation_count = sum(1 for d in recent_data
                            if d['termination_reason'] in ['connectivity_violation', 'constraint_violation'])
        constraint_violation_rate = violation_count / len(recent_data)

        return {
            'success_rate': success_rate,
            'avg_connectivity': avg_connectivity,
            'avg_reward': avg_reward,
            'avg_quality': avg_quality,
            'constraint_violation_rate': constraint_violation_rate,
            'episode_count': len(recent_data)
        }

    def should_adjust_constraints(self, target_success_rate: float = 0.6,
                                target_connectivity: float = 0.5) -> Tuple[bool, str]:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒæ•´çº¦æŸ"""
        metrics = self.get_performance_metrics()

        if metrics['episode_count'] < 10:  # æ•°æ®ä¸è¶³
            return False, "insufficient_data"

        # æˆåŠŸç‡è¿‡ä½ -> æ”¾æ¾çº¦æŸ
        if metrics['success_rate'] < target_success_rate * 0.7:
            return True, "relax_low_success"

        # è¿é€šæ€§è¿‡ä½ -> åŠ å¼ºçº¦æŸ
        if metrics['avg_connectivity'] < target_connectivity:
            return True, "tighten_low_connectivity"

        # çº¦æŸè¿è§„ç‡è¿‡é«˜ -> æ”¾æ¾çº¦æŸ
        if metrics['constraint_violation_rate'] > 0.3:
            return True, "relax_high_violation"

        # è¡¨ç°è‰¯å¥½ -> é€æ¸åŠ å¼ºçº¦æŸ
        if (metrics['success_rate'] > target_success_rate and
            metrics['avg_connectivity'] > target_connectivity * 1.2):
            return True, "tighten_good_performance"

        return False, "no_adjustment"


class RiskPredictor:
    """è½»é‡çº§é£é™©é¢„æµ‹å™¨"""

    def __init__(self, device: torch.device):
        self.device = device
        self.history = defaultdict(list)  # èŠ‚ç‚¹å†å²å†³ç­–æ¨¡å¼

    def predict_risk(self,
                    node_id: int,
                    current_partition: torch.Tensor,
                    boundary_nodes: torch.Tensor,
                    hetero_data: Any) -> RiskAssessmentResult:
        """
        é¢„æµ‹èŠ‚ç‚¹ç§»åŠ¨çš„é£é™©ç­‰çº§

        Args:
            node_id: ç›®æ ‡èŠ‚ç‚¹ID
            current_partition: å½“å‰åˆ†åŒºçŠ¶æ€
            boundary_nodes: è¾¹ç•ŒèŠ‚ç‚¹
            hetero_data: å›¾æ•°æ®

        Returns:
            é£é™©è¯„ä¼°ç»“æœ
        """
        # æ£€æŸ¥è¾“å…¥å‚æ•°
        if current_partition is None:
            # æä¾›é»˜è®¤çš„å¯ç”¨åˆ†åŒºè€Œä¸æ˜¯ç©ºåˆ—è¡¨
            return RiskAssessmentResult(
                risk_level='medium',
                risk_score=0.5,
                available_partitions=list(range(1, 4)),  # é»˜è®¤3ä¸ªåˆ†åŒº
                fallback_reason="current_partition_is_none"
            )

        # 1. åŸºç¡€é£é™©è¯„ä¼°ï¼šèŠ‚ç‚¹åº¦æ•°å’Œå½“å‰åˆ†åŒºå¤§å°
        node_degree = self._get_node_degree(node_id, hetero_data)

        # å®‰å…¨æ£€æŸ¥èŠ‚ç‚¹ID
        if node_id >= len(current_partition):
            return RiskAssessmentResult(
                risk_level='medium',
                risk_score=0.5,
                available_partitions=list(range(1, 4)),  # é»˜è®¤3ä¸ªåˆ†åŒº
                fallback_reason="invalid_node_id"
            )

        current_node_partition = current_partition[node_id].item()
        partition_size = (current_partition == current_node_partition).sum().item()

        # 2. è®¡ç®—é£é™©åˆ†æ•°
        risk_score = 0.0

        # é«˜åº¦æ•°èŠ‚ç‚¹é£é™©æ›´é«˜ï¼ˆå¯èƒ½æ˜¯å…³é”®è¿æ¥ç‚¹ï¼‰
        if node_degree > 3:
            risk_score += 0.3
        elif node_degree > 2:
            risk_score += 0.1

        # å°åˆ†åŒºä¸­çš„èŠ‚ç‚¹é£é™©æ›´é«˜
        if partition_size <= 2:
            risk_score += 0.4
        elif partition_size <= 3:
            risk_score += 0.2

        # 3. å†å²æ¨¡å¼åˆ†æ
        if node_id in self.history:
            recent_failures = sum(1 for result in self.history[node_id][-5:] if not result)
            if recent_failures > 2:
                risk_score += 0.3

        # 4. ç¡®å®šé£é™©ç­‰çº§å’Œå¯ç”¨åˆ†åŒº
        if risk_score < 0.2:
            risk_level = 'safe'
            # å®‰å…¨çº§åˆ«ï¼šæä¾›é‚»å±…åˆ†åŒºä½œä¸ºå¯ç”¨é€‰é¡¹
            available_partitions = self._get_neighbor_partitions(node_id, current_partition, hetero_data)
        elif risk_score < 0.4:
            risk_level = 'low'
            # ä½é£é™©ï¼šæä¾›é‚»å±…åˆ†åŒº
            available_partitions = self._get_neighbor_partitions(node_id, current_partition, hetero_data)
        elif risk_score < 0.7:
            risk_level = 'medium'
            # ä¸­ç­‰é£é™©ï¼šåªæä¾›å½“å‰åˆ†åŒºçš„é‚»å±…åˆ†åŒº
            available_partitions = self._get_neighbor_partitions(node_id, current_partition, hetero_data)
        else:
            risk_level = 'high'
            # é«˜é£é™©ï¼šä»ç„¶æä¾›é€‰é¡¹ï¼Œä½†æ ‡è®°ä¸ºé«˜é£é™©
            available_partitions = self._get_neighbor_partitions(node_id, current_partition, hetero_data)

        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå¯ç”¨åˆ†åŒº
        if not available_partitions:
            # å¦‚æœæ²¡æœ‰é‚»å±…åˆ†åŒºï¼Œè‡³å°‘æä¾›é™¤å½“å‰åˆ†åŒºå¤–çš„å…¶ä»–åˆ†åŒº
            num_partitions = int(current_partition.max().item())
            current_partition_id = current_node_partition
            available_partitions = [p for p in range(1, num_partitions + 1) if p != current_partition_id]
            if not available_partitions:
                available_partitions = list(range(1, num_partitions + 1))

        return RiskAssessmentResult(
            risk_level=risk_level,
            risk_score=risk_score,
            available_partitions=available_partitions,
            fallback_reason=None
        )

    def _get_neighbor_partitions(self, node_id: int, current_partition: torch.Tensor, hetero_data: Any) -> List[int]:
        """
        è·å–èŠ‚ç‚¹é‚»å±…æ‰€åœ¨çš„åˆ†åŒºåˆ—è¡¨

        Args:
            node_id: èŠ‚ç‚¹ID
            current_partition: å½“å‰åˆ†åŒºçŠ¶æ€
            hetero_data: å›¾æ•°æ®

        Returns:
            é‚»å±…åˆ†åŒºIDåˆ—è¡¨
        """
        neighbor_partitions = set()

        try:
            if hetero_data and 'bus' in hetero_data:
                # ä»å¼‚æ„å›¾æ•°æ®ä¸­è·å–è¾¹ä¿¡æ¯
                edge_data = None
                for edge_type in hetero_data.edge_types:
                    if edge_type[0] == 'bus' and edge_type[2] == 'bus':
                        edge_data = hetero_data[edge_type]
                        break

                if edge_data is not None and hasattr(edge_data, 'edge_index'):
                    edge_index = edge_data.edge_index
                    # æ‰¾åˆ°ä¸node_idç›¸è¿çš„æ‰€æœ‰èŠ‚ç‚¹
                    neighbors = []
                    # å‡ºè¾¹
                    out_neighbors = edge_index[1][edge_index[0] == node_id]
                    # å…¥è¾¹
                    in_neighbors = edge_index[0][edge_index[1] == node_id]
                    neighbors = torch.cat([out_neighbors, in_neighbors]).unique()

                    # è·å–é‚»å±…èŠ‚ç‚¹çš„åˆ†åŒº
                    for neighbor in neighbors:
                        neighbor_id = neighbor.item()
                        if neighbor_id < len(current_partition):
                            partition_id = current_partition[neighbor_id].item()
                            if partition_id > 0:  # æœ‰æ•ˆåˆ†åŒº
                                neighbor_partitions.add(partition_id)
        except Exception as e:
            logger.debug(f"Error getting neighbor partitions for node {node_id}: {e}")

        # ç§»é™¤å½“å‰èŠ‚ç‚¹æ‰€åœ¨çš„åˆ†åŒº
        current_node_partition = current_partition[node_id].item()
        neighbor_partitions.discard(current_node_partition)

        return list(neighbor_partitions)

        return RiskAssessmentResult(
            risk_level=risk_level,
            risk_score=risk_score,
            available_partitions=[],  # å°†åœ¨åç»­å¡«å……
            fallback_reason=None
        )

    def _get_node_degree(self, node_id: int, hetero_data: Any) -> int:
        """è·å–èŠ‚ç‚¹åº¦æ•°"""
        try:
            edge_index = hetero_data['bus', 'connects_to', 'bus'].edge_index
            degree = (edge_index[0] == node_id).sum() + (edge_index[1] == node_id).sum()
            return degree.item()
        except:
            return 1  # é»˜è®¤åº¦æ•°

    def update_history(self, node_id: int, success: bool):
        """æ›´æ–°èŠ‚ç‚¹å†å²å†³ç­–ç»“æœ"""
        self.history[node_id].append(success)
        # åªä¿ç•™æœ€è¿‘10æ¬¡è®°å½•
        if len(self.history[node_id]) > 10:
            self.history[node_id] = self.history[node_id][-10:]


class EnhancedPPOAgent(PPOAgent):
    """
    å¢å¼ºçš„PPOæ™ºèƒ½ä½“ - æ”¯æŒåŠ¨ä½œåµŒå…¥
    
    ç»§æ‰¿è‡ªåŸPPOAgentï¼Œé‡å†™å…³é”®æ–¹æ³•ä»¥æ”¯æŒåŒå¡”æ¶æ„
    """
    
    def __init__(self,
                 state_dim: int,
                 num_partitions: int,
                 config: Dict[str, Any],
                 device: str = 'auto'):
        """
        åˆå§‹åŒ–å¢å¼ºçš„PPOæ™ºèƒ½ä½“

        Args:
            state_dim: çŠ¶æ€ç»´åº¦
            num_partitions: åˆå§‹åˆ†åŒºæ•°
            config: é…ç½®å­—å…¸ï¼Œéœ€åŒ…å«:
                - node_dim: èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
                - edge_dim: è¾¹ç‰¹å¾ç»´åº¦
                - gnn_hidden: GNNéšè—å±‚ç»´åº¦
                - strategy_vector_dim: ç­–ç•¥å‘é‡ç»´åº¦
                - partition_encoder: åˆ†åŒºç¼–ç å™¨é…ç½®
                - use_two_tower: æ˜¯å¦ä½¿ç”¨åŒå¡”æ¶æ„
            device: è®¡ç®—è®¾å¤‡
        """
        # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨å®é™…çš„çŠ¶æ€ç»´åº¦ï¼Œè€Œä¸æ˜¯é…ç½®ä¸­çš„å›ºå®šå€¼
        # state_dimæ˜¯ä»ç¯å¢ƒå®é™…è·å–çš„èŠ‚ç‚¹åµŒå…¥ç»´åº¦ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒæ¨¡å¼
        node_embedding_dim = state_dim  # ä½¿ç”¨å®é™…çš„èŠ‚ç‚¹åµŒå…¥ç»´åº¦

        # åŒºåŸŸåµŒå…¥ç»´åº¦ = 2 * å®é™…èŠ‚ç‚¹åµŒå…¥ç»´åº¦
        region_embedding_dim = 2 * node_embedding_dim
        
        # å¤„ç†è®¾å¤‡å‚æ•°
        if isinstance(device, str):
            if device == 'auto':
                device_obj = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            else:
                device_obj = torch.device(device)
        else:
            device_obj = device
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œä¼ é€’æ­£ç¡®çš„å‚æ•°
        super().__init__(
            node_embedding_dim=node_embedding_dim,
            region_embedding_dim=region_embedding_dim,
            num_partitions=num_partitions,
            agent_config=config,  # ä¼ é€’æ•´ä¸ªconfigä½œä¸ºagent_config
            device=device_obj
        )

        # ã€æ–°å¢ã€‘åˆå§‹åŒ–é¢„æµ‹æ€§å›é€€æœºåˆ¶ç»„ä»¶
        self.connectivity_cache = ConnectivityCache(
            max_size=config.get('cache_size', 1000),
            ttl_seconds=config.get('cache_ttl', 300)
        )
        self.risk_predictor = RiskPredictor(device_obj)

        # ã€æ–°å¢ã€‘è‡ªé€‚åº”çº¦æŸå‚æ•°
        self.adaptive_constraint_config = config.get('adaptive_constraints', {
            'enabled': True,
            'initial_strictness': 0.8,
            'min_strictness': 0.3,
            'max_strictness': 1.0,
            'adjustment_rate': 0.05,
            'evaluation_window': 50,  # è¯„ä¼°çª—å£å¤§å°
            'success_threshold': 0.6,  # æˆåŠŸç‡é˜ˆå€¼
            'connectivity_threshold': 0.5  # è¿é€šæ€§é˜ˆå€¼
        })
        self.current_constraint_strictness = self.adaptive_constraint_config['initial_strictness']

        # ã€æ–°å¢ã€‘æ€§èƒ½ç›‘æ§ç»„ä»¶
        self.performance_monitor = PerformanceMonitor(
            window_size=self.adaptive_constraint_config['evaluation_window']
        )
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨åŒå¡”æ¶æ„ï¼ˆçˆ¶ç±»å·²ç»åˆ›å»ºäº†åŒå¡”æ¶æ„ï¼‰
        self.use_two_tower = config.get('use_two_tower', True)
        self.num_partitions = num_partitions

        # ã€è°ƒè¯•ã€‘æ‰“å°åŒå¡”æ¶æ„ä¿¡æ¯
        if self.use_two_tower and hasattr(self, 'actor') and hasattr(self, 'partition_encoder'):
            print(f"ğŸ—ï¸ EnhancedPPOAgentåŒå¡”æ¶æ„é…ç½®:")
            print(f"   âœ… State Tower (Actor): {type(self.actor).__name__}")
            print(f"   âœ… Action Tower (PartitionEncoder): {type(self.partition_encoder).__name__}")
            print(f"   ğŸ“ Strategy Vector Dim: {self.strategy_vector_dim}")
            print(f"   ğŸ”— Node Embedding Dim: {self.node_embedding_dim}")
            print(f"   ğŸ”— Region Embedding Dim: {self.region_embedding_dim}")
            print(f"   ğŸ¯ Partitions: {self.num_partitions}")
            logger.info("EnhancedPPOAgent using two-tower architecture from parent class")
        else:
            print(f"âš ï¸  EnhancedPPOAgentæ ‡å‡†æ¶æ„é…ç½®:")
            print(f"   node_embedding_dim: {self.node_embedding_dim}")
            print(f"   region_embedding_dim: {self.region_embedding_dim}")
            print(f"   state_dim: {state_dim}")
            print(f"   num_partitions: {self.num_partitions}")
            logger.info("EnhancedPPOAgent using standard architecture")
    
    def select_action(self,
                     state: Dict[str, torch.Tensor],
                     training: bool = True,
                     return_embeddings: bool = False) -> Tuple[Optional[Tuple[int, int]], float, float, Optional[Dict]]:
        """
        ä½¿ç”¨åŠ¨ä½œåµŒå…¥æœºåˆ¶é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: çŠ¶æ€å­—å…¸ï¼Œéœ€åŒ…å«:
                - available_partitions: æ¯ä¸ªèŠ‚ç‚¹çš„å¯ç”¨åˆ†åŒºä¿¡æ¯
                - partition_features: åˆ†åŒºç‰¹å¾ä¿¡æ¯
            training: æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼
            return_embeddings: æ˜¯å¦è¿”å›åµŒå…¥å‘é‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        
        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ (node_idx, partition_idx)
            log_prob: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            value: çŠ¶æ€ä»·å€¼ä¼°è®¡
            embeddings: åµŒå…¥ä¿¡æ¯ï¼ˆå¦‚æœreturn_embeddings=Trueï¼‰
        """
        # å¢å¼ºæ™ºèƒ½ä½“å§‹ç»ˆä½¿ç”¨åŒå¡”æ¶æ„
        
        with torch.no_grad():
            # 1. å‡†å¤‡æ‰¹å¤„ç†çŠ¶æ€
            batched_state = self._prepare_batched_state([state])
            
            # 2. è·å–ä»·å€¼ä¼°è®¡
            value = self.critic(batched_state).item()
            
            # 3. æ£€æŸ¥è¾¹ç•ŒèŠ‚ç‚¹
            if batched_state.boundary_nodes.numel() == 0:
                return None, 0.0, value, None
            
            # 4. è·å–èŠ‚ç‚¹logitså’Œç­–ç•¥å‘é‡
            node_logits, strategy_vectors = self.actor(batched_state)
            
            # 5. èŠ‚ç‚¹é€‰æ‹©
            # è·å–æ¯ä¸ªè¾¹ç•ŒèŠ‚ç‚¹æ˜¯å¦æœ‰æœ‰æ•ˆåˆ†åŒº
            boundary_mask = batched_state.action_mask[batched_state.boundary_nodes]
            valid_node_mask = boundary_mask.any(dim=1)
            node_logits = node_logits.masked_fill(~valid_node_mask, -1e9)
            
            if training:
                node_probs = F.softmax(node_logits, dim=0)
                node_dist = torch.distributions.Categorical(probs=node_probs)
                node_action_idx = node_dist.sample()
            else:
                node_action_idx = torch.argmax(node_logits)
            
            # 6. è·å–é€‰ä¸­èŠ‚ç‚¹çš„ç­–ç•¥å‘é‡
            selected_strategy_vector = strategy_vectors[node_action_idx]
            selected_node = batched_state.boundary_nodes[node_action_idx].item()
            
            # 7. è·å–è¯¥èŠ‚ç‚¹çš„å¯ç”¨åˆ†åŒºä¿¡æ¯ï¼ˆå››å±‚æ™ºèƒ½çº¦æŸå¤„ç†ï¼‰
            risk_result = self._get_available_partitions_with_risk_assessment(
                state, selected_node, batched_state
            )
            available_partitions = risk_result.available_partitions
            
            # 8. ç¼–ç å¯ç”¨åˆ†åŒº
            partition_embeddings = self._encode_partitions(
                state, available_partitions
            )
            
            # 9. è®¡ç®—ç›¸ä¼¼åº¦å¹¶é€‰æ‹©åˆ†åŒº
            similarities = self.partition_encoder.compute_similarity(
                selected_strategy_vector,
                partition_embeddings,
                temperature=1.0  # å¯ä»¥ä»configä¸­è·å–
            )
            
            # 10. åŸºäºç›¸ä¼¼åº¦é€‰æ‹©åˆ†åŒº
            if training:
                partition_probs = F.softmax(similarities, dim=0)
                partition_dist = torch.distributions.Categorical(probs=partition_probs)
                partition_idx = partition_dist.sample()
                
                # è®¡ç®—æ€»çš„log_prob
                node_log_prob = node_dist.log_prob(node_action_idx)
                partition_log_prob = partition_dist.log_prob(partition_idx)
                log_prob = (node_log_prob + partition_log_prob).item()
            else:
                partition_idx = torch.argmax(similarities)
                log_prob = 0.0
            
            # 11. è½¬æ¢ä¸ºå®é™…çš„åˆ†åŒºID
            selected_partition = available_partitions[partition_idx.item()]

            # ç¡®ä¿åˆ†åŒºIDæ˜¯æ•´æ•°è€Œä¸æ˜¯å¼ é‡
            if isinstance(selected_partition, torch.Tensor):
                selected_partition = selected_partition.item()

            action = (selected_node, selected_partition)
            
            # å‡†å¤‡è¿”å›çš„åµŒå…¥ä¿¡æ¯
            embeddings = None
            if return_embeddings:
                embeddings = {
                    'strategy_vector': selected_strategy_vector.cpu().numpy(),
                    'partition_embeddings': partition_embeddings.cpu().numpy(),
                    'similarities': similarities.cpu().numpy(),
                    'node_logits': node_logits.cpu().numpy()
                }
            
            return action, log_prob, value, embeddings
    
    def _get_available_partitions(self, 
                                 state: Dict,
                                 node_id: int,
                                 batched_state: BatchedState) -> List[int]:
        """
        è·å–èŠ‚ç‚¹çš„å¯ç”¨åˆ†åŒºåˆ—è¡¨ï¼ˆç¡®ä¿è¿é€šæ€§ï¼‰
        
        Args:
            state: åŸå§‹çŠ¶æ€å­—å…¸
            node_id: èŠ‚ç‚¹ID
            batched_state: æ‰¹å¤„ç†çŠ¶æ€
        
        Returns:
            å¯ç”¨åˆ†åŒºIDåˆ—è¡¨
        """
        # ä»ç¯å¢ƒæä¾›çš„ä¿¡æ¯ä¸­è·å–
        if 'connectivity_safe_partitions' in state:
            node_safe_partitions = state['connectivity_safe_partitions'].get(node_id, [])
            return node_safe_partitions
        
        # ä½¿ç”¨action_mask
        if hasattr(batched_state, 'action_mask'):
            # æ‰¾åˆ°èŠ‚ç‚¹åœ¨boundary_nodesä¸­çš„ä½ç½®
            node_positions = (batched_state.boundary_nodes == node_id).nonzero(as_tuple=True)[0]
            if len(node_positions) > 0:
                node_pos = node_positions[0]
                # ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨node_idä½œä¸ºaction_maskçš„ç´¢å¼•ï¼Œè€Œä¸æ˜¯boundary_nodes[node_pos]
                # å› ä¸ºaction_maskçš„ç¬¬ä¸€ç»´æ˜¯æŒ‰èŠ‚ç‚¹IDæ’åˆ—çš„ï¼Œä¸æ˜¯æŒ‰boundary_nodesä½ç½®æ’åˆ—çš„
                if node_id < batched_state.action_mask.size(0):
                    mask = batched_state.action_mask[node_id]
                    available = torch.where(mask)[0].cpu().tolist()
                    return [p + 1 for p in available]  # è½¬æ¢ä¸º1-indexed
                else:
                    logger.warning(f"èŠ‚ç‚¹ID {node_id} è¶…å‡ºaction_maskèŒƒå›´ {batched_state.action_mask.size(0)}")
                    return []
        
        # é»˜è®¤è¿”å›æ‰€æœ‰åˆ†åŒº
        return list(range(1, self.num_partitions + 1))

    def _get_action_mask_partitions(self,
                                   node_id: int,
                                   batched_state: BatchedState) -> List[int]:
        """
        ä½¿ç”¨action_maskè·å–èŠ‚ç‚¹çš„å¯ç”¨åˆ†åŒºï¼ˆä¸åŸå§‹PPOAgentä¸€è‡´ï¼‰

        Args:
            node_id: èŠ‚ç‚¹ID
            batched_state: æ‰¹å¤„ç†çŠ¶æ€

        Returns:
            å¯ç”¨åˆ†åŒºIDåˆ—è¡¨
        """
        if hasattr(batched_state, 'action_mask') and batched_state.action_mask is not None:
            # æ£€æŸ¥action_maskçš„ç»´åº¦
            if batched_state.action_mask.dim() == 2:
                # äºŒç»´mask: [num_nodes, num_partitions]
                if node_id < batched_state.action_mask.size(0):
                    mask = batched_state.action_mask[node_id]
                    available = torch.where(mask)[0].cpu().tolist()
                    return [p + 1 for p in available]  # è½¬æ¢ä¸º1-indexed
            else:
                # ä¸€ç»´maskæˆ–å…¶ä»–æ ¼å¼ï¼Œå°è¯•æ‰¾åˆ°èŠ‚ç‚¹åœ¨boundary_nodesä¸­çš„ä½ç½®
                node_positions = (batched_state.boundary_nodes == node_id).nonzero(as_tuple=True)[0]
                if len(node_positions) > 0:
                    node_pos = node_positions[0]
                    # è·å–è¯¥èŠ‚ç‚¹çš„action_mask
                    if node_pos < batched_state.action_mask.size(0):
                        mask = batched_state.action_mask[node_pos]
                        available = torch.where(mask)[0].cpu().tolist()
                        return [p + 1 for p in available]  # è½¬æ¢ä¸º1-indexed

        # å¦‚æœaction_maskä¸å¯ç”¨ï¼Œè¿”å›é»˜è®¤åˆ†åŒºï¼ˆé™¤å½“å‰åˆ†åŒºå¤–ï¼‰
        if hasattr(batched_state, 'partition_assignment') and batched_state.partition_assignment is not None:
            current_partition = batched_state.partition_assignment[node_id].item()
            num_partitions = int(batched_state.partition_assignment.max().item())
            return [p for p in range(1, num_partitions + 1) if p != current_partition]

        # æœ€åçš„å›é€€ï¼šè¿”å›é»˜è®¤åˆ†åŒº
        return [1, 2, 3]

    def _get_available_partitions_with_risk_assessment(self,
                                                      state: Dict,
                                                      node_id: int,
                                                      batched_state: BatchedState) -> RiskAssessmentResult:
        """
        å››å±‚æ™ºèƒ½çº¦æŸå¤„ç†æœºåˆ¶

        å±‚æ¬¡ç»“æ„ï¼š
        1. è¿é€šæ€§å®‰å…¨åˆ†åŒºï¼ˆæœ€ä¸¥æ ¼ï¼Œé¿å…å‰²ç‚¹é—®é¢˜ï¼‰
        2. é£é™©è¯„ä¼°å±‚ï¼ˆåŸºäºé¢„æµ‹çš„ä¸­ç­‰é£é™©åˆ†åŒºï¼‰
        3. Action Maskå›é€€ï¼ˆä¸åŸå§‹PPOAgentä¸€è‡´ï¼‰
        4. è½¯çº¦æŸæ¨¡å¼ï¼ˆç¡®ä¿ç³»ç»Ÿä¸å¡æ­»ï¼‰

        Args:
            state: åŸå§‹çŠ¶æ€å­—å…¸
            node_id: èŠ‚ç‚¹ID
            batched_state: æ‰¹å¤„ç†çŠ¶æ€

        Returns:
            é£é™©è¯„ä¼°ç»“æœï¼ŒåŒ…å«å¯ç”¨åˆ†åŒºå’Œé£é™©ä¿¡æ¯
        """
        # ç¬¬ä¸€å±‚ï¼šè¿é€šæ€§å®‰å…¨åˆ†åŒºï¼ˆæœ€ä¼˜é€‰æ‹©ï¼‰
        safe_partitions = self._get_connectivity_safe_partitions(state, node_id, batched_state)

        if safe_partitions:
            return RiskAssessmentResult(
                risk_level='safe',
                risk_score=0.0,
                available_partitions=safe_partitions,
                fallback_reason=None,
                cache_hit=False
            )

        # ç¬¬äºŒå±‚ï¼šé£é™©è¯„ä¼°å±‚ï¼ˆæ™ºèƒ½é¢„æµ‹ï¼‰
        risk_result = self.risk_predictor.predict_risk(
            node_id,
            batched_state.partition_assignment if hasattr(batched_state, 'partition_assignment') else state.get('partition_assignment'),
            batched_state.boundary_nodes,
            state.get('hetero_data')
        )

        # åŸºäºé£é™©ç­‰çº§è·å–åˆ†åŒº
        if risk_result.risk_level in ['safe', 'low']:
            # ä½é£é™©ï¼šä½¿ç”¨ç¼“å­˜ä¼˜åŒ–çš„è¿é€šæ€§æ£€æŸ¥
            risk_assessed_partitions = self._get_risk_assessed_partitions(
                state, node_id, batched_state, risk_result.risk_level
            )
            if risk_assessed_partitions:
                risk_result.available_partitions = risk_assessed_partitions
                risk_result.fallback_reason = f"risk_assessment_{risk_result.risk_level}"
                return risk_result

        # ç¬¬ä¸‰å±‚ï¼šAction Maskå›é€€ï¼ˆä¸åŸå§‹PPOAgentä¸€è‡´ï¼‰
        logger.warning(f"No safe/low-risk partitions for node {node_id}, falling back to action_mask")
        action_mask_partitions = self._get_action_mask_partitions(node_id, batched_state)

        if action_mask_partitions:
            return RiskAssessmentResult(
                risk_level='medium',
                risk_score=0.6,
                available_partitions=action_mask_partitions,
                fallback_reason="action_mask_fallback",
                cache_hit=False
            )

        # ç¬¬å››å±‚ï¼šè½¯çº¦æŸæ¨¡å¼ï¼ˆæœ€åä¿éšœï¼‰
        logger.warning(f"All fallbacks failed for node {node_id}, using soft constraint mode")
        all_partitions = list(range(1, self.num_partitions + 1))

        return RiskAssessmentResult(
            risk_level='high',
            risk_score=0.9,
            available_partitions=all_partitions,
            fallback_reason="soft_constraint_fallback",
            cache_hit=False
        )

    def _get_connectivity_safe_partitions(self,
                                        state: Dict,
                                        node_id: int,
                                        batched_state: BatchedState) -> List[int]:
        """è·å–è¿é€šæ€§å®‰å…¨çš„åˆ†åŒºï¼ˆç¬¬ä¸€å±‚ï¼‰"""
        # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒæä¾›çš„å®‰å…¨åˆ†åŒºä¿¡æ¯
        if 'connectivity_safe_partitions' in state:
            safe_partitions = state['connectivity_safe_partitions'].get(node_id, [])
            if safe_partitions:
                return safe_partitions

        # å¦‚æœç¯å¢ƒæ²¡æœ‰æä¾›ï¼Œå°è¯•ä»ç¯å¢ƒå¯¹è±¡ç›´æ¥è·å–
        if hasattr(state, 'env') and hasattr(state['env'], 'get_connectivity_safe_partitions'):
            try:
                safe_partitions = state['env'].get_connectivity_safe_partitions(node_id)
                if safe_partitions:
                    return safe_partitions
            except Exception as e:
                logger.debug(f"Failed to get safe partitions from env: {e}")

        # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—çš„å®‰å…¨åˆ†åŒºï¼Œè¿”å›ç©ºåˆ—è¡¨
        # è¿™å°†è§¦å‘é£é™©è¯„ä¼°å±‚
        return []

    def _get_risk_assessed_partitions(self,
                                    state: Dict,
                                    node_id: int,
                                    batched_state: BatchedState,
                                    risk_level: str) -> List[int]:
        """
        åŸºäºé£é™©è¯„ä¼°è·å–åˆ†åŒºï¼ˆç¬¬äºŒå±‚ï¼‰
        ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–çš„è¿é€šæ€§æ£€æŸ¥
        """
        available_partitions = []

        # è·å–åŸºç¡€å¯ç”¨åˆ†åŒºï¼ˆæ¥è‡ªaction_maskï¼‰
        base_partitions = self._get_action_mask_partitions(node_id, batched_state)

        if not base_partitions:
            return []

        # æ ¹æ®é£é™©ç­‰çº§å†³å®šæ£€æŸ¥ç­–ç•¥
        if risk_level == 'safe':
            # å®‰å…¨çº§åˆ«ï¼šæ£€æŸ¥æ‰€æœ‰åˆ†åŒº
            partitions_to_check = base_partitions
        else:  # 'low'
            # ä½é£é™©çº§åˆ«ï¼šåªæ£€æŸ¥å‰å‡ ä¸ªåˆ†åŒºï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
            partitions_to_check = base_partitions[:min(3, len(base_partitions))]

        # ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–çš„è¿é€šæ€§æ£€æŸ¥
        current_partition = (batched_state.partition_assignment
                           if hasattr(batched_state, 'partition_assignment')
                           else state.get('partition_assignment'))

        for target_partition in partitions_to_check:
            # æ£€æŸ¥ç¼“å­˜
            cached_result = self.connectivity_cache.get(current_partition, node_id, target_partition)

            if cached_result is not None:
                # ç¼“å­˜å‘½ä¸­
                if not cached_result.get('violates_connectivity', True):
                    available_partitions.append(target_partition)
            else:
                # ç¼“å­˜æœªå‘½ä¸­ï¼Œè¿›è¡Œå®é™…è¿é€šæ€§æ£€æŸ¥
                violation_result = self._perform_connectivity_check(
                    current_partition, node_id, target_partition, state
                )

                # ç¼“å­˜ç»“æœ
                self.connectivity_cache.put(
                    current_partition,
                    node_id,
                    target_partition,
                    violation_result
                )

                # å¦‚æœä¸è¿åè¿é€šæ€§ï¼Œæ·»åŠ åˆ°å¯ç”¨åˆ†åŒº
                if not violation_result.get('violates_connectivity', True):
                    available_partitions.append(target_partition)

        return available_partitions

    def update_constraint_strictness(self, episode_info: Dict[str, Any] = None):
        """
        æ™ºèƒ½è‡ªé€‚åº”çº¦æŸè°ƒèŠ‚æœºåˆ¶

        Args:
            episode_info: å•ä¸ªepisodeçš„ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œç”¨äºè®°å½•ï¼‰
        """
        if not self.adaptive_constraint_config['enabled']:
            return

        # è®°å½•episodeæ•°æ®
        if episode_info:
            self.performance_monitor.record_episode(episode_info)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´
        should_adjust, reason = self.performance_monitor.should_adjust_constraints(
            self.adaptive_constraint_config['success_threshold'],
            self.adaptive_constraint_config['connectivity_threshold']
        )

        if not should_adjust:
            return

        # è·å–å½“å‰æ€§èƒ½æŒ‡æ ‡
        metrics = self.performance_monitor.get_performance_metrics()
        old_strictness = self.current_constraint_strictness
        adjustment_rate = self.adaptive_constraint_config['adjustment_rate']

        # æ ¹æ®åŸå› è°ƒæ•´çº¦æŸä¸¥æ ¼ç¨‹åº¦
        if reason == "relax_low_success":
            # æˆåŠŸç‡è¿‡ä½ï¼Œå¤§å¹…æ”¾æ¾çº¦æŸ
            self.current_constraint_strictness -= adjustment_rate * 2.0
            logger.info(f"ğŸ”§ æ”¾æ¾çº¦æŸ (æˆåŠŸç‡è¿‡ä½: {metrics['success_rate']:.2f})")

        elif reason == "relax_high_violation":
            # è¿è§„ç‡è¿‡é«˜ï¼Œä¸­ç­‰æ”¾æ¾çº¦æŸ
            self.current_constraint_strictness -= adjustment_rate * 1.5
            logger.info(f"ğŸ”§ æ”¾æ¾çº¦æŸ (è¿è§„ç‡è¿‡é«˜: {metrics['constraint_violation_rate']:.2f})")

        elif reason == "tighten_low_connectivity":
            # è¿é€šæ€§è¿‡ä½ï¼Œä¸­ç­‰åŠ å¼ºçº¦æŸ
            self.current_constraint_strictness += adjustment_rate * 1.5
            logger.info(f"ğŸ”§ åŠ å¼ºçº¦æŸ (è¿é€šæ€§è¿‡ä½: {metrics['avg_connectivity']:.2f})")

        elif reason == "tighten_good_performance":
            # è¡¨ç°è‰¯å¥½ï¼Œå°å¹…åŠ å¼ºçº¦æŸ
            self.current_constraint_strictness += adjustment_rate * 0.5
            logger.info(f"ğŸ”§ æ¸è¿›åŠ å¼ºçº¦æŸ (è¡¨ç°è‰¯å¥½)")

        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        self.current_constraint_strictness = max(
            self.adaptive_constraint_config['min_strictness'],
            min(self.adaptive_constraint_config['max_strictness'], self.current_constraint_strictness)
        )

        # è®°å½•è°ƒæ•´ä¿¡æ¯
        if abs(self.current_constraint_strictness - old_strictness) > 0.01:
            logger.info(f"ğŸ“Š çº¦æŸä¸¥æ ¼ç¨‹åº¦: {old_strictness:.3f} â†’ {self.current_constraint_strictness:.3f}")
            logger.debug(f"   åŸå› : {reason}")
            logger.debug(f"   æ€§èƒ½æŒ‡æ ‡: æˆåŠŸç‡={metrics['success_rate']:.2f}, "
                        f"è¿é€šæ€§={metrics['avg_connectivity']:.2f}, "
                        f"è¿è§„ç‡={metrics['constraint_violation_rate']:.2f}")

    def get_adaptive_constraint_info(self) -> Dict[str, Any]:
        """è·å–è‡ªé€‚åº”çº¦æŸçš„å½“å‰çŠ¶æ€ä¿¡æ¯"""
        metrics = self.performance_monitor.get_performance_metrics()

        return {
            'current_strictness': self.current_constraint_strictness,
            'config': self.adaptive_constraint_config,
            'performance_metrics': metrics,
            'total_episodes': self.performance_monitor.current_episode,
            'recent_episodes': len(self.performance_monitor.episode_data)
        }

    def _perform_connectivity_check(self,
                                  current_partition: torch.Tensor,
                                  node_id: int,
                                  target_partition: int,
                                  state: Dict) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®é™…çš„è¿é€šæ€§æ£€æŸ¥

        Args:
            current_partition: å½“å‰åˆ†åŒºçŠ¶æ€
            node_id: è¦ç§»åŠ¨çš„èŠ‚ç‚¹ID
            target_partition: ç›®æ ‡åˆ†åŒºID
            state: çŠ¶æ€å­—å…¸ï¼ŒåŒ…å«å›¾ç»“æ„ä¿¡æ¯

        Returns:
            è¿é€šæ€§æ£€æŸ¥ç»“æœå­—å…¸
        """
        try:
            # å°è¯•ä½¿ç”¨ç¯å¢ƒçš„action_spaceè¿›è¡Œè¿é€šæ€§æ£€æŸ¥
            if 'env' in state and hasattr(state['env'], 'action_space'):
                env = state['env']
                if hasattr(env.action_space, 'mask_handler'):
                    action = (node_id, target_partition)
                    return env.action_space.mask_handler.check_connectivity_violation(
                        current_partition, action
                    )

            # å›é€€åˆ°ç®€åŒ–çš„è¿é€šæ€§æ£€æŸ¥
            return self._simplified_connectivity_check(
                current_partition, node_id, target_partition, state
            )

        except Exception as e:
            logger.warning(f"è¿é€šæ€§æ£€æŸ¥å¤±è´¥: {e}")
            # å¤±è´¥æ—¶å‡è®¾è¿åè¿é€šæ€§ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
            return {
                'violates_connectivity': True,
                'violation_severity': 0.5,
                'isolated_nodes_count': 1,
                'affected_partition_size': 1
            }

    def _simplified_connectivity_check(self,
                                     current_partition: torch.Tensor,
                                     node_id: int,
                                     target_partition: int,
                                     state: Dict) -> Dict[str, Any]:
        """
        ç®€åŒ–çš„è¿é€šæ€§æ£€æŸ¥ï¼ˆå½“æ— æ³•è®¿é—®å®Œæ•´çš„mask_handleræ—¶ä½¿ç”¨ï¼‰

        åŸºäºå¯å‘å¼è§„åˆ™ï¼š
        1. å¦‚æœåŸåˆ†åŒºåªæœ‰1-2ä¸ªèŠ‚ç‚¹ï¼Œç§»åŠ¨æ˜¯å®‰å…¨çš„
        2. å¦‚æœèŠ‚ç‚¹åº¦æ•°å¾ˆä½ï¼Œç§»åŠ¨é£é™©è¾ƒå°
        3. å…¶ä»–æƒ…å†µä¿å®ˆå¤„ç†
        """
        current_node_partition = current_partition[node_id].item()

        # è®¡ç®—åŸåˆ†åŒºå¤§å°
        partition_size = (current_partition == current_node_partition).sum().item()

        # å°åˆ†åŒºç§»åŠ¨é€šå¸¸æ˜¯å®‰å…¨çš„
        if partition_size <= 2:
            return {
                'violates_connectivity': False,
                'violation_severity': 0.0,
                'isolated_nodes_count': 0,
                'affected_partition_size': partition_size
            }

        # è·å–èŠ‚ç‚¹åº¦æ•°ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        node_degree = 1  # é»˜è®¤åº¦æ•°
        if 'hetero_data' in state:
            try:
                hetero_data = state['hetero_data']
                edge_index = hetero_data['bus', 'connects_to', 'bus'].edge_index
                node_degree = (edge_index[0] == node_id).sum() + (edge_index[1] == node_id).sum()
                node_degree = node_degree.item()
            except:
                pass

        # åŸºäºåº¦æ•°å’Œåˆ†åŒºå¤§å°çš„å¯å‘å¼åˆ¤æ–­
        if node_degree <= 2 and partition_size >= 4:
            # ä½åº¦æ•°èŠ‚ç‚¹åœ¨å¤§åˆ†åŒºä¸­ç§»åŠ¨ç›¸å¯¹å®‰å…¨
            violation_severity = 0.2
        elif node_degree >= 4 and partition_size <= 4:
            # é«˜åº¦æ•°èŠ‚ç‚¹åœ¨å°åˆ†åŒºä¸­ç§»åŠ¨é£é™©è¾ƒé«˜
            violation_severity = 0.8
        else:
            # ä¸­ç­‰é£é™©
            violation_severity = 0.4

        return {
            'violates_connectivity': violation_severity > 0.5,
            'violation_severity': violation_severity,
            'isolated_nodes_count': 1 if violation_severity > 0.7 else 0,
            'affected_partition_size': partition_size
        }

    def _prepare_constraint_aware_state(self, state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        ä¸ºçº¦æŸæ„ŸçŸ¥ç­–ç•¥å‘é‡ç”Ÿæˆå‡†å¤‡å¢å¼ºçŠ¶æ€

        è¿™ä¸ªæ–¹æ³•ä¸ºåç»­çš„æ–¹æ¡ˆ1å®ç°é¢„ç•™æ¥å£ï¼Œå°†çº¦æŸä¿¡æ¯èå…¥çŠ¶æ€ç¼–ç 

        Args:
            state: åŸå§‹çŠ¶æ€å­—å…¸

        Returns:
            å¢å¼ºçš„çŠ¶æ€å­—å…¸ï¼ŒåŒ…å«çº¦æŸæ„ŸçŸ¥ä¿¡æ¯
        """
        enhanced_state = state.copy()

        # ã€æ‰©å±•ç‚¹1ã€‘æ·»åŠ çº¦æŸä¸¥æ ¼ç¨‹åº¦ä¿¡æ¯
        enhanced_state['constraint_strictness'] = torch.tensor(
            self.current_constraint_strictness,
            device=self.device,
            dtype=torch.float32
        )

        # ã€æ‰©å±•ç‚¹2ã€‘æ·»åŠ å†å²è¿è§„ä¿¡æ¯
        if hasattr(self, 'performance_monitor'):
            metrics = self.performance_monitor.get_performance_metrics()
            enhanced_state['violation_rate'] = torch.tensor(
                metrics.get('constraint_violation_rate', 0.0),
                device=self.device,
                dtype=torch.float32
            )
            enhanced_state['success_rate'] = torch.tensor(
                metrics.get('success_rate', 0.0),
                device=self.device,
                dtype=torch.float32
            )

        # ã€æ‰©å±•ç‚¹3ã€‘æ·»åŠ è¿é€šæ€§é£é™©é¢„æµ‹
        if 'boundary_nodes' in state:
            boundary_nodes = state['boundary_nodes']
            risk_scores = []

            for node in boundary_nodes:
                if torch.is_tensor(node):
                    node_id = node.item()
                else:
                    node_id = int(node)

                # ä½¿ç”¨é£é™©é¢„æµ‹å™¨è®¡ç®—é£é™©åˆ†æ•°
                risk_result = self.risk_predictor.predict_risk(
                    node_id,
                    state.get('partition_assignment', torch.zeros(1)),
                    boundary_nodes,
                    state.get('hetero_data')
                )
                risk_scores.append(risk_result.risk_score)

            enhanced_state['node_risk_scores'] = torch.tensor(
                risk_scores, device=self.device, dtype=torch.float32
            )

        return enhanced_state

    def _generate_constraint_aware_strategy_vector(self,
                                                  node_features: torch.Tensor,
                                                  constraint_context: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        ç”Ÿæˆçº¦æŸæ„ŸçŸ¥çš„ç­–ç•¥å‘é‡ï¼ˆæ–¹æ¡ˆ1çš„æ ¸å¿ƒæ¥å£ï¼‰

        è¿™ä¸ªæ–¹æ³•ä¸ºåç»­å®ç°çº¦æŸæ„ŸçŸ¥ç­–ç•¥å‘é‡ç”Ÿæˆé¢„ç•™æ¥å£

        Args:
            node_features: èŠ‚ç‚¹ç‰¹å¾
            constraint_context: çº¦æŸä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            çº¦æŸæ„ŸçŸ¥çš„ç­–ç•¥å‘é‡
        """
        # ã€å½“å‰å®ç°ã€‘ä½¿ç”¨ç°æœ‰çš„ç­–ç•¥ç”Ÿæˆå™¨
        # ã€æœªæ¥æ‰©å±•ã€‘è¿™é‡Œå°†èå…¥çº¦æŸä¿¡æ¯

        # æå–çº¦æŸç‰¹å¾
        constraint_features = []

        if 'constraint_strictness' in constraint_context:
            constraint_features.append(constraint_context['constraint_strictness'])

        if 'violation_rate' in constraint_context:
            constraint_features.append(constraint_context['violation_rate'])

        if 'success_rate' in constraint_context:
            constraint_features.append(constraint_context['success_rate'])

        # å¦‚æœæœ‰çº¦æŸç‰¹å¾ï¼Œå°†å…¶ä¸èŠ‚ç‚¹ç‰¹å¾ç»“åˆ
        if constraint_features:
            constraint_tensor = torch.stack(constraint_features).unsqueeze(0)
            # æ‰©å±•çº¦æŸç‰¹å¾ä»¥åŒ¹é…èŠ‚ç‚¹ç‰¹å¾çš„æ‰¹æ¬¡ç»´åº¦
            if node_features.dim() > 1:
                constraint_tensor = constraint_tensor.expand(node_features.size(0), -1)

            # çº¦æŸæ„ŸçŸ¥ç‰¹å¾èåˆï¼šå°†çº¦æŸä¿¡æ¯ä¸èŠ‚ç‚¹ç‰¹å¾ç»“åˆ
            enhanced_features = torch.cat([node_features, constraint_tensor], dim=-1)
        else:
            enhanced_features = node_features

        # çº¦æŸæ„ŸçŸ¥ç­–ç•¥ç”Ÿæˆ
        # å½“å‰ä½¿ç”¨ç°æœ‰çš„ç­–ç•¥ç”Ÿæˆå™¨ï¼ˆéœ€è¦é€‚é…è¾“å…¥ç»´åº¦ï¼‰
        return enhanced_features

    def get_constraint_aware_capabilities(self) -> Dict[str, bool]:
        """
        è·å–çº¦æŸæ„ŸçŸ¥èƒ½åŠ›çŠ¶æ€

        Returns:
            èƒ½åŠ›çŠ¶æ€å­—å…¸
        """
        return {
            'predictive_fallback': True,
            'connectivity_cache': True,
            'adaptive_constraints': self.adaptive_constraint_config['enabled'],
            'risk_prediction': True,
            'constraint_aware_state': True,  # æ¶æ„å‡†å¤‡å®Œæˆ
            'constraint_aware_strategy': False,  # å¾…å®ç°ï¼ˆæ–¹æ¡ˆ1ï¼‰
            'performance_monitoring': hasattr(self, 'performance_monitor')
        }
    
    def _encode_partitions(self, 
                          state: Dict,
                          partition_ids: List[int]) -> torch.Tensor:
        """
        ç¼–ç åˆ†åŒºç‰¹å¾
        
        Args:
            state: çŠ¶æ€å­—å…¸ï¼Œéœ€åŒ…å«partition_info
            partition_ids: è¦ç¼–ç çš„åˆ†åŒºIDåˆ—è¡¨
        
        Returns:
            åˆ†åŒºåµŒå…¥çŸ©é˜µ [num_partitions, embedding_dim]
        """
        # ä»çŠ¶æ€ä¸­è·å–åˆ†åŒºä¿¡æ¯
        partition_info = state.get('partition_info', {})
        grid_info = state.get('grid_info', {})
        
        # å‡†å¤‡åˆ†åŒºç‰¹å¾
        partitions_data = []
        for pid in partition_ids:
            if pid in partition_info:
                partitions_data.append(partition_info[pid])
            else:
                # åˆ›å»ºé»˜è®¤ç‰¹å¾
                partitions_data.append({
                    'nodes': [],
                    'load': 0.0,
                    'generation': 0.0,
                    'internal_edges': 0,
                    'boundary_nodes': 0
                })
        
        # ä½¿ç”¨åˆ†åŒºç¼–ç å™¨ç¼–ç 
        embeddings = self.partition_encoder.encode_partitions(
            partitions_data,
            grid_info,
            self.device
        )
        
        return embeddings
    
    def update_partition_cache(self, 
                             partition_changes: Dict[int, List[int]]):
        """
        æ›´æ–°åˆ†åŒºç¼“å­˜ï¼ˆå½“åˆ†åŒºå‘ç”Ÿå˜åŒ–æ—¶ï¼‰
        
        Args:
            partition_changes: å˜åŒ–çš„åˆ†åŒº {partition_id: affected_nodes}
        """
        if hasattr(self.partition_encoder, 'invalidate_cache'):
            self.partition_encoder.invalidate_cache(list(partition_changes.keys()))
    
    def get_strategy_vector(self, 
                           state: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        è·å–å½“å‰çŠ¶æ€çš„ç­–ç•¥å‘é‡ï¼ˆç”¨äºåˆ†æï¼‰
        
        Args:
            state: çŠ¶æ€è§‚å¯Ÿ
        
        Returns:
            æ‰€æœ‰è¾¹ç•ŒèŠ‚ç‚¹çš„ç­–ç•¥å‘é‡ [num_boundary_nodes, strategy_vector_dim]
        """
        if not self.use_two_tower:
            raise ValueError("Strategy vector only available in two-tower mode")
        
        with torch.no_grad():
            batched_state = self._prepare_batched_state([state])
            _, strategy_vectors = self.actor(batched_state)
            return strategy_vectors


def create_enhanced_agent(state_dim: int,
                         num_partitions: int,
                         config: Dict[str, Any],
                         device: str = 'auto') -> EnhancedPPOAgent:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºå¢å¼ºPPOæ™ºèƒ½ä½“

    Args:
        state_dim: çŠ¶æ€ç»´åº¦
        num_partitions: åˆ†åŒºæ•°
        config: é…ç½®å­—å…¸
        device: è®¾å¤‡

    Returns:
        å¢å¼ºPPOæ™ºèƒ½ä½“å®ä¾‹
    """
    return EnhancedPPOAgent(state_dim, num_partitions, config, device)