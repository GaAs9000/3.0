import torch
import numpy as np
from typing import Dict, Tuple, List, Optional, Union, Any
from torch_geometric.data import HeteroData
import copy
from .scenario_context import ScenarioContext

try:
    from code.src.rl.state import StateManager
    from code.src.rl.action_space import ActionSpace, ActionMask
    from code.src.rl.reward import RewardFunction
    from code.src.rl.utils import MetisInitializer, PartitionEvaluator
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        from code.src.rl.state import StateManager
        from code.src.rl.action_space import ActionSpace, ActionMask
        from code.src.rl.reward import RewardFunction
        from code.src.rl.utils import MetisInitializer, PartitionEvaluator
    except ImportError:
        print("è­¦å‘Šï¼šæ— æ³•å¯¼å…¥RLæ¨¡å—çš„æŸäº›ç»„ä»¶")


class PowerGridPartitioningEnv:
    """
    ç”µåŠ›ç½‘ç»œåˆ†å‰²MDPç¯å¢ƒ
    """

    def __init__(self,
                 hetero_data: HeteroData,
                 node_embeddings: Dict[str, torch.Tensor],
                 num_partitions: int,
                 reward_weights: Dict[str, float] = None,
                 max_steps: int = 200,
                 device: torch.device = None,
                 attention_weights: Dict[str, torch.Tensor] = None,
                 config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–ç”µåŠ›ç½‘ç»œåˆ†å‰²ç¯å¢ƒ

        å‚æ•°:
            hetero_data: æ¥è‡ªæ•°æ®å¤„ç†çš„å¼‚æ„å›¾æ•°æ®
            node_embeddings: GATç¼–ç å™¨é¢„è®¡ç®—çš„èŠ‚ç‚¹åµŒå…¥
            num_partitions: ç›®æ ‡åˆ†åŒºæ•°é‡ï¼ˆKï¼‰
            reward_weights: å¥–åŠ±ç»„ä»¶æƒé‡
            max_steps: æ¯ä¸ªå›åˆçš„æœ€å¤§æ­¥æ•°
            device: ç”¨äºè®¡ç®—çš„Torchè®¾å¤‡
            attention_weights: GATç¼–ç å™¨æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºå¢å¼ºåµŒå…¥
            config: é…ç½®å­—å…¸ï¼Œç”¨äºæ§åˆ¶è¾“å‡ºè¯¦ç»†ç¨‹åº¦
        """
        self.device = device or torch.device('cpu')
        self.hetero_data = hetero_data.to(self.device)
        self.num_partitions = num_partitions
        self.max_steps = max_steps
        self.config = config

        # ç”Ÿæˆå¢å¼ºçš„èŠ‚ç‚¹åµŒå…¥ï¼ˆå¦‚æœæä¾›äº†æ³¨æ„åŠ›æƒé‡ï¼‰
        enhanced_embeddings = self._generate_enhanced_embeddings(
            node_embeddings, attention_weights
        ) if attention_weights else node_embeddings

        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.state_manager = StateManager(hetero_data, enhanced_embeddings, device, config)
        self.action_space = ActionSpace(hetero_data, num_partitions, device)

        # ã€æ–°å¢ã€‘ç¡®ä¿ActionMaskå¤„ç†å™¨è¢«åˆå§‹åŒ–
        from code.src.rl.action_space import ActionMask
        self.action_space.mask_handler = ActionMask(hetero_data, device)

        # å¥–åŠ±å‡½æ•°æ”¯æŒ - ç»Ÿä¸€ä½¿ç”¨è‡ªé€‚åº”è´¨é‡å¯¼å‘å¥–åŠ±ç³»ç»Ÿ
        reward_config = {
            'reward_weights': reward_weights,
            'thresholds': reward_weights.get('thresholds', {}) if reward_weights else {},
            'max_steps': max_steps
        }
        # å¦‚æœconfigä¸­æœ‰adaptive_qualityé…ç½®ï¼Œæ·»åŠ åˆ°reward_configä¸­
        if config and 'adaptive_quality' in config:
            reward_config['adaptive_quality'] = config['adaptive_quality']

        self.reward_function = RewardFunction(
            hetero_data,
            config=reward_config,
            device=device
        )

        self.metis_initializer = MetisInitializer(hetero_data, device, config)
        self.evaluator = PartitionEvaluator(hetero_data, device)

        # ã€ä¿ç•™ã€‘ç”¨äºå­˜å‚¨ä¸Šä¸€æ­¥çš„æŒ‡æ ‡ï¼Œå®ç°å¢é‡å¥–åŠ±
        self.previous_metrics = None

        # ã€æ–°å¢ã€‘åŠ¨æ€çº¦æŸå‚æ•° - ç”¨äºæ¥æ”¶AdaptiveDirectorçš„æŒ‡ä»¤
        # é»˜è®¤ä½¿ç”¨ç¡¬çº¦æŸæ¨¡å¼ä»¥ä¿æŒå‘åå…¼å®¹æ€§
        self.dynamic_constraint_params = {
            'constraint_mode': 'hard',  # 'hard' æˆ– 'soft'
            'connectivity_penalty': 0.5,  # è½¯çº¦æŸæƒ©ç½šå¼ºåº¦
            'action_mask_relaxation': 0.0,  # åŠ¨ä½œæ©ç æ”¾æ¾ç¨‹åº¦ (0.0-1.0)
            'violation_tolerance': 0.1,  # è¿è§„å®¹å¿åº¦
        }

        # ã€å‘åå…¼å®¹ã€‘å¦‚æœé…ç½®ä¸­æŒ‡å®šäº†è½¯çº¦æŸï¼Œåˆ™å¯ç”¨
        if config and config.get('adaptive_curriculum', {}).get('enabled', False):
            self.dynamic_constraint_params['constraint_mode'] = 'soft'

        # ç¯å¢ƒçŠ¶æ€
        self.current_step = 0
        self.episode_history = []
        self.is_terminated = False
        self.is_truncated = False

        # ç¼“å­˜é¢‘ç¹ä½¿ç”¨çš„æ•°æ®
        self._setup_cached_data()

    def update_dynamic_constraints(self, params: Dict[str, Any]):
        """
        æ›´æ–°åŠ¨æ€çº¦æŸå‚æ•°ï¼ˆç”±AdaptiveDirectorè°ƒç”¨ï¼‰

        å‚æ•°:
            params: åŒ…å«çº¦æŸå‚æ•°çš„å­—å…¸
        """
        if 'constraint_mode' in params:
            self.dynamic_constraint_params['constraint_mode'] = params['constraint_mode']
        if 'connectivity_penalty' in params:
            self.dynamic_constraint_params['connectivity_penalty'] = params['connectivity_penalty']
        if 'action_mask_relaxation' in params:
            self.dynamic_constraint_params['action_mask_relaxation'] = params['action_mask_relaxation']
        if 'violation_tolerance' in params:
            self.dynamic_constraint_params['violation_tolerance'] = params['violation_tolerance']

    def _setup_cached_data(self):
        """è®¾ç½®é¢‘ç¹è®¿é—®çš„ç¼“å­˜æ•°æ®"""
        # æ‰€æœ‰ç±»å‹èŠ‚ç‚¹çš„æ€»æ•°
        self.total_nodes = sum(x.shape[0] for x in self.hetero_data.x_dict.values())
        
        # å…¨å±€èŠ‚ç‚¹æ˜ å°„ï¼ˆæœ¬åœ°ç´¢å¼•åˆ°å…¨å±€ç´¢å¼•ï¼‰
        self.global_node_mapping = self.state_manager.get_global_node_mapping()
        
        # ç”¨äºå¥–åŠ±è®¡ç®—çš„è¾¹ä¿¡æ¯
        self.edge_info = self._extract_edge_info()
        
    def _extract_edge_info(self) -> Dict[str, torch.Tensor]:
        """æå–å¥–åŠ±è®¡ç®—æ‰€éœ€çš„è¾¹ä¿¡æ¯"""
        edge_info = {}
        
        # æ”¶é›†æ‰€æœ‰è¾¹åŠå…¶å±æ€§
        all_edges = []
        all_edge_attrs = []
        
        for edge_type, edge_index in self.hetero_data.edge_index_dict.items():
            edge_attr = self.hetero_data.edge_attr_dict[edge_type]
            
            # å°†æœ¬åœ°ç´¢å¼•è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•
            src_type, _, dst_type = edge_type
            src_global = self.state_manager.local_to_global(edge_index[0], src_type)
            dst_global = self.state_manager.local_to_global(edge_index[1], dst_type)
            
            global_edges = torch.stack([src_global, dst_global], dim=0)
            all_edges.append(global_edges)
            all_edge_attrs.append(edge_attr)
        
        edge_info['edge_index'] = torch.cat(all_edges, dim=1)
        edge_info['edge_attr'] = torch.cat(all_edge_attrs, dim=0)
        
        return edge_info

    def _generate_enhanced_embeddings(self,
                                    node_embeddings: Dict[str, torch.Tensor],
                                    attention_weights: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        ç”Ÿæˆå¢å¼ºçš„é™æ€èŠ‚ç‚¹ç‰¹å¾åµŒå…¥ H'

        å°†è¾¹çº§æ³¨æ„åŠ›æƒé‡èšåˆä¸ºèŠ‚ç‚¹ç‰¹å¾ï¼Œç„¶åä¸åŸå§‹åµŒå…¥è¿æ¥

        å‚æ•°:
            node_embeddings: åŸå§‹èŠ‚ç‚¹åµŒå…¥ H
            attention_weights: GATç¼–ç å™¨çš„è¾¹çº§æ³¨æ„åŠ›æƒé‡

        è¿”å›:
            enhanced_embeddings: å¢å¼ºçš„èŠ‚ç‚¹åµŒå…¥ H' = concat(H, H_attn)
        """
        # æ­¥éª¤1ï¼šè®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„èšåˆæ³¨æ„åŠ›åˆ†æ•°
        node_attention_scores = self._aggregate_attention_to_nodes(attention_weights)

        # æ­¥éª¤2ï¼šå°†æ³¨æ„åŠ›åˆ†æ•°ä¸åŸå§‹åµŒå…¥è¿æ¥
        enhanced_embeddings = {}

        for node_type, embeddings in node_embeddings.items():
            # è·å–è¯¥èŠ‚ç‚¹ç±»å‹çš„æ³¨æ„åŠ›åˆ†æ•°
            if node_type in node_attention_scores and node_attention_scores[node_type] is not None:
                attention_features = node_attention_scores[node_type]

                # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
                if torch.isnan(embeddings).any() or torch.isinf(embeddings).any():
                    print(f"  âš ï¸ {node_type}: æ£€æµ‹åˆ°åŸå§‹åµŒå…¥ä¸­çš„NaN/Infå€¼")
                    embeddings = torch.nan_to_num(embeddings, nan=0.0, posinf=1.0, neginf=-1.0)

                if torch.isnan(attention_features).any() or torch.isinf(attention_features).any():
                    print(f"  âš ï¸ {node_type}: æ£€æµ‹åˆ°æ³¨æ„åŠ›ç‰¹å¾ä¸­çš„NaN/Infå€¼")
                    attention_features = torch.nan_to_num(attention_features, nan=0.0, posinf=1.0, neginf=-1.0)

                # è¿æ¥åŸå§‹åµŒå…¥å’Œæ³¨æ„åŠ›ç‰¹å¾: H' = concat(H, H_attn)
                enhanced_emb = torch.cat([embeddings, attention_features], dim=1)

                # æ£€æŸ¥è¿æ¥åçš„åµŒå…¥æ˜¯å¦æœ‰æå€¼
                if torch.isnan(enhanced_emb).any() or torch.isinf(enhanced_emb).any():
                    print(f"  âš ï¸ {node_type}: æ£€æµ‹åˆ°å¢å¼ºåµŒå…¥ä¸­çš„NaN/Infå€¼ï¼Œè¿›è¡Œæ¸…ç†")
                    enhanced_emb = torch.nan_to_num(enhanced_emb, nan=0.0, posinf=1.0, neginf=-1.0)

                enhanced_embeddings[node_type] = enhanced_emb
            else:
                # å¦‚æœæ²¡æœ‰æ³¨æ„åŠ›æƒé‡ï¼Œä½¿ç”¨åŸå§‹åµŒå…¥
                # ä»ç„¶æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
                if torch.isnan(embeddings).any() or torch.isinf(embeddings).any():
                    print(f"  âš ï¸ {node_type}: æ£€æµ‹åˆ°åŸå§‹åµŒå…¥ä¸­çš„NaN/Infå€¼")
                    embeddings = torch.nan_to_num(embeddings, nan=0.0, posinf=1.0, neginf=-1.0)

                enhanced_embeddings[node_type] = embeddings

        return enhanced_embeddings

    def _aggregate_attention_to_nodes(self,
                                    attention_weights: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        å°†è¾¹çº§æ³¨æ„åŠ›æƒé‡èšåˆä¸ºèŠ‚ç‚¹çº§ç‰¹å¾

        å¯¹æ¯ä¸ªèŠ‚ç‚¹ iï¼Œè®¡ç®—å…¶èšåˆæ³¨æ„åŠ›åˆ†æ•°ï¼š
        h_attn(i) = (1/|N(i)|) * Î£_{j âˆˆ N(i)} Î±_{jâ†’i}

        å‚æ•°:
            attention_weights: è¾¹çº§æ³¨æ„åŠ›æƒé‡å­—å…¸

        è¿”å›:
            node_attention_scores: æ¯ä¸ªèŠ‚ç‚¹ç±»å‹çš„æ³¨æ„åŠ›åˆ†æ•° [num_nodes, 1]
        """
        # åˆå§‹åŒ–èŠ‚ç‚¹æ³¨æ„åŠ›åˆ†æ•°ç´¯ç§¯å™¨
        node_attention_accumulator = {}
        node_degree_counter = {}
        edge_type_to_key_mapping = {}  # æ·»åŠ è¿™ä¸ªå˜é‡

        # åˆå§‹åŒ–æ‰€æœ‰èŠ‚ç‚¹ç±»å‹çš„ç´¯ç§¯å™¨
        for node_type in self.hetero_data.x_dict.keys():
            num_nodes = self.hetero_data.x_dict[node_type].shape[0]
            node_attention_accumulator[node_type] = torch.zeros(num_nodes, device=self.device)
            node_degree_counter[node_type] = torch.zeros(num_nodes, device=self.device)

        # å¤„ç†æ¯ç§è¾¹ç±»å‹
        has_attention = False
        for edge_type, edge_index in self.hetero_data.edge_index_dict.items():
            src_type, relation, dst_type = edge_type
            
            # ä½¿ç”¨æ”¹è¿›çš„é”®åŒ¹é…ï¼Œæ‰¾ä¸åˆ°æ—¶è¿”å›None
            attn_weights = self._get_attention_weights_for_edge_type(
                edge_type, attention_weights, edge_type_to_key_mapping
            )
            
            # å¦‚æœæ‰¾ä¸åˆ°æƒé‡ï¼Œåˆ™è·³è¿‡æ­¤è¾¹ç±»å‹
            if attn_weights is None:
                continue
            
            has_attention = True
            # å¤„ç†ç»´åº¦å’Œå¤šå¤´æ³¨æ„åŠ›ï¼ˆç»´åº¦ä¸åŒ¹é…æ—¶è¿”å›Noneï¼‰
            processed_weights = self._process_attention_weights(
                attn_weights, edge_index, edge_type
            )
            
            # å¦‚æœæƒé‡å¤„ç†å¤±è´¥ï¼ˆå¦‚ç»´åº¦ä¸åŒ¹é…ï¼‰ï¼Œåˆ™è·³è¿‡æ­¤è¾¹ç±»å‹
            if processed_weights is None:
                continue
            
            # é«˜æ•ˆçš„èŠ‚ç‚¹èšåˆ
            dst_nodes = edge_index[1]
            node_attention_accumulator[dst_type].index_add_(0, dst_nodes, processed_weights)
            node_degree_counter[dst_type].index_add_(0, dst_nodes, torch.ones_like(processed_weights))

        # å¦‚æœæ²¡æœ‰ä»»ä½•æ³¨æ„åŠ›æƒé‡ï¼Œç›´æ¥è¿”å›
        if not has_attention:
            return {node_type: None for node_type in self.hetero_data.x_dict.keys()}

        # è®¡ç®—å¹³å‡æ³¨æ„åŠ›åˆ†æ•°ï¼ˆé¿å…é™¤é›¶ï¼‰
        node_attention_scores = {}
        for node_type in node_attention_accumulator.keys():
            degrees = node_degree_counter[node_type]
            # é¿å…é™¤é›¶ï¼šåº¦ä¸º0çš„èŠ‚ç‚¹æ³¨æ„åŠ›åˆ†æ•°è®¾ä¸º0
            avg_attention = torch.where(
                degrees > 0,
                node_attention_accumulator[node_type] / degrees,
                torch.zeros_like(node_attention_accumulator[node_type])
            )

            # è½¬æ¢ä¸ºåˆ—å‘é‡ [num_nodes, 1]
            node_attention_scores[node_type] = avg_attention.unsqueeze(1)

        return node_attention_scores

    def _get_attention_weights_for_edge_type(self,
                                            edge_type: tuple,
                                            attention_weights: Dict[str, torch.Tensor],
                                            edge_type_to_key_mapping: Dict[str, str]) -> Optional[torch.Tensor]:
        """
        è·å–ç‰¹å®šè¾¹ç±»å‹çš„æ³¨æ„åŠ›æƒé‡

        å‚æ•°:
            edge_type: è¾¹ç±»å‹ (src_type, relation, dst_type)
            attention_weights: è¾¹çº§æ³¨æ„åŠ›æƒé‡å­—å…¸
            edge_type_to_key_mapping: è¾¹ç±»å‹åˆ°æ³¨æ„åŠ›æƒé‡é”®çš„æ˜ å°„

        è¿”å›:
            æ‰¾åˆ°çš„æ³¨æ„åŠ›æƒé‡ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›None
        """
        # æ„å»ºè¾¹ç±»å‹åˆ°æ³¨æ„åŠ›æƒé‡é”®çš„æ˜ å°„
        edge_type_key = f"{edge_type[0]}__{edge_type[1]}__{edge_type[2]}"
        edge_type_to_key_mapping[edge_type_key] = edge_type_key

        # å°è¯•å¤šç§é”®æ ¼å¼æ¥æŸ¥æ‰¾æ³¨æ„åŠ›æƒé‡
        found_weights = None
        used_key = None

        # 1. å°è¯•æ ‡å‡†æ ¼å¼
        if edge_type_key in attention_weights:
            found_weights = attention_weights[edge_type_key]
            used_key = edge_type_key
        else:
            # 2. å°è¯•æŸ¥æ‰¾åŒ…å«ç›¸å…³ä¿¡æ¯çš„é”®
            for key, weights in attention_weights.items():
                if (edge_type[0] in key and edge_type[2] in key and edge_type[1] in key) or \
                   ("unknown_edge_type" in key):
                    found_weights = weights
                    used_key = key
                    break

        if found_weights is None:
            # print(f"    âš ï¸ æœªæ‰¾åˆ°è¾¹ç±»å‹ {edge_type_key} çš„æ³¨æ„åŠ›æƒé‡")
            # print(f"       å¯ç”¨çš„æ³¨æ„åŠ›æƒé‡é”®: {list(attention_weights.keys())}")
            return None

        attn_weights = found_weights.to(self.device)
        # print(f"    ğŸ” è¾¹ç±»å‹ {edge_type} ä½¿ç”¨æ³¨æ„åŠ›æƒé‡é”®: {used_key}")
        return attn_weights

    def _process_attention_weights(self, 
                                 attn_weights: torch.Tensor,
                                 edge_index: torch.Tensor,
                                 edge_type: tuple) -> Optional[torch.Tensor]:
        """
        å¤„ç†æ³¨æ„åŠ›æƒé‡çš„ç»´åº¦å’Œå¤šå¤´æ³¨æ„åŠ›ã€‚
        å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œåˆ™è¿”å› Noneï¼Œè¡¨ç¤ºå¿½ç•¥æ­¤æƒé‡ã€‚
        """
        num_edges = edge_index.shape[1]
        
        # å¤„ç†å¤šå¤´æ³¨æ„åŠ›
        if attn_weights.dim() > 1:
            if attn_weights.shape[-1] > 1:  # å¤šå¤´æ³¨æ„åŠ›
                attn_weights = attn_weights.mean(dim=-1)
            else:
                attn_weights = attn_weights.squeeze(-1)
        
        # ç»´åº¦éªŒè¯
        if attn_weights.shape[0] != num_edges:
            print(
                f"    âš ï¸ æ³¨æ„åŠ›æƒé‡ç»´åº¦ä¸åŒ¹é… - è¾¹ç±»å‹: {edge_type}, "
                f"æƒé‡æ•°é‡: {attn_weights.shape[0]}, è¾¹æ•°é‡: {num_edges}."
            )
            print(f"    ğŸ”§ å°†å¿½ç•¥æ­¤è¾¹ç±»å‹çš„æ³¨æ„åŠ›æƒé‡ã€‚")
            return None
        
        return attn_weights

    def reset(self, seed: Optional[int] = None, scenario_context: Optional[ScenarioContext] = None) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:
        """
        å°†ç¯å¢ƒé‡ç½®ä¸ºåˆå§‹çŠ¶æ€
        
        å‚æ•°:
            seed: ç”¨äºå¯é‡å¤æ€§çš„éšæœºç§å­
            scenario_context: åœºæ™¯ä¸Šä¸‹æ–‡ï¼ˆç”¨äºåœºæ™¯æ„ŸçŸ¥å¥–åŠ±ï¼‰
            
        è¿”å›:
            observation: åˆå§‹çŠ¶æ€è§‚å¯Ÿ
            info: é™„åŠ ä¿¡æ¯
        """
        if seed is not None:
            torch.manual_seed(seed)
            np.random.seed(seed)
            
        # ä½¿ç”¨METISåˆå§‹åŒ–åˆ†åŒº
        initial_partition = self.metis_initializer.initialize_partition(self.num_partitions)
        
        # ä½¿ç”¨åˆå§‹åˆ†åŒºé‡ç½®çŠ¶æ€ç®¡ç†å™¨
        self.state_manager.reset(initial_partition)
        
        # é‡ç½®ç¯å¢ƒçŠ¶æ€
        self.current_step = 0
        self.episode_history = []
        self.is_terminated = False
        self.is_truncated = False

        # é‡ç½®å¥–åŠ±å‡½æ•°çŠ¶æ€
        self.reward_function.reset_episode(scenario_context)
        
        # è·å–åˆå§‹è§‚å¯Ÿ
        observation = self.state_manager.get_observation()

        # æ·»åŠ åŠ¨ä½œæ©ç åˆ°è§‚å¯Ÿä¸­
        observation['action_mask'] = self.get_action_mask(use_advanced_constraints=True)
        
        # ã€ä¿®å¤ã€‘ç»Ÿä¸€ä½¿ç”¨æ–°å¥–åŠ±ç³»ç»Ÿè®¡ç®—åˆå§‹æŒ‡æ ‡ï¼Œé¿å…åŒå¥—æŒ‡æ ‡ç³»ç»Ÿå†²çª
        initial_metrics = self.reward_function.get_current_metrics(
            self.state_manager.current_partition
        )

        # ã€æ–°å¢ã€‘åœ¨å›åˆå¼€å§‹æ—¶ï¼Œè®¡ç®—å¹¶å­˜å‚¨åˆå§‹åˆ†åŒºçš„æŒ‡æ ‡
        self.previous_metrics = initial_metrics
        
        info = {
            'step': self.current_step,
            'metrics': initial_metrics,
            'partition': self.state_manager.current_partition.clone(),
            'boundary_nodes': self.state_manager.get_boundary_nodes(),
            'valid_actions': self.action_space.get_valid_actions(
                self.state_manager.current_partition,
                self.state_manager.get_boundary_nodes()
            ),
            'scenario_context': scenario_context.to_dict() if scenario_context else None
        }
        
        return observation, info
    

    

        
    def step(self, action: Tuple[int, int]) -> Tuple[Dict[str, torch.Tensor], float, bool, bool, Dict[str, Any]]:
        """
        åœ¨ç¯å¢ƒä¸­æ‰§è¡Œä¸€æ­¥ - æ”¯æŒåŠ¨æ€è½¯çº¦æŸç³»ç»Ÿ

        å‚æ•°:
            action: (node_idx, target_partition)çš„å…ƒç»„

        è¿”å›:
            observation: ä¸‹ä¸€çŠ¶æ€è§‚å¯Ÿ
            reward: å³æ—¶å¥–åŠ±
            terminated: å›åˆæ˜¯å¦ç»ˆæ­¢
            truncated: å›åˆæ˜¯å¦è¢«æˆªæ–­
            info: é™„åŠ ä¿¡æ¯
        """
        if self.is_terminated or self.is_truncated:
            raise RuntimeError("æ— æ³•åœ¨å·²ç»ˆæ­¢/æˆªæ–­çš„ç¯å¢ƒä¸­æ‰§è¡Œæ­¥éª¤ã€‚è¯·å…ˆè°ƒç”¨reset()ã€‚")

        # 1. åŸºç¡€åŠ¨ä½œéªŒè¯ï¼ˆåªæ£€æŸ¥åŸºæœ¬æœ‰æ•ˆæ€§ï¼šè¾¹ç•ŒèŠ‚ç‚¹ã€ç›¸é‚»åˆ†åŒºç­‰ï¼‰
        if not self.action_space.is_valid_action(
            action,
            self.state_manager.current_partition,
            self.state_manager.get_boundary_nodes()
        ):
            # æ— æ•ˆåŠ¨ä½œæƒ©ç½š
            return self.state_manager.get_observation(), -2.0, True, False, {'termination_reason': 'invalid_action'}

        # 2. ã€æ–°å¢ã€‘è¿é€šæ€§çº¦æŸæ£€æŸ¥ï¼ˆè½¯çº¦æŸæ¨¡å¼ï¼‰
        constraint_violation_info = self.action_space.mask_handler.check_connectivity_violation(
            self.state_manager.current_partition, action
        )

        # 3. æ ¹æ®çº¦æŸæ¨¡å¼å†³å®šæ˜¯å¦æ‰§è¡ŒåŠ¨ä½œ
        constraint_mode = self.dynamic_constraint_params.get('constraint_mode', 'hard')

        if constraint_mode == 'hard' and constraint_violation_info and constraint_violation_info['violates_connectivity']:
            # ç¡¬çº¦æŸæ¨¡å¼ï¼šæ‹’ç»æ‰§è¡Œè¿è§„åŠ¨ä½œ
            # ã€ä¿®å¤ã€‘å°†è¿é€šæ€§è¿è§„æƒ©ç½šä»-10.0é™ä½åˆ°-3.0ï¼Œä»ç„¶ä¸¥å‰ä½†ä¸ä¼šå‹å€’å…¶ä»–å¥–åŠ±
            return self.state_manager.get_observation(), -3.0, True, False, {
                'termination_reason': 'connectivity_violation',
                'violation_info': constraint_violation_info
            }

        # 4. æ‰§è¡ŒåŠ¨ä½œï¼Œæ›´æ–°å†…éƒ¨çŠ¶æ€
        node_idx, target_partition = action
        self.state_manager.update_partition(node_idx, target_partition)

        # 5. ã€æ ¸å¿ƒã€‘è®¡ç®—å¥–åŠ± - ä½¿ç”¨è‡ªé€‚åº”è´¨é‡å¯¼å‘å¥–åŠ±ç³»ç»Ÿ
        plateau_result = None
        # è·å–å½“å‰åœºæ™¯ä¸Šä¸‹æ–‡
        current_scenario_context = getattr(self.reward_function, 'current_scenario_context', None)
        # ä½¿ç”¨è‡ªé€‚åº”è´¨é‡å¯¼å‘å¥–åŠ±å‡½æ•°
        reward, plateau_result = self.reward_function.compute_incremental_reward(
            self.state_manager.current_partition,
            action,
            current_scenario_context
        )

        # 6. ç»Ÿä¸€ä½¿ç”¨æ–°å¥–åŠ±ç³»ç»Ÿçš„æŒ‡æ ‡ï¼ˆä¿®å¤åŒå¥—æŒ‡æ ‡ç³»ç»Ÿå†²çªï¼‰
        current_metrics = self.reward_function.get_current_metrics(
            self.state_manager.current_partition
        )
        quality_score = self.reward_function.get_current_quality_score(
            self.state_manager.current_partition
        )

        # è¡¥å……ä¸€äº›ç¯å¢ƒéœ€è¦çš„é¢å¤–æŒ‡æ ‡
        # ã€ä¿®å¤ã€‘æ”¹è¿›connectivityè®¡ç®—ï¼Œä¸å†ç®€å•å‡è®¾ä¸º1.0
        connectivity_score = self._compute_connectivity_score(self.state_manager.current_partition)
        current_metrics.update({
            'connectivity': connectivity_score,
            'step': self.current_step
        })

        self.last_reward_components = {
            'incremental_reward': reward,
            'current_metrics': current_metrics,
            'quality_score': quality_score,
            'plateau_result': plateau_result
        }

        # 7. ã€æ–°å¢ã€‘åº”ç”¨è½¯çº¦æŸæƒ©ç½š
        constraint_penalty = 0.0
        if constraint_mode == 'soft' and constraint_violation_info and constraint_violation_info['violates_connectivity']:
            # è½¯çº¦æŸæ¨¡å¼ï¼šæ ¹æ®è¿è§„ä¸¥é‡ç¨‹åº¦åº”ç”¨æƒ©ç½š
            penalty_strength = self.dynamic_constraint_params.get('connectivity_penalty', 0.5)
            violation_severity = constraint_violation_info['violation_severity']
            constraint_penalty = penalty_strength * violation_severity
            reward -= constraint_penalty

            # è®°å½•æƒ©ç½šä¿¡æ¯
            self.last_reward_components['constraint_penalty'] = constraint_penalty
            self.last_reward_components['violation_info'] = constraint_violation_info
        
        # 5. ã€å…³é”®ã€‘æ›´æ–°"ä¸Šä¸€æ­¥"çš„æŒ‡æ ‡ï¼Œä¸ºä¸‹ä¸€æ¬¡è®¡ç®—åšå‡†å¤‡
        self.previous_metrics = current_metrics

        # 6. ã€æ–°å¢ã€‘æ£€æŸ¥å¹³å°æœŸæ—©åœæ¡ä»¶
        early_stop_triggered = False
        early_stop_confidence = 0.0
        if plateau_result is not None:
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ—©åœæ¡ä»¶
            early_stop_threshold = self.reward_function.adaptive_quality_config['efficiency_reward']['early_stop_confidence']
            if (plateau_result.plateau_detected and
                plateau_result.confidence > early_stop_threshold):
                early_stop_triggered = True
                early_stop_confidence = plateau_result.confidence

        # 7. æ›´æ–°æ­¥æ•°å’Œæ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        self.current_step += 1
        terminated, truncated = self._check_termination()

        # 8. ã€æ–°å¢ã€‘åº”ç”¨æ—©åœé€»è¾‘
        if early_stop_triggered and not terminated and not truncated:
            terminated = True
            # åœ¨infoä¸­è®°å½•æ—©åœä¿¡æ¯
            info_bonus = {
                'early_termination': True,
                'termination_reason': 'quality_plateau_reached',
                'plateau_confidence': early_stop_confidence,
                'plateau_details': plateau_result._asdict() if plateau_result else {}
            }
        
        # 9. ã€æ ¸å¿ƒæ”¹è¿›ã€‘åŒºåˆ†ç»“æŸç±»å‹ï¼Œåº”ç”¨ç»ˆå±€å¥–åŠ±
        if terminated or truncated:
            # ç¡®å®šç»ˆæ­¢ç±»å‹ï¼ˆè€ƒè™‘æ—©åœæƒ…å†µï¼‰
            if early_stop_triggered:
                termination_type = 'early_stop'
            else:
                termination_type = self._determine_termination_type(terminated, truncated)

            # ä½¿ç”¨å¥–åŠ±å‡½æ•°è®¡ç®—ç»ˆå±€å¥–åŠ±
            final_reward, final_components = self.reward_function.compute_final_reward(
                self.state_manager.current_partition,
                termination_type
            )
            reward += final_reward

            # åˆå¹¶æ—©åœä¿¡æ¯å’Œç»ˆå±€å¥–åŠ±ä¿¡æ¯
            if early_stop_triggered:
                info_bonus.update(final_components)
                info_bonus['termination_type'] = termination_type
                info_bonus['final_reward'] = final_reward
            else:
                info_bonus = final_components
                info_bonus['termination_type'] = termination_type
                info_bonus['final_reward'] = final_reward
        else:
            info_bonus = {}
        
        # 8. å‡†å¤‡è¿”å›ä¿¡æ¯
        observation = self.state_manager.get_observation()

        # æ·»åŠ åŠ¨ä½œæ©ç åˆ°è§‚å¯Ÿä¸­
        observation['action_mask'] = self.get_action_mask(use_advanced_constraints=True)
        
        info = {
            'step': self.current_step,
            'metrics': current_metrics,
            'reward': reward,
            'reward_mode': 'adaptive_quality',
            **info_bonus
        }

        # æ·»åŠ å¥–åŠ±ç»„ä»¶ä¿¡æ¯
        if self.last_reward_components is not None:
            info['reward_components'] = self.last_reward_components
        
        return observation, reward, terminated, truncated, info
    


    def _determine_termination_type(self, terminated: bool, truncated: bool) -> str:
        """
        ç¡®å®šç»ˆæ­¢ç±»å‹ï¼Œç”¨äºåŒå±‚å¥–åŠ±å‡½æ•°

        Returns:
            'natural': è‡ªç„¶å®Œæˆï¼ˆæ‰€æœ‰èŠ‚ç‚¹åˆ†é…å®Œæˆï¼‰
            'timeout': è¶…æ—¶ç»“æŸ
            'stuck': æå‰å¡ä½ï¼ˆæ— æœ‰æ•ˆåŠ¨ä½œä½†æœªå®Œæˆï¼‰
        """
        if terminated:
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰èŠ‚ç‚¹éƒ½è¢«åˆ†é…
            unassigned_count = (self.state_manager.current_partition == 0).sum().item()
            if unassigned_count == 0:
                return 'natural'
            else:
                return 'stuck'
        elif truncated:
            return 'timeout'
        else:
            return 'unknown'
        
    def _check_termination(self) -> Tuple[bool, bool]:
        """
        æ£€æŸ¥å›åˆæ˜¯å¦åº”è¯¥ç»ˆæ­¢æˆ–æˆªæ–­ - æ”¯æŒè½¯çº¦æŸæ¨¡å¼

        è¿”å›:
            terminated: è‡ªç„¶ç»ˆæ­¢ï¼ˆæ”¶æ•›æˆ–æ— æœ‰æ•ˆåŠ¨ä½œï¼‰
            truncated: äººå·¥ç»ˆæ­¢ï¼ˆè¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼‰
        """
        # æ£€æŸ¥æˆªæ–­ï¼ˆæœ€å¤§æ­¥æ•°ï¼‰
        if self.current_step >= self.max_steps:
            return False, True

        # æ£€æŸ¥è‡ªç„¶ç»ˆæ­¢
        boundary_nodes = self.state_manager.get_boundary_nodes()

        # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨å½“å‰çš„çº¦æŸæ¨¡å¼æ¥è·å–æœ‰æ•ˆåŠ¨ä½œ
        constraint_mode = self.dynamic_constraint_params.get('constraint_mode', 'hard')
        valid_actions = self.action_space.get_valid_actions(
            self.state_manager.current_partition,
            boundary_nodes,
            constraint_mode=constraint_mode
        )

        # æ²¡æœ‰å‰©ä½™æœ‰æ•ˆåŠ¨ä½œ
        if len(valid_actions) == 0:
            return True, False

        # æ”¶æ•›æ£€æŸ¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self._check_convergence():
            return True, False

        return False, False
        
    def _check_convergence(self, window_size: int = 10, threshold: float = 0.01) -> bool:
        """
        åŸºäºæœ€è¿‘å¥–åŠ±å†å²æ£€æŸ¥åˆ†åŒºæ˜¯å¦æ”¶æ•›
        
        å‚æ•°:
            window_size: è¦è€ƒè™‘çš„æœ€è¿‘æ­¥æ•°
            threshold: æ”¶æ•›é˜ˆå€¼
            
        è¿”å›:
            å¦‚æœæ”¶æ•›è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        if len(self.episode_history) < window_size:
            return False
            
        recent_rewards = [step['reward'] for step in self.episode_history[-window_size:]]
        reward_std = np.std(recent_rewards)
        
        return reward_std < threshold
        
    def render(self, mode: str = 'human') -> Optional[np.ndarray]:
        """
        æ¸²æŸ“ç¯å¢ƒçš„å½“å‰çŠ¶æ€
        
        å‚æ•°:
            mode: æ¸²æŸ“æ¨¡å¼ï¼ˆ'human', 'rgb_array', æˆ– 'ansi'ï¼‰
            
        è¿”å›:
            æ¸²æŸ“è¾“å‡ºï¼ˆå–å†³äºæ¨¡å¼ï¼‰
        """
        if mode == 'ansi':
            # åŸºäºæ–‡æœ¬çš„æ¸²æŸ“
            output = []
            output.append(f"æ­¥æ•°: {self.current_step}/{self.max_steps}")
            output.append(f"åˆ†åŒºæ•°: {self.num_partitions}")
            output.append(f"æ€»èŠ‚ç‚¹æ•°: {self.total_nodes}")
            
            # åˆ†åŒºåˆ†å¸ƒ
            partition_counts = torch.bincount(
                self.state_manager.current_partition, 
                minlength=self.num_partitions + 1
            )[1:]  # è·³è¿‡åˆ†åŒº0
            output.append(f"åˆ†åŒºå¤§å°: {partition_counts.tolist()}")
            
            # è¾¹ç•ŒèŠ‚ç‚¹
            boundary_nodes = self.state_manager.get_boundary_nodes()
            output.append(f"è¾¹ç•ŒèŠ‚ç‚¹: {len(boundary_nodes)}")
            
            return '\n'.join(output)
            
        elif mode == 'human':
            print(self.render('ansi'))
            return None
            
        else:
            raise NotImplementedError(f"æ¸²æŸ“æ¨¡å¼ '{mode}' æœªå®ç°")
            
    def close(self):
        """æ¸…ç†ç¯å¢ƒèµ„æº"""
        # æ¸…ç†ç¼“å­˜æ•°æ®
        if hasattr(self, 'edge_info'):
            del self.edge_info
        if hasattr(self, 'global_node_mapping'):
            del self.global_node_mapping
            
        # æ¸…ç†ç»„ä»¶å¼•ç”¨
        self.state_manager = None
        self.action_space = None
        self.reward_function = None
        self.metis_initializer = None
        self.evaluator = None
        
    def get_action_mask(self, use_advanced_constraints: bool = True) -> torch.Tensor:
        """
        è·å–å½“å‰çŠ¶æ€çš„åŠ¨ä½œæ©ç  - æ”¯æŒåŠ¨æ€çº¦æŸæ¨¡å¼

        å‚æ•°:
            use_advanced_constraints: æ˜¯å¦ä½¿ç”¨é«˜çº§çº¦æŸ

        è¿”å›:
            æŒ‡ç¤ºæœ‰æ•ˆåŠ¨ä½œçš„å¸ƒå°”å¼ é‡
        """
        if use_advanced_constraints:
            # ä½¿ç”¨é«˜çº§æ©ç ï¼Œæ ¹æ®å½“å‰çº¦æŸæ¨¡å¼å†³å®šè¡Œä¸º
            constraint_mode = self.dynamic_constraint_params.get('constraint_mode', 'hard')
            state = {'partition_assignment': self.state_manager.current_partition}

            return self.action_space.get_advanced_mask(
                state,
                self.state_manager.get_boundary_nodes(),
                self.hetero_data,
                apply_constraints=True,
                constraint_mode=constraint_mode
            )
        else:
            # ä½¿ç”¨åŸºç¡€æ©ç 
            return self.action_space.get_action_mask(
                self.state_manager.current_partition,
                self.state_manager.get_boundary_nodes()
            )
        
    def get_state_info(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰çŠ¶æ€çš„è¯¦ç»†ä¿¡æ¯
        
        è¿”å›:
            åŒ…å«çŠ¶æ€ä¿¡æ¯çš„å­—å…¸
        """
        return {
            'current_partition': self.state_manager.current_partition.clone(),
            'boundary_nodes': self.state_manager.get_boundary_nodes(),
            'step': self.current_step,
            'max_steps': self.max_steps,
            'num_partitions': self.num_partitions,
            'total_nodes': self.total_nodes,
            'is_terminated': self.is_terminated,
            'is_truncated': self.is_truncated
        }

    def _compute_connectivity_score(self, partition: torch.Tensor) -> float:
        """
        è®¡ç®—åˆ†åŒºè¿é€šæ€§åˆ†æ•°

        Args:
            partition: å½“å‰åˆ†åŒºæ–¹æ¡ˆ

        Returns:
            è¿é€šæ€§åˆ†æ•° [0, 1]ï¼Œ1è¡¨ç¤ºå®Œå…¨è¿é€š
        """
        try:
            # è·å–åˆ†åŒºæ•°é‡
            num_partitions = partition.max().item()
            if num_partitions <= 0:
                return 0.0

            # ç®€åŒ–çš„è¿é€šæ€§æ£€æŸ¥ï¼šæ£€æŸ¥æ¯ä¸ªåˆ†åŒºæ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªèŠ‚ç‚¹
            connected_partitions = 0
            for i in range(1, num_partitions + 1):
                if (partition == i).any():
                    connected_partitions += 1

            # è¿é€šæ€§åˆ†æ•° = æœ‰èŠ‚ç‚¹çš„åˆ†åŒºæ•° / æ€»åˆ†åŒºæ•°
            connectivity_score = connected_partitions / num_partitions

            # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæ‰€æœ‰èŠ‚ç‚¹éƒ½è¢«åˆ†é…ï¼Œç»™äºˆé¢å¤–å¥–åŠ±
            unassigned_nodes = (partition == 0).sum().item()
            if unassigned_nodes == 0:
                connectivity_score = min(1.0, connectivity_score + 0.1)

            return float(connectivity_score)

        except Exception as e:
            # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè¿”å›ä¿å®ˆä¼°è®¡
            return 0.5

    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜æ•°æ®"""
        # æ¸…ç†ç¼“å­˜æ•°æ®
        if hasattr(self, 'edge_info'):
            del self.edge_info
        if hasattr(self, 'global_node_mapping'):
            del self.global_node_mapping
            
        # æ¸…ç†ç»„ä»¶å¼•ç”¨
        self.state_manager = None
        self.action_space = None
        self.reward_function = None
        self.metis_initializer = None
        self.evaluator = None
