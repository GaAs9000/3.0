"""
ç”µåŠ›ç½‘ç»œåˆ†åŒºMDPçš„çŠ¶æ€ç®¡ç†

æœ¬æ¨¡å—ç®¡ç†MDPå…¬å¼ä¸­æŒ‡å®šçš„çŠ¶æ€è¡¨ç¤ºï¼š
- èŠ‚ç‚¹ç‰¹å¾åµŒå…¥ï¼ˆHæˆ–H'ï¼‰ï¼šé™æ€çš„ï¼Œç”±GATç¼–ç å™¨é¢„è®¡ç®—ï¼Œå¯èƒ½åŒ…å«æ³¨æ„åŠ›å¢å¼ºä¿¡æ¯
- èŠ‚ç‚¹åˆ†é…æ ‡ç­¾ï¼ˆz_tï¼‰ï¼šåŠ¨æ€çš„ï¼Œéšæ¯ä¸ªåŠ¨ä½œå˜åŒ–
- è¾¹ç•ŒèŠ‚ç‚¹ï¼šä»å½“å‰åˆ†åŒºæ´¾ç”Ÿ
- åŒºåŸŸèšåˆåµŒå…¥ï¼šç”¨äºç­–ç•¥è¾“å…¥
"""

import torch
import torch.nn as nn
import numpy as np
import time
from collections import defaultdict
from typing import Dict, List, Tuple, Optional, Set, Any
from torch_geometric.data import HeteroData
import torch_sparse


class IntelligentRegionEmbedding:
    """
    æ™ºèƒ½åŒºåŸŸåµŒå…¥ç”Ÿæˆå™¨ï¼Œä¼˜é›…å¤„ç†ç©ºåˆ†åŒº
    """

    def __init__(self, embedding_dim, device, config=None):
        self.embedding_dim = embedding_dim
        self.device = device
        self.config = config or {}

        # å­¦ä¹ çš„ç©ºåˆ†åŒºè¡¨ç¤ºï¼ˆå¯è®­ç»ƒå‚æ•°ï¼‰
        self.empty_partition_embedding = nn.Parameter(
            torch.randn(2 * embedding_dim, device=device) * 0.1
        )

        # åˆ†åŒºå†å²ä¿¡æ¯
        self.partition_history = {}
        self.current_step = 0

        # é…ç½®å‚æ•°
        self.use_learned_empty_embedding = config.get('use_learned_empty_embedding', True)
        self.use_positional_encoding = config.get('use_positional_encoding', True)
        self.use_historical_decay = config.get('use_historical_decay', True)

    def compute_region_embedding(self, partition_id, partition_nodes, node_embeddings):
        """
        è®¡ç®—åŒºåŸŸåµŒå…¥ï¼Œæ™ºèƒ½å¤„ç†ç©ºåˆ†åŒº
        """
        if len(partition_nodes) > 0:
            # éç©ºåˆ†åŒºï¼šæ­£å¸¸è®¡ç®—
            partition_embeddings = node_embeddings[partition_nodes]

            # å¤šç§èšåˆæ–¹å¼
            mean_embedding = torch.mean(partition_embeddings, dim=0)
            max_embedding = torch.max(partition_embeddings, dim=0)[0]

            # å¯é€‰ï¼šæ·»åŠ æ›´å¤šç»Ÿè®¡ä¿¡æ¯
            if self.config.get('use_advanced_pooling', False):
                min_embedding = torch.min(partition_embeddings, dim=0)[0]
                std_embedding = torch.std(partition_embeddings, dim=0)

                region_embedding = torch.cat([
                    mean_embedding, max_embedding,
                    min_embedding, std_embedding
                ], dim=0)
            else:
                region_embedding = torch.cat([mean_embedding, max_embedding], dim=0)

            # æ›´æ–°å†å²
            self._update_history(partition_id, region_embedding, 'active')

            return region_embedding

        else:
            # ç©ºåˆ†åŒºï¼šæ™ºèƒ½å¤„ç†
            return self._handle_empty_partition(partition_id)

    def _handle_empty_partition(self, partition_id):
        """
        æ™ºèƒ½å¤„ç†ç©ºåˆ†åŒº
        """
        # ç­–ç•¥1: ä½¿ç”¨å­¦ä¹ çš„ç©ºåˆ†åŒºåµŒå…¥
        if self.use_learned_empty_embedding:
            base_embedding = self.empty_partition_embedding
        else:
            base_embedding = torch.zeros(2 * self.embedding_dim, device=self.device)

        # ç­–ç•¥2: åŸºäºå†å²ä¿¡æ¯è°ƒæ•´
        if self.use_historical_decay and partition_id in self.partition_history:
            history = self.partition_history[partition_id]

            # å¦‚æœåˆ†åŒºæœ€è¿‘æ˜¯æ´»è·ƒçš„ï¼Œä½¿ç”¨è¡°å‡çš„å†å²åµŒå…¥
            if history['last_active_step'] is not None:
                steps_since_active = self.current_step - history['last_active_step']
                decay_factor = np.exp(-steps_since_active / 10.0)  # æŒ‡æ•°è¡°å‡

                if decay_factor > 0.1 and history['last_embedding'] is not None:
                    # æ··åˆå†å²åµŒå…¥å’Œç©ºåµŒå…¥
                    embedding = (decay_factor * history['last_embedding'] +
                               (1 - decay_factor) * base_embedding)

                    self._update_history(partition_id, embedding, 'decayed')
                    return embedding

        # ç­–ç•¥3: æ·»åŠ ä½ç½®ç¼–ç ä¿¡æ¯
        if self.use_positional_encoding:
            position_encoding = self._get_partition_position_encoding(partition_id)
            embedding = base_embedding + 0.1 * position_encoding
        else:
            embedding = base_embedding

        self._update_history(partition_id, embedding, 'empty')
        return embedding

    def _get_partition_position_encoding(self, partition_id):
        """
        ä¸ºåˆ†åŒºIDç”Ÿæˆä½ç½®ç¼–ç 
        """
        # ç®€å•çš„æ­£å¼¦ä½ç½®ç¼–ç 
        pe = torch.zeros(2 * self.embedding_dim, device=self.device)

        position = partition_id
        div_term = torch.exp(
            torch.arange(0, 2 * self.embedding_dim, 2).float() *
            (-np.log(10000.0) / (2 * self.embedding_dim))
        ).to(self.device)

        pe[0::2] = torch.sin(position * div_term)
        pe[1::2] = torch.cos(position * div_term)

        return pe

    def _update_history(self, partition_id, embedding, status):
        """
        æ›´æ–°åˆ†åŒºå†å²
        """
        if partition_id not in self.partition_history:
            self.partition_history[partition_id] = {
                'last_embedding': None,
                'last_active_step': None,
                'status_history': []
            }

        history = self.partition_history[partition_id]

        if status == 'active':
            history['last_embedding'] = embedding.detach()
            history['last_active_step'] = self.current_step

        history['status_history'].append({
            'step': self.current_step,
            'status': status
        })

        # é™åˆ¶å†å²é•¿åº¦
        if len(history['status_history']) > 100:
            history['status_history'] = history['status_history'][-100:]

    def update_step(self, step):
        """æ›´æ–°å½“å‰æ­¥æ•°"""
        self.current_step = step

    def get_embedding_statistics(self):
        """è·å–åµŒå…¥ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_partitions': len(self.partition_history),
            'active_partitions': 0,
            'empty_partitions': 0,
            'decayed_partitions': 0
        }

        for history in self.partition_history.values():
            if history['status_history']:
                last_status = history['status_history'][-1]['status']
                if last_status == 'active':
                    stats['active_partitions'] += 1
                elif last_status == 'empty':
                    stats['empty_partitions'] += 1
                elif last_status == 'decayed':
                    stats['decayed_partitions'] += 1

        return stats


class StateManager:
    """
    ç®¡ç†ç”µåŠ›ç½‘ç»œåˆ†åŒºçš„MDPçŠ¶æ€è¡¨ç¤º
    
    çŠ¶æ€ç»„ä»¶ï¼š
    1. èŠ‚ç‚¹ç‰¹å¾åµŒå…¥ï¼ˆH'ï¼‰- æ¥è‡ªGATç¼–ç å™¨çš„é™æ€çŸ©é˜µï¼Œå¯èƒ½åŒ…å«æ³¨æ„åŠ›å¢å¼ºä¿¡æ¯
       - å¦‚æœæä¾›äº†æ³¨æ„åŠ›æƒé‡ï¼šH' = concat(H, H_attn)
       - å¦‚æœæ²¡æœ‰æ³¨æ„åŠ›æƒé‡ï¼šH' = Hï¼ˆåŸå§‹åµŒå…¥ï¼‰
    2. èŠ‚ç‚¹åˆ†é…æ ‡ç­¾ï¼ˆz_tï¼‰- åŠ¨æ€åˆ†åŒºåˆ†é…
    3. è¾¹ç•ŒèŠ‚ç‚¹ï¼ˆBdry_tï¼‰- ä¸ä¸åŒåˆ†åŒºä¸­é‚»å±…ç›¸è¿çš„èŠ‚ç‚¹
    4. åŒºåŸŸèšåˆåµŒå…¥ - æ¯ä¸ªåŒºåŸŸçš„å‡å€¼/æœ€å¤§æ± åŒ–åµŒå…¥
    """
    
    def __init__(self,
                 hetero_data: HeteroData,
                 node_embeddings: Dict[str, torch.Tensor],
                 device: torch.device,
                 config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨

        Args:
            hetero_data: å¼‚æ„å›¾æ•°æ®
            node_embeddings: æ¥è‡ªGATç¼–ç å™¨çš„é¢„è®¡ç®—èŠ‚ç‚¹åµŒå…¥
                           - å¯èƒ½æ˜¯åŸå§‹åµŒå…¥Hï¼Œä¹Ÿå¯èƒ½æ˜¯å¢å¼ºåµŒå…¥H' = concat(H, H_attn)
                           - å…·ä½“å–å†³äºEnvironmentæ˜¯å¦æä¾›äº†æ³¨æ„åŠ›æƒé‡
            device: è®¡ç®—è®¾å¤‡
            config: é…ç½®å­—å…¸ï¼Œç”¨äºæ§åˆ¶è¾“å‡ºè¯¦ç»†ç¨‹åº¦
        """
        self.device = device
        self.hetero_data = hetero_data.to(device)
        self.config = config

        # è·å–è°ƒè¯•é…ç½®
        debug_config = config.get('debug', {}) if config else {}
        self.training_output = debug_config.get('training_output', {})

        # è®¾ç½®èŠ‚ç‚¹æ˜ å°„å’ŒåµŒå…¥
        self._setup_node_mappings()
        self._setup_node_embeddings(node_embeddings)
        self._setup_adjacency_info()

        # åˆ›å»ºæ™ºèƒ½åŒºåŸŸåµŒå…¥ç”Ÿæˆå™¨
        self.region_embedder = IntelligentRegionEmbedding(
            self.embedding_dim,
            self.device,
            config=config.get('region_embedding', {
                'use_learned_empty_embedding': True,
                'use_positional_encoding': True,
                'use_historical_decay': True,
                'use_advanced_pooling': False
            }) if config else {}
        )

        # çŠ¶æ€å˜é‡
        self.current_partition = None
        self.boundary_nodes = None
        self.region_embeddings = None
        self.current_step = 0
        
    def _setup_node_mappings(self):
        """è®¾ç½®å±€éƒ¨å’Œå…¨å±€èŠ‚ç‚¹ç´¢å¼•ä¹‹é—´çš„æ˜ å°„"""
        self.node_types = list(self.hetero_data.x_dict.keys())
        self.total_nodes = sum(x.shape[0] for x in self.hetero_data.x_dict.values())
        
        # åˆ›å»ºä»å±€éƒ¨ç´¢å¼•åˆ°å…¨å±€ç´¢å¼•çš„æ˜ å°„
        self.local_to_global_map = {}
        self.global_to_local_map = {}
        
        global_idx = 0
        for node_type in self.node_types:
            num_nodes = self.hetero_data.x_dict[node_type].shape[0]
            
            # è¯¥ç±»å‹çš„å±€éƒ¨åˆ°å…¨å±€æ˜ å°„
            local_indices = torch.arange(num_nodes, device=self.device)
            global_indices = torch.arange(global_idx, global_idx + num_nodes, device=self.device)
            
            self.local_to_global_map[node_type] = global_indices
            
            # å…¨å±€åˆ°å±€éƒ¨æ˜ å°„
            for local_idx, global_idx_val in zip(local_indices, global_indices):
                self.global_to_local_map[global_idx_val.item()] = (node_type, local_idx.item())
                
            global_idx += num_nodes
            
    def _setup_node_embeddings(self, node_embeddings: Dict[str, torch.Tensor]):
        """
        è®¾ç½®è¿æ¥çš„èŠ‚ç‚¹åµŒå…¥çŸ©é˜µH'
        
        è¿™é‡Œçš„node_embeddingså¯èƒ½æ˜¯ï¼š
        1. åŸå§‹GATåµŒå…¥Hï¼ˆå¦‚æœEnvironmentæ²¡æœ‰æä¾›æ³¨æ„åŠ›æƒé‡ï¼‰
        2. å¢å¼ºåµŒå…¥H' = concat(H, H_attn)ï¼ˆå¦‚æœEnvironmentæä¾›äº†æ³¨æ„åŠ›æƒé‡ï¼‰
        
        æ— è®ºå“ªç§æƒ…å†µï¼Œæˆ‘ä»¬éƒ½å°†å…¶ä½œä¸ºå®Œæ•´çš„èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µä½¿ç”¨
        """
        # å°†æ‰€æœ‰èŠ‚ç‚¹åµŒå…¥è¿æ¥æˆå•ä¸ªçŸ©é˜µ
        embedding_list = []
        for node_type in self.node_types:
            embeddings = node_embeddings[node_type].to(self.device)
            embedding_list.append(embeddings)
            
        self.node_embeddings = torch.cat(embedding_list, dim=0)  # å½¢çŠ¶ï¼š[total_nodes, embedding_dim]
        self.embedding_dim = self.node_embeddings.shape[1]
        
        # è®°å½•åµŒå…¥ç»´åº¦ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        show_state_manager_details = self.training_output.get('show_state_manager_details', True)
        only_show_errors = self.training_output.get('only_show_errors', False)

        if show_state_manager_details and not only_show_errors:
            print(f"ğŸ”§ StateManager: è®¾ç½®èŠ‚ç‚¹åµŒå…¥çŸ©é˜µï¼Œå½¢çŠ¶ {self.node_embeddings.shape}")
            print(f"   - æ€»èŠ‚ç‚¹æ•°: {self.total_nodes}")
            print(f"   - åµŒå…¥ç»´åº¦: {self.embedding_dim}")
            print(f"   - æ³¨æ„ï¼šæ­¤åµŒå…¥å¯èƒ½åŒ…å«GATåŸå§‹åµŒå…¥ + æ³¨æ„åŠ›å¢å¼ºä¿¡æ¯")
        
    def _setup_adjacency_info(self):
        """è®¾ç½®ç”¨äºè¾¹ç•ŒèŠ‚ç‚¹è®¡ç®—çš„é‚»æ¥ä¿¡æ¯ - ä½¿ç”¨ç¨€ç–å¼ é‡ä¼˜åŒ–"""
        # æ”¶é›†æ‰€æœ‰è¾¹ç”¨äºæ„å»ºç¨€ç–å¼ é‡
        all_edges = []
        
        for edge_type, edge_index in self.hetero_data.edge_index_dict.items():
            src_type, _, dst_type = edge_type
            
            # è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•
            src_global = self.local_to_global(edge_index[0], src_type)
            dst_global = self.local_to_global(edge_index[1], dst_type)
            
            # æ·»åŠ è¾¹ï¼ˆæ— å‘å›¾éœ€è¦åŒå‘è¾¹ï¼‰
            edges = torch.stack([src_global, dst_global], dim=0)
            all_edges.append(edges)
            # æ·»åŠ åå‘è¾¹
            all_edges.append(torch.stack([dst_global, src_global], dim=0))
        
        # åˆå¹¶æ‰€æœ‰è¾¹
        if all_edges:
            edge_index = torch.cat(all_edges, dim=1)
            # å»é‡
            edge_index = torch.unique(edge_index, dim=1)
        else:
            edge_index = torch.zeros((2, 0), dtype=torch.long, device=self.device)
        
        # åˆ›å»ºç¨€ç–å¼ é‡
        edge_attr = torch.ones(edge_index.shape[1], device=self.device)
        self.adjacency_sparse = torch_sparse.SparseTensor(
            row=edge_index[0],
            col=edge_index[1],
            value=edge_attr,
            sparse_sizes=(self.total_nodes, self.total_nodes)
        ).to(self.device).requires_grad_(False)
        
        # ç¼“å­˜è¾¹ç´¢å¼•ç”¨äºå‘é‡åŒ–è¾¹ç•ŒèŠ‚ç‚¹è®¡ç®—
        self.edge_index = edge_index
        
        # é¢„è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„é‚»å±…ç´¢å¼•ï¼ˆç”¨äºå¢é‡æ›´æ–°ï¼‰
        self._precompute_neighbors()
    
    def _precompute_neighbors(self):
        """é¢„è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„é‚»å±…ï¼Œç”¨äºå¿«é€ŸæŸ¥è¯¢"""
        self.neighbors_cache = {}
        
        # ä»ç¨€ç–å¼ é‡ä¸­æå–æ¯ä¸ªèŠ‚ç‚¹çš„é‚»å±…
        row_ptr, col, _ = self.adjacency_sparse.csr()
        for node_idx in range(self.total_nodes):
            start_idx = row_ptr[node_idx]
            end_idx = row_ptr[node_idx + 1]
            neighbors = col[start_idx:end_idx]
            self.neighbors_cache[node_idx] = neighbors
        
    def local_to_global(self, local_indices: torch.Tensor, node_type: str) -> torch.Tensor:
        """å°†ç»™å®šèŠ‚ç‚¹ç±»å‹çš„å±€éƒ¨ç´¢å¼•è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•"""
        return self.local_to_global_map[node_type][local_indices]
        
    def global_to_local(self, global_idx: int) -> Tuple[str, int]:
        """å°†å…¨å±€ç´¢å¼•è½¬æ¢ä¸º(node_type, local_index)"""
        return self.global_to_local_map[global_idx]
        
    def reset(self, initial_partition: torch.Tensor):
        """
        ä½¿ç”¨åˆå§‹åˆ†åŒºé‡ç½®çŠ¶æ€
        
        Args:
            initial_partition: åˆå§‹åˆ†åŒºåˆ†é… [total_nodes]
        """
        self.current_partition = initial_partition.to(self.device)
        self._update_derived_state()
        
    def update_partition(self, node_idx: int, new_partition: int):
        """
        æ›´æ–°å•ä¸ªèŠ‚ç‚¹çš„åˆ†åŒºåˆ†é…
        
        Args:
            node_idx: å…¨å±€èŠ‚ç‚¹ç´¢å¼•
            new_partition: æ–°çš„åˆ†åŒºåˆ†é…
        """
        old_partition = self.current_partition[node_idx].item()
        self.current_partition[node_idx] = new_partition
        
        # é«˜æ•ˆæ›´æ–°è¾¹ç•ŒèŠ‚ç‚¹
        self._update_boundary_nodes_incremental(node_idx, old_partition, new_partition)
        
        # æ›´æ–°å—å½±å“åˆ†åŒºçš„åŒºåŸŸåµŒå…¥
        self._update_region_embeddings_incremental(old_partition, new_partition)
        
    def _update_derived_state(self):
        """æ›´æ–°æ‰€æœ‰æ´¾ç”ŸçŠ¶æ€ç»„ä»¶"""
        self._compute_boundary_nodes()
        self._compute_region_embeddings()
        
    def _compute_boundary_nodes(self):
        """ä»å½“å‰åˆ†åŒºè®¡ç®—è¾¹ç•ŒèŠ‚ç‚¹ - å‘é‡åŒ–å®ç°"""
        if hasattr(self, 'edge_index') and self.edge_index.shape[1] > 0:
            # ä½¿ç”¨å‘é‡åŒ–æ–¹æ³•
            src, dst = self.edge_index[0], self.edge_index[1]
            
            # æ£€æŸ¥å“ªäº›è¾¹è¿æ¥ä¸åŒåˆ†åŒºçš„èŠ‚ç‚¹
            diff_mask = self.current_partition[src] != self.current_partition[dst]
            
            # æ‰¾åˆ°æ‰€æœ‰æ¶‰åŠè·¨åŒºè¿æ¥çš„èŠ‚ç‚¹
            boundary_nodes = torch.unique(
                torch.cat([src[diff_mask], dst[diff_mask]])
            )
            
            self.boundary_nodes = boundary_nodes
        else:
            # å¦‚æœæ²¡æœ‰è¾¹ï¼Œå°±æ²¡æœ‰è¾¹ç•ŒèŠ‚ç‚¹
            self.boundary_nodes = torch.empty(0, dtype=torch.long, device=self.device)
        
    def _update_boundary_nodes_incremental(self, changed_node: int, old_partition: int, new_partition: int):
        """å•ä¸ªèŠ‚ç‚¹å˜åŒ–åå¢é‡æ›´æ–°è¾¹ç•ŒèŠ‚ç‚¹"""
        # è½¬æ¢ä¸ºé›†åˆä»¥è¿›è¡Œé«˜æ•ˆæ“ä½œ
        if self.boundary_nodes is not None:
            try:
                boundary_set = set(self.boundary_nodes.cpu().numpy())
            except RuntimeError:
                boundary_set = set(self.boundary_nodes.cpu().tolist())
        else:
            boundary_set = set()
        
        # æ£€æŸ¥å˜åŒ–çš„èŠ‚ç‚¹
        is_boundary = False
        if changed_node in self.neighbors_cache:
            neighbors = self.neighbors_cache[changed_node]
            for neighbor_idx in neighbors:
                neighbor_partition = self.current_partition[neighbor_idx].item()
                if neighbor_partition != new_partition:
                    is_boundary = True
                    break
                
        if is_boundary:
            boundary_set.add(changed_node)
        else:
            boundary_set.discard(changed_node)
            
        # æ£€æŸ¥å˜åŒ–èŠ‚ç‚¹çš„æ‰€æœ‰é‚»å±…
        if changed_node in self.neighbors_cache:
            neighbors = self.neighbors_cache[changed_node]
            for neighbor_idx in neighbors:
                neighbor_idx_int = neighbor_idx.item()
                neighbor_partition = self.current_partition[neighbor_idx_int].item()
                
                # æ£€æŸ¥é‚»å±…æ˜¯å¦ç°åœ¨æ˜¯è¾¹ç•ŒèŠ‚ç‚¹
                neighbor_is_boundary = False
                if neighbor_idx_int in self.neighbors_cache:
                    neighbor_neighbors = self.neighbors_cache[neighbor_idx_int]
                    for neighbor_neighbor_idx in neighbor_neighbors:
                        neighbor_neighbor_partition = self.current_partition[neighbor_neighbor_idx].item()
                        if neighbor_neighbor_partition != neighbor_partition:
                            neighbor_is_boundary = True
                            break
                        
                if neighbor_is_boundary:
                    boundary_set.add(neighbor_idx_int)
                else:
                    boundary_set.discard(neighbor_idx_int)
                
        self.boundary_nodes = torch.tensor(list(boundary_set), dtype=torch.long, device=self.device)
        
    def _compute_region_embeddings(self):
        """è®¡ç®—æ‰€æœ‰åˆ†åŒºçš„åŒºåŸŸèšåˆåµŒå…¥ - ä½¿ç”¨æ™ºèƒ½åµŒå…¥ç”Ÿæˆå™¨"""
        num_partitions = self.current_partition.max().item()
        self.region_embeddings = {}

        # æ›´æ–°åµŒå…¥ç”Ÿæˆå™¨çš„æ­¥æ•°
        self.region_embedder.update_step(self.current_step)

        for partition_id in range(1, num_partitions + 1):
            # æ‰¾åˆ°è¯¥åˆ†åŒºä¸­çš„èŠ‚ç‚¹
            partition_mask = (self.current_partition == partition_id)
            partition_nodes = torch.where(partition_mask)[0]

            # ä½¿ç”¨æ™ºèƒ½åµŒå…¥ç”Ÿæˆå™¨
            self.region_embeddings[partition_id] = self.region_embedder.compute_region_embedding(
                partition_id, partition_nodes, self.node_embeddings
            )
                
    def _update_region_embeddings_incremental(self, old_partition: int, new_partition: int):
        """å¢é‡æ›´æ–°å—å½±å“åˆ†åŒºçš„åŒºåŸŸåµŒå…¥ - ä½¿ç”¨æ™ºèƒ½åµŒå…¥ç”Ÿæˆå™¨"""
        # æ›´æ–°åµŒå…¥ç”Ÿæˆå™¨çš„æ­¥æ•°
        self.region_embedder.update_step(self.current_step)

        for partition_id in [old_partition, new_partition]:
            partition_mask = (self.current_partition == partition_id)
            partition_nodes = torch.where(partition_mask)[0]

            # ä½¿ç”¨æ™ºèƒ½åµŒå…¥ç”Ÿæˆå™¨
            self.region_embeddings[partition_id] = self.region_embedder.compute_region_embedding(
                partition_id, partition_nodes, self.node_embeddings
            )

    def update_step(self, step):
        """æ›´æ–°å½“å‰æ­¥æ•°"""
        self.current_step = step
        if hasattr(self, 'region_embedder'):
            self.region_embedder.update_step(step)

    def get_embedding_statistics(self):
        """è·å–åµŒå…¥ç»Ÿè®¡ä¿¡æ¯"""
        if hasattr(self, 'region_embedder'):
            return self.region_embedder.get_embedding_statistics()
        return {}

    def get_observation(self) -> Dict[str, torch.Tensor]:
        """
        è·å–RLæ™ºèƒ½ä½“çš„å½“å‰çŠ¶æ€è§‚å¯Ÿ

        Returns:
            åŒ…å«çŠ¶æ€ç»„ä»¶çš„å­—å…¸
        """
        # å°†åŒºåŸŸåµŒå…¥ä½œä¸ºå¼ é‡
        # æ³¨æ„ï¼šregion_embeddingsçš„é”®æ˜¯å®é™…çš„åˆ†åŒºIDï¼Œä¸ä¸€å®šæ˜¯è¿ç»­çš„1,2,3...
        # å› ä¸ºæŸäº›åˆ†åŒºå¯èƒ½åœ¨è¾¹ç•ŒèŠ‚ç‚¹è®¾ç½®ä¸º0åå˜ä¸ºç©ºåˆ†åŒº
        if self.region_embeddings:
            partition_ids = sorted(self.region_embeddings.keys())
            region_embedding_tensor = torch.stack([
                self.region_embeddings[pid] for pid in partition_ids
            ], dim=0)
        else:
            # å¦‚æœæ²¡æœ‰åŒºåŸŸåµŒå…¥ï¼Œåˆ›å»ºç©ºå¼ é‡
            region_embedding_tensor = torch.empty(0, 2 * self.embedding_dim, device=self.device)
        
        # è¾¹ç•ŒèŠ‚ç‚¹ç‰¹å¾
        if len(self.boundary_nodes) > 0:
            boundary_features = self.node_embeddings[self.boundary_nodes]
        else:
            boundary_features = torch.empty(0, self.embedding_dim, device=self.device)
            
        observation = {
            'node_embeddings': self.node_embeddings,  # [total_nodes, embedding_dim]
            'region_embeddings': region_embedding_tensor,  # [num_partitions, 2*embedding_dim]
            'boundary_features': boundary_features,  # [num_boundary, embedding_dim]
            'current_partition': self.current_partition,  # [total_nodes]
            'boundary_nodes': self.boundary_nodes,  # [num_boundary]
        }
        
        return observation
        
    def get_boundary_nodes(self) -> torch.Tensor:
        """è·å–å½“å‰è¾¹ç•ŒèŠ‚ç‚¹"""
        return self.boundary_nodes if self.boundary_nodes is not None else torch.empty(0, dtype=torch.long, device=self.device)
        
    def get_global_node_mapping(self) -> Dict[str, torch.Tensor]:
        """è·å–ä»èŠ‚ç‚¹ç±»å‹åˆ°å…¨å±€ç´¢å¼•çš„æ˜ å°„"""
        return self.local_to_global_map
        
    def get_partition_info(self) -> Dict[str, torch.Tensor]:
        """è·å–è¯¦ç»†çš„åˆ†åŒºä¿¡æ¯"""
        num_partitions = self.current_partition.max().item()
        partition_sizes = torch.bincount(self.current_partition, minlength=num_partitions + 1)[1:]
        
        return {
            'partition_assignments': self.current_partition,
            'partition_sizes': partition_sizes,
            'num_partitions': num_partitions,
            'boundary_nodes': self.get_boundary_nodes()
        }
