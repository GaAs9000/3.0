"""
ç”µåŠ›ç½‘ç»œåˆ†åŒºå¼ºåŒ–å­¦ä¹ çš„å®ç”¨å‡½æ•°

æœ¬æ¨¡å—æä¾›å®ç”¨å‡½æ•°ï¼ŒåŒ…æ‹¬ï¼š
- ç”¨äºåˆå§‹åˆ†åŒºçš„METISåˆå§‹åŒ–
- åˆ†åŒºè´¨é‡è¯„ä¼°
- çŠ¶æ€ç®¡ç†åŠ©æ‰‹
- å¯è§†åŒ–å·¥å…·
"""

import torch
import numpy as np
import networkx as nx
import random
from typing import Dict, List, Tuple, Optional, Any
from torch_geometric.data import HeteroData
import warnings

try:
    import pymetis
    METIS_AVAILABLE = True
except (ImportError, RuntimeError):
    METIS_AVAILABLE = False
    warnings.warn("PyMetis åº“åŠ è½½å¤±è´¥ã€‚å°†ä½¿ç”¨å¤‡ç”¨æ–¹æ³•ï¼ˆè°±èšç±»æˆ–éšæœºåˆ†åŒºï¼‰è¿›è¡Œåˆå§‹åŒ–ã€‚")

try:
    from sklearn.cluster import SpectralClustering
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    warnings.warn("Scikit-learnä¸å¯ç”¨ã€‚ä½¿ç”¨éšæœºåˆå§‹åŒ–ä½œä¸ºå›é€€ã€‚")


class MetisInitializer:
    """
    åŸºäºMETISçš„ç”µåŠ›ç½‘ç»œåˆ†åŒºåˆå§‹åŒ–
    
    ä½¿ç”¨METISå›¾åˆ†åŒºç®—æ³•æä¾›åˆå§‹åˆ†åŒºï¼Œ
    èŠ‚ç‚¹æƒé‡åŸºäºåŠŸç‡è´Ÿè½½ã€‚
    """
    
    def __init__(self, hetero_data: HeteroData, device: torch.device):
        """
        åˆå§‹åŒ–METISåˆ†åŒºå™¨
        
        Args:
            hetero_data: å¼‚æ„å›¾æ•°æ®
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device
        self.hetero_data = hetero_data.to(device)
        
        # è®¾ç½®ä¸METISå…¼å®¹çš„å›¾è¡¨ç¤º
        self._setup_graph_representation()
        
    def _setup_graph_representation(self):
        """è®¾ç½®ä¸METISå…¼å®¹çš„å›¾è¡¨ç¤º"""
        # è·å–èŠ‚ç‚¹æ€»æ•°
        self.total_nodes = sum(x.shape[0] for x in self.hetero_data.x_dict.values())
        
        # è®¾ç½®èŠ‚ç‚¹æ˜ å°„
        self._setup_node_mappings()
        
        # æå–èŠ‚ç‚¹æƒé‡ï¼ˆåŠŸç‡è´Ÿè½½ï¼‰
        self._extract_node_weights()
        
        # ä¸ºMETISæ„å»ºé‚»æ¥åˆ—è¡¨
        self._build_adjacency_list()
        
    def _setup_node_mappings(self):
        """è®¾ç½®å±€éƒ¨å’Œå…¨å±€èŠ‚ç‚¹ç´¢å¼•ä¹‹é—´çš„æ˜ å°„"""
        self.node_types = list(self.hetero_data.x_dict.keys())
        self.local_to_global_map = {}
        
        global_idx = 0
        for node_type in self.node_types:
            num_nodes = self.hetero_data.x_dict[node_type].shape[0]
            global_indices = torch.arange(global_idx, global_idx + num_nodes, device=self.device)
            self.local_to_global_map[node_type] = global_indices
            global_idx += num_nodes
            
    def _local_to_global(self, local_indices: torch.Tensor, node_type: str) -> torch.Tensor:
        """å°†å±€éƒ¨ç´¢å¼•è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•"""
        return self.local_to_global_map[node_type][local_indices]
        
    def _extract_node_weights(self):
        """åŸºäºåŠŸç‡è´Ÿè½½æå–èŠ‚ç‚¹æƒé‡"""
        # è¿æ¥æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾
        all_features = []
        for node_type in self.node_types:
            features = self.hetero_data.x_dict[node_type]
            all_features.append(features)
            
        all_node_features = torch.cat(all_features, dim=0)
        
        # æå–è´Ÿè½½æ•°æ®ï¼ˆå‡è®¾Pdåœ¨ç´¢å¼•0å¤„ï¼‰
        if all_node_features.shape[1] > 0:
            loads = all_node_features[:, 0]  # Pdåˆ—
            # è½¬æ¢ä¸ºMETISçš„æ­£æ•´æ•°ï¼ˆç¼©æ”¾å¹¶æ·»åŠ åç§»ï¼‰
            loads_scaled = (loads * 1000 + 1000).clamp(min=1).int()
            try:
                self.node_weights = loads_scaled.cpu().numpy()
            except RuntimeError:
                # numpyè½¬æ¢å¤±è´¥æ—¶çš„å›é€€
                self.node_weights = [int(x) for x in loads_scaled.cpu().tolist()]
        else:
            # å¦‚æœæ²¡æœ‰è´Ÿè½½æ•°æ®åˆ™ä½¿ç”¨å‡åŒ€æƒé‡
            self.node_weights = [1] * self.total_nodes
            
    def _build_adjacency_list(self):
        """
        ä»å¼‚æ„å›¾æ„å»ºé‚»æ¥åˆ—è¡¨ (ä¿®å¤ç‰ˆæœ¬)
        """
        self.adjacency_list = [[] for _ in range(self.total_nodes)]

        # éå†æ‰€æœ‰è¾¹ç±»å‹ï¼Œæ„å»ºé‚»æ¥å…³ç³»
        for edge_type in self.hetero_data.edge_types:
            edge_index = self.hetero_data[edge_type].edge_index

            # è§£æè¾¹ç±»å‹å…ƒç»„ (src_node_type, relation, dst_node_type)
            src_node_type, relation, dst_node_type = edge_type

            # ä½¿ç”¨å·²æœ‰çš„æ˜ å°„æ–¹æ³•è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•
            src_global = self._local_to_global(edge_index[0], src_node_type)
            dst_global = self._local_to_global(edge_index[1], dst_node_type)

            # æ·»åŠ è¾¹åˆ°é‚»æ¥åˆ—è¡¨
            for src, dst in zip(src_global, dst_global):
                src_idx = src.item()
                dst_idx = dst.item()

                # æ£€æŸ¥å…¨å±€ç´¢å¼•æœ‰æ•ˆæ€§
                if 0 <= src_idx < self.total_nodes and 0 <= dst_idx < self.total_nodes:
                    # æ·»åŠ åŒå‘è¿æ¥ï¼ˆæ— å‘å›¾ï¼‰
                    if dst_idx not in self.adjacency_list[src_idx]:
                        self.adjacency_list[src_idx].append(dst_idx)
                    if src_idx not in self.adjacency_list[dst_idx]:
                        self.adjacency_list[dst_idx].append(src_idx)

        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        edge_count = sum(len(neighbors) for neighbors in self.adjacency_list) // 2
        non_isolated = sum(1 for neighbors in self.adjacency_list if len(neighbors) > 0)
        print(f"ğŸ”— æ„å»ºé‚»æ¥åˆ—è¡¨: {edge_count} æ¡è¾¹, {non_isolated} ä¸ªéå­¤ç«‹èŠ‚ç‚¹")
        
    def initialize_partition(self, num_partitions: int) -> torch.Tensor:
        """
        ã€æœ€ç»ˆç‰ˆã€‘ä½¿ç”¨METISåˆå§‹åŒ–åˆ†åŒºï¼Œä¿è¯è¿é€šæ€§ï¼Œå¹¶ä¸ºRLåˆ›é€ åˆå§‹åŠ¨ä½œç©ºé—´ã€‚
        """
        partition_tensor = None
        if METIS_AVAILABLE and self.total_nodes > num_partitions:
            try:
                # æ­¥éª¤1: è·å–åŸºç¡€åˆ†åŒº
                partition_tensor = self._metis_partition(num_partitions)
            except Exception as e:
                warnings.warn(f"METISåˆ†åŒºå¤±è´¥ï¼š{e}ã€‚ä½¿ç”¨å›é€€æ–¹æ³•ã€‚")

        if partition_tensor is None:
            if SKLEARN_AVAILABLE:
                partition_tensor = self._spectral_partition(num_partitions)
            else:
                partition_tensor = self._random_partition(num_partitions)

        # æ­¥éª¤2: ä¿è¯åˆ†åŒºå†…éƒ¨è¿é€šæ€§
        repaired_partition = self._check_and_repair_connectivity(partition_tensor, num_partitions)

        # ã€æ–°å¢ã€‘æ­¥éª¤3: åˆ›é€ åˆå§‹åŠ¨ä½œç©ºé—´ï¼Œå°†è¾¹ç•ŒèŠ‚ç‚¹ç½®ä¸º"æœªåˆ†åŒº"(æ ‡ç­¾0)
        final_partition = self._create_action_space_on_boundaries(repaired_partition)

        return final_partition
            
    def _metis_partition(self, num_partitions: int) -> torch.Tensor:
        """ä½¿ç”¨PyMetisç®—æ³•åˆ†åŒº"""
        # æ£€æŸ¥æ˜¯å¦æœ‰è¾¹
        if not any(self.adjacency_list):
            # æ²¡æœ‰è¾¹ - ä½¿ç”¨éšæœºåˆ†åŒº
            return self._random_partition(num_partitions)
            
        try:
            # PyMetis éœ€è¦é‚»æ¥åˆ—è¡¨æ ¼å¼ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ numpy æ•°ç»„
            adjacency_list = [np.array(neighbors, dtype=np.int32) for neighbors in self.adjacency_list]
            
            # ä½¿ç”¨ PyMetis è¿›è¡Œåˆ†åŒº
            n_cuts, partition = pymetis.part_graph(num_partitions, adjacency=adjacency_list)
            
            # PyMetis è¿”å› 0-based æ ‡ç­¾ï¼Œè½¬æ¢ä¸º 1-based
            partition_tensor = torch.tensor(partition, device=self.device) + 1
            
            # print(f"âœ… PyMetis åˆå§‹åŒ–åˆ†åŒºæˆåŠŸï¼šåˆ‡è¾¹æ•° = {n_cuts}")
            return partition_tensor
            
        except Exception as e:
            warnings.warn(f"PyMetiså¤±è´¥ï¼š{e}ã€‚ä½¿ç”¨è°±èšç±»å›é€€ã€‚")
            return self._spectral_partition(num_partitions)
            
    def _spectral_partition(self, num_partitions: int) -> torch.Tensor:
        """ä½¿ç”¨è°±èšç±»åˆ†åŒº"""
        # æ„å»ºé‚»æ¥çŸ©é˜µ
        adj_matrix = np.zeros((self.total_nodes, self.total_nodes))
        
        for i, neighbors in enumerate(self.adjacency_list):
            for j in neighbors:
                adj_matrix[i, j] = 1.0
                
        # å¤„ç†è¾¹ç•Œæƒ…å†µ
        if np.sum(adj_matrix) == 0:
            return self._random_partition(num_partitions)
            
        try:
            clustering = SpectralClustering(
                n_clusters=num_partitions,
                affinity='precomputed',
                random_state=42
            )
            
            partition = clustering.fit_predict(adj_matrix)
            
            # è½¬æ¢ä¸ºåŸºäº1çš„ç´¢å¼•å’Œtorchå¼ é‡
            partition_tensor = torch.tensor(partition, device=self.device) + 1
            return partition_tensor
            
        except Exception as e:
            warnings.warn(f"è°±èšç±»å¤±è´¥ï¼š{e}ã€‚ä½¿ç”¨éšæœºåˆ†åŒºã€‚")
            return self._random_partition(num_partitions)
            
    def _random_partition(self, num_partitions: int) -> torch.Tensor:
        """ä½œä¸ºæœ€ç»ˆå›é€€çš„éšæœºåˆ†åŒº"""
        partition = torch.randint(
            1, num_partitions + 1,
            (self.total_nodes,),
            device=self.device
        )
        return partition

    def _check_and_repair_connectivity(self, partition_labels: torch.Tensor, num_partitions: int) -> torch.Tensor:
        """
        æ£€æŸ¥å¹¶ä¿®å¤åˆ†åŒºè¿é€šæ€§

        Args:
            partition_labels: åˆå§‹åˆ†åŒºæ ‡ç­¾
            num_partitions: åˆ†åŒºæ•°é‡

        Returns:
            ä¿®å¤åçš„åˆ†åŒºæ ‡ç­¾
        """
        # print("ğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤åˆ†åŒºè¿é€šæ€§...")
        labels_np = partition_labels.cpu().numpy()

        # æ„å»ºNetworkXå›¾
        G = nx.Graph()
        G.add_nodes_from(range(self.total_nodes))

        # æ·»åŠ è¾¹
        for i in range(self.total_nodes):
            for neighbor in self.adjacency_list[i]:
                G.add_edge(i, neighbor)

        # æ£€æŸ¥æ¯ä¸ªåˆ†åŒºçš„è¿é€šæ€§
        repaired_labels = labels_np.copy()
        needs_repair = True
        repair_iterations = 0
        max_repair_iterations = 3

        while needs_repair and repair_iterations < max_repair_iterations:
            needs_repair = False
            repair_iterations += 1
            
            for partition_id in range(1, num_partitions + 1):
                partition_nodes = np.where(repaired_labels == partition_id)[0]

                if len(partition_nodes) <= 1:
                    continue

                # æå–å­å›¾
                subgraph = G.subgraph(partition_nodes)

                # æ£€æŸ¥è¿é€šæ€§
                if not nx.is_connected(subgraph):
                    # print(f"âš ï¸ åˆ†åŒº {partition_id} ä¸è¿é€šï¼Œæ­£åœ¨ä¿®å¤... (ç¬¬{repair_iterations}æ¬¡å°è¯•)")
                    needs_repair = True

                    # è·å–è¿é€šåˆ†é‡ï¼ŒæŒ‰å¤§å°æ’åº
                    components = sorted(list(nx.connected_components(subgraph)), key=len, reverse=True)
                    largest_component = components[0]

                    # å°†è¾ƒå°åˆ†é‡çš„èŠ‚ç‚¹é‡æ–°åˆ†é…
                    for component in components[1:]:
                        for node in component:
                            # æ‰¾åˆ°è¯¥èŠ‚ç‚¹çš„é‚»å±…åˆ†åŒºç»Ÿè®¡
                            neighbor_partitions_count = {}
                            for neighbor in self.adjacency_list[node]:
                                if neighbor < len(repaired_labels):
                                    neighbor_partition = repaired_labels[neighbor]
                                    if neighbor_partition != 0:  # å¿½ç•¥æœªåˆ†åŒºçš„é‚»å±…
                                        neighbor_partitions_count[neighbor_partition] = neighbor_partitions_count.get(neighbor_partition, 0) + 1

                            if neighbor_partitions_count:
                                # é€‰æ‹©è¿æ¥æœ€å¤šçš„é‚»å±…åˆ†åŒº
                                best_partition = max(neighbor_partitions_count, key=neighbor_partitions_count.get)
                                repaired_labels[node] = best_partition
                            else:
                                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆé‚»å±…ï¼Œä¿æŒåœ¨ä¸»åˆ†é‡ä¸­
                                pass

        if repair_iterations >= max_repair_iterations:
            warnings.warn(f"è¿é€šæ€§ä¿®å¤è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°({max_repair_iterations})ï¼Œå¯èƒ½ä»æœ‰ä¸è¿é€šçš„åˆ†åŒº")
        # else:
            # print("âœ… åˆ†åŒºè¿é€šæ€§ä¿®å¤å®Œæˆ")
            
        return torch.from_numpy(repaired_labels).to(self.device)

    def _create_action_space_on_boundaries(self, partition_labels: torch.Tensor) -> torch.Tensor:
        """
        ã€æ–°å¢ã€‘è¯†åˆ«åˆ†åŒºè¾¹ç•Œï¼Œå¹¶å°†è¾¹ç•ŒèŠ‚ç‚¹ç½®ä¸º"æœªåˆ†åŒº"(0)ï¼Œä¸ºRL Agentåˆ›é€ åŠ¨ä½œç©ºé—´ã€‚
        """
        # print("ğŸ” è¯†åˆ«è¾¹ç•ŒèŠ‚ç‚¹å¹¶åˆ›é€ åˆå§‹åŠ¨ä½œç©ºé—´...")
        labels_np = partition_labels.cpu().numpy()
        boundary_nodes = set()

        # éå†æ‰€æœ‰è¾¹æ¥æ‰¾åˆ°è¾¹ç•ŒèŠ‚ç‚¹
        for i in range(self.total_nodes):
            for neighbor in self.adjacency_list[i]:
                if labels_np[i] != labels_np[neighbor]:
                    boundary_nodes.add(i)
                    # åªè¦å‘ç°ä¸€ä¸ªé‚»å±…åœ¨ä¸åŒåŒºï¼Œå°±æ˜¯è¾¹ç•ŒèŠ‚ç‚¹ï¼Œå¯ä»¥è·³å‡ºå†…å±‚å¾ªç¯
                    break

        if not boundary_nodes:
            # å¦‚æœæ²¡æœ‰è¾¹ç•Œï¼ˆä¾‹å¦‚ï¼Œå›¾æœ¬èº«å°±æ˜¯éè¿é€šçš„ï¼‰ï¼Œéšæœºé€‰æ‹©ä¸€äº›èŠ‚ç‚¹
            warnings.warn("æœªå‘ç°è¾¹ç•ŒèŠ‚ç‚¹ï¼Œéšæœºé€‰æ‹©5%çš„èŠ‚ç‚¹ä½œä¸ºå¯ç§»åŠ¨èŠ‚ç‚¹ã€‚")
            num_to_unassign = max(1, int(self.total_nodes * 0.05))
            nodes_to_unassign = random.sample(range(self.total_nodes), num_to_unassign)
        else:
            nodes_to_unassign = list(boundary_nodes)

        # å°†è¿™äº›è¾¹ç•ŒèŠ‚ç‚¹çš„åˆ†åŒºæ ‡ç­¾è®¾ç½®ä¸º0
        final_labels_np = labels_np.copy()
        final_labels_np[nodes_to_unassign] = 0

        # print(f"âœ… æˆåŠŸå°† {len(nodes_to_unassign)} ä¸ªè¾¹ç•ŒèŠ‚ç‚¹ç½®ä¸º'æœªåˆ†åŒº'çŠ¶æ€ã€‚")

        return torch.from_numpy(final_labels_np).to(self.device)


class PartitionEvaluator:
    """
    ç»¼åˆåˆ†åŒºè´¨é‡è¯„ä¼°
    
    æä¾›è¯„ä¼°åˆ†åŒºè´¨é‡çš„å„ç§æŒ‡æ ‡ï¼ŒåŒ…æ‹¬
    ç”µæ°”ã€æ‹“æ‰‘å’Œè´Ÿè½½å¹³è¡¡æŒ‡æ ‡ã€‚
    """
    
    def __init__(self, hetero_data: HeteroData, device: torch.device):
        """
        åˆå§‹åŒ–åˆ†åŒºè¯„ä¼°å™¨
        
        Args:
            hetero_data: å¼‚æ„å›¾æ•°æ®
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device
        self.hetero_data = hetero_data.to(device)
        
        # è®¾ç½®è¯„ä¼°æ•°æ®
        self._setup_evaluation_data()
        
    def _setup_evaluation_data(self):
        """è®¾ç½®è¯„ä¼°æ‰€éœ€çš„æ•°æ®"""
        # è®¾ç½®èŠ‚ç‚¹æ˜ å°„
        self._setup_node_mappings()
        
        # æå–åŠŸç‡æ•°æ®
        self._extract_power_data()
        
        # æå–ç”µæ°”æ•°æ®
        self._extract_electrical_data()
        
    def _setup_node_mappings(self):
        """è®¾ç½®èŠ‚ç‚¹ç±»å‹æ˜ å°„"""
        self.node_types = list(self.hetero_data.x_dict.keys())
        self.local_to_global_map = {}
        
        global_idx = 0
        for node_type in self.node_types:
            num_nodes = self.hetero_data.x_dict[node_type].shape[0]
            global_indices = torch.arange(global_idx, global_idx + num_nodes, device=self.device)
            self.local_to_global_map[node_type] = global_indices
            global_idx += num_nodes
            
        self.total_nodes = global_idx
        
    def _local_to_global(self, local_indices: torch.Tensor, node_type: str) -> torch.Tensor:
        """å°†å±€éƒ¨ç´¢å¼•è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•"""
        return self.local_to_global_map[node_type][local_indices]
        
    def _extract_power_data(self):
        """ä»èŠ‚ç‚¹ç‰¹å¾ä¸­æå–åŠŸç‡æ•°æ®"""
        all_features = []
        for node_type in self.node_types:
            features = self.hetero_data.x_dict[node_type]
            all_features.append(features)
            
        self.all_node_features = torch.cat(all_features, dim=0)
        
        # æå–åŠŸç‡æ•°æ®
        self.load_active = self.all_node_features[:, 0]  # Pd
        if self.all_node_features.shape[1] > 9:
            self.gen_active = self.all_node_features[:, 9]  # Pg
        else:
            self.gen_active = torch.zeros_like(self.load_active)
            
    def _extract_electrical_data(self):
        """ä»è¾¹ä¸­æå–ç”µæ°”æ•°æ®"""
        self.all_edges = []
        self.all_admittances = []
        
        for edge_type, edge_index in self.hetero_data.edge_index_dict.items():
            edge_attr = self.hetero_data.edge_attr_dict[edge_type]
            src_type, _, dst_type = edge_type
            
            # è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•
            src_global = self._local_to_global(edge_index[0], src_type)
            dst_global = self._local_to_global(edge_index[1], dst_type)
            
            global_edges = torch.stack([src_global, dst_global], dim=0)
            self.all_edges.append(global_edges)
            
            # æå–å¯¼çº³
            if edge_attr.shape[1] > 4:
                admittances = edge_attr[:, 4]  # yåˆ—
            else:
                admittances = torch.ones(edge_attr.shape[0], device=self.device)
                
            self.all_admittances.append(admittances)
            
        if self.all_edges:
            self.edge_index = torch.cat(self.all_edges, dim=1)
            self.edge_admittances = torch.cat(self.all_admittances, dim=0)
        else:
            self.edge_index = torch.empty(2, 0, device=self.device)
            self.edge_admittances = torch.empty(0, device=self.device)
            
    def evaluate_partition(self, partition: torch.Tensor) -> Dict[str, float]:
        """
        ç»¼åˆåˆ†åŒºè¯„ä¼°
        
        Args:
            partition: åˆ†åŒºåˆ†é… [total_nodes]
            
        Returns:
            åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """
        metrics = {}
        
        # åŸºæœ¬åˆ†åŒºä¿¡æ¯
        num_partitions = partition.max().item()
        partition_sizes = torch.bincount(partition, minlength=num_partitions + 1)[1:]
        
        # è´Ÿè½½å¹³è¡¡æŒ‡æ ‡
        partition_loads = torch.zeros(num_partitions, device=self.device)
        for i in range(1, num_partitions + 1):
            mask = (partition == i)
            if mask.any():
                partition_loads[i-1] = self.load_active[mask].sum()
                
        load_mean = torch.mean(partition_loads)
        load_std = torch.std(partition_loads)
        load_cv = (load_std / load_mean).item() if load_mean > 0 else 0.0
        
        # è€¦åˆæŒ‡æ ‡
        if self.edge_index.shape[1] > 0:
            src_partitions = partition[self.edge_index[0]]
            dst_partitions = partition[self.edge_index[1]]
            coupling_mask = (src_partitions != dst_partitions)
            
            coupling_edges = coupling_mask.sum().item()
            total_coupling = self.edge_admittances[coupling_mask].sum().item() if coupling_mask.any() else 0.0
        else:
            coupling_edges = 0
            total_coupling = 0.0
            
        # åŠŸç‡å¹³è¡¡æŒ‡æ ‡
        power_imbalances = []
        for i in range(1, num_partitions + 1):
            mask = (partition == i)
            if mask.any():
                gen = self.gen_active[mask].sum()
                load = self.load_active[mask].sum()
                imbalance = abs(gen - load).item()
                power_imbalances.append(imbalance)
                
        # è¿é€šæ€§æ£€æŸ¥ï¼ˆç®€åŒ–ç‰ˆï¼‰
        connectivity = self._check_connectivity(partition)
        
        try:
            partition_sizes_list = partition_sizes.cpu().numpy().tolist()
        except RuntimeError:
            partition_sizes_list = partition_sizes.cpu().tolist()

        metrics.update({
            'num_partitions': num_partitions,
            'partition_sizes': partition_sizes_list,
            'load_cv': load_cv,
            'load_variance': torch.var(partition_loads).item(),
            'coupling_edges': coupling_edges,
            'total_coupling': total_coupling,
            'power_imbalance_mean': np.mean(power_imbalances) if power_imbalances else 0.0,
            'power_imbalance_max': np.max(power_imbalances) if power_imbalances else 0.0,
            'connectivity': connectivity
        })
        
        return metrics
        
    def _check_connectivity(self, partition: torch.Tensor) -> float:
        """
        æ£€æŸ¥åˆ†åŒºè¿é€šæ€§ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            partition: åˆ†åŒºåˆ†é…
            
        Returns:
            è¿é€šæ€§å¾—åˆ†ï¼ˆå¦‚æœæ‰€æœ‰åˆ†åŒºè¿é€šä¸º1.0ï¼Œå¦åˆ™æ›´ä½ï¼‰
        """
        # ç›®å‰è¿”å›1.0ï¼ˆå‡è®¾è¿é€šï¼‰
        # å®Œæ•´å®ç°å°†æ£€æŸ¥æ¯ä¸ªåˆ†åŒºå†…çš„å›¾è¿é€šæ€§
        return 1.0
