#!/usr/bin/env python3
"""
GNN Pre-trainer for Power Grid Partitioning

ç”Ÿäº§çº§çš„å›¾ç¥ç»ç½‘ç»œé¢„è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒï¼š
- å¤šä»»åŠ¡è‡ªç›‘ç£å­¦ä¹ ï¼ˆå¹³æ»‘æ€§ã€å¯¹æ¯”å­¦ä¹ ã€ç‰©ç†ä¸€è‡´æ€§ï¼‰
- å¤šGPUè®­ç»ƒæ”¯æŒ
- æ–­ç‚¹ç»­è®­
- æ—©åœæœºåˆ¶
- TensorBoardé›†æˆ
- å®Œæ•´çš„éªŒè¯è¯„ä¼°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from torch_geometric.data import Data, Batch, DataLoader
from torch_geometric.nn import global_mean_pool
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Union
from pathlib import Path
import logging
import time
import json
from dataclasses import dataclass
from collections import defaultdict
import warnings

logger = logging.getLogger(__name__)


@dataclass
class PretrainConfig:
    """é¢„è®­ç»ƒé…ç½®"""
    # è®­ç»ƒå‚æ•°
    epochs: int = 100
    batch_size: int = 32
    learning_rate: float = 1e-3
    weight_decay: float = 1e-5
    
    # æŸå¤±æƒé‡
    smoothness_weight: float = 1.0
    contrastive_weight: float = 0.5
    physics_weight: float = 0.3
    
    # æ—©åœå‚æ•°
    early_stopping_patience: int = 20
    early_stopping_min_delta: float = 1e-4
    
    # éªŒè¯å‚æ•°
    validation_split: float = 0.1
    validation_interval: int = 5
    
    # å¯¹æ¯”å­¦ä¹ å‚æ•°
    temperature: float = 0.5
    negative_samples: int = 10
    
    # æ£€æŸ¥ç‚¹å‚æ•°
    checkpoint_dir: str = "data/pretrain/checkpoints"
    save_interval: int = 10
    
    # è®¾å¤‡å‚æ•°
    device: str = "auto"
    num_workers: int = 4
    pin_memory: bool = True
    
    # æ—¥å¿—å‚æ•°
    log_dir: str = "data/pretrain/logs"
    log_interval: int = 10
    

class SmoothnessLoss(nn.Module):
    """
    å¹³æ»‘æ€§æŸå¤± - é¼“åŠ±ç›¸é‚»èŠ‚ç‚¹å…·æœ‰ç›¸ä¼¼çš„åµŒå…¥
    
    åŸºäºç”µæ°”è·ç¦»åŠ æƒï¼Œå¯¼çº³è¶Šå¤§ï¼ˆé˜»æŠ—è¶Šå°ï¼‰çš„è¿æ¥æƒé‡è¶Šé«˜
    """
    
    def __init__(self, admittance_index: int = 3):
        super().__init__()
        self.admittance_index = admittance_index
        
    def forward(self, node_embeddings: torch.Tensor, 
                edge_index: torch.Tensor,
                edge_attr: Optional[torch.Tensor] = None,
                batch: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è®¡ç®—å¹³æ»‘æ€§æŸå¤±
        
        Args:
            node_embeddings: èŠ‚ç‚¹åµŒå…¥ [N, D]
            edge_index: è¾¹ç´¢å¼• [2, E]
            edge_attr: è¾¹ç‰¹å¾ [E, F]ï¼ŒåŒ…å«å¯¼çº³ä¿¡æ¯
            batch: æ‰¹æ¬¡ç´¢å¼• [N]
            
        Returns:
            å¹³æ»‘æ€§æŸå¤±å€¼
        """
        src, dst = edge_index
        
        # è®¡ç®—ç›¸é‚»èŠ‚ç‚¹çš„åµŒå…¥è·ç¦»
        dist = F.mse_loss(node_embeddings[src], node_embeddings[dst], reduction='none')
        dist = dist.mean(dim=-1)  # [E]
        
        # æå–å¯¼çº³æƒé‡
        if edge_attr is not None and edge_attr.shape[1] > self.admittance_index:
            # ä½¿ç”¨å¯¼çº³ä½œä¸ºæƒé‡ï¼ˆå¯¼çº³è¶Šå¤§ï¼Œæƒé‡è¶Šé«˜ï¼‰
            admittance = edge_attr[:, self.admittance_index].clamp(min=1e-6)
            weights = admittance / admittance.mean()  # å½’ä¸€åŒ–æƒé‡
        else:
            # å¦‚æœæ²¡æœ‰å¯¼çº³ä¿¡æ¯ï¼Œä½¿ç”¨å‡åŒ€æƒé‡
            weights = torch.ones_like(dist)
            
        # åŠ æƒå¹³å‡
        weighted_loss = (dist * weights).mean()
        
        return weighted_loss


class ContrastiveLoss(nn.Module):
    """
    å¯¹æ¯”å­¦ä¹ æŸå¤± - å¢å¼ºèŠ‚ç‚¹è¡¨ç¤ºçš„åˆ¤åˆ«æ€§
    
    æ­£æ ·æœ¬ï¼šç›¸é‚»èŠ‚ç‚¹
    è´Ÿæ ·æœ¬ï¼šéç›¸é‚»çš„éšæœºèŠ‚ç‚¹
    """
    
    def __init__(self, temperature: float = 0.5):
        super().__init__()
        self.temperature = temperature
        
    def forward(self, node_embeddings: torch.Tensor,
                edge_index: torch.Tensor,
                num_neg_samples: int = 10,
                batch: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±
        
        Args:
            node_embeddings: èŠ‚ç‚¹åµŒå…¥ [N, D]
            edge_index: è¾¹ç´¢å¼• [2, E]
            num_neg_samples: æ¯ä¸ªæ­£æ ·æœ¬çš„è´Ÿæ ·æœ¬æ•°é‡
            batch: æ‰¹æ¬¡ç´¢å¼• [N]
            
        Returns:
            å¯¹æ¯”å­¦ä¹ æŸå¤±å€¼
        """
        device = node_embeddings.device
        N = node_embeddings.size(0)
        src, dst = edge_index
        
        # å½’ä¸€åŒ–åµŒå…¥
        node_embeddings = F.normalize(node_embeddings, p=2, dim=-1)
        
        # æ­£æ ·æœ¬ç›¸ä¼¼åº¦
        pos_sim = (node_embeddings[src] * node_embeddings[dst]).sum(dim=-1)  # [E]
        pos_sim = pos_sim / self.temperature
        
        # è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
        neg_sims = []
        for i in range(len(src)):
            # ä¸ºæ¯ä¸ªæºèŠ‚ç‚¹é‡‡æ ·è´Ÿæ ·æœ¬
            if batch is not None:
                # ç¡®ä¿è´Ÿæ ·æœ¬æ¥è‡ªåŒä¸€ä¸ªå›¾
                same_batch_mask = batch == batch[src[i]]
                candidate_nodes = torch.where(same_batch_mask)[0]
            else:
                candidate_nodes = torch.arange(N, device=device)
                
            # æ’é™¤æºèŠ‚ç‚¹å’Œç›®æ ‡èŠ‚ç‚¹
            mask = (candidate_nodes != src[i]) & (candidate_nodes != dst[i])
            candidate_nodes = candidate_nodes[mask]
            
            # éšæœºé‡‡æ ·
            if len(candidate_nodes) > num_neg_samples:
                neg_indices = torch.randperm(len(candidate_nodes))[:num_neg_samples]
                neg_nodes = candidate_nodes[neg_indices]
            else:
                neg_nodes = candidate_nodes
                
            # è®¡ç®—è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
            neg_sim = (node_embeddings[src[i]] * node_embeddings[neg_nodes]).sum(dim=-1)
            neg_sim = neg_sim / self.temperature
            neg_sims.append(neg_sim)
            
        # è®¡ç®—InfoNCEæŸå¤±
        loss = 0
        for i in range(len(pos_sim)):
            # åˆ†å­ï¼šæ­£æ ·æœ¬
            numerator = torch.exp(pos_sim[i])
            
            # åˆ†æ¯ï¼šæ­£æ ·æœ¬ + æ‰€æœ‰è´Ÿæ ·æœ¬
            denominator = numerator + torch.exp(neg_sims[i]).sum()
            
            # è´Ÿå¯¹æ•°ä¼¼ç„¶
            loss += -torch.log(numerator / denominator + 1e-8)
            
        return loss.mean()


class PhysicsConsistencyLoss(nn.Module):
    """
    ç‰©ç†ä¸€è‡´æ€§æŸå¤± - ç¡®ä¿åµŒå…¥åæ˜ ç”µåŠ›ç³»ç»Ÿçš„ç‰©ç†ç‰¹æ€§
    
    åŸºäºåŸºå°”éœå¤«å®šå¾‹å’ŒåŠŸç‡å¹³è¡¡çº¦æŸ
    """
    
    def __init__(self):
        super().__init__()
        
    def forward(self, node_embeddings: torch.Tensor,
                node_features: torch.Tensor,
                edge_index: torch.Tensor,
                edge_attr: Optional[torch.Tensor] = None,
                batch: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è®¡ç®—ç‰©ç†ä¸€è‡´æ€§æŸå¤±
        
        Args:
            node_embeddings: èŠ‚ç‚¹åµŒå…¥ [N, D]
            node_features: èŠ‚ç‚¹ç‰¹å¾ï¼ˆåŒ…å«è´Ÿè·ã€å‘ç”µç­‰ï¼‰ [N, F]
            edge_index: è¾¹ç´¢å¼• [2, E]
            edge_attr: è¾¹ç‰¹å¾ [E, F]
            batch: æ‰¹æ¬¡ç´¢å¼• [N]
            
        Returns:
            ç‰©ç†ä¸€è‡´æ€§æŸå¤±å€¼
        """
        # ç®€åŒ–ç‰ˆï¼šç¡®ä¿å…·æœ‰ç›¸ä¼¼è´Ÿè·/å‘ç”µç‰¹æ€§çš„èŠ‚ç‚¹å…·æœ‰ç›¸ä¼¼çš„åµŒå…¥
        
        # æå–è´Ÿè·å’Œå‘ç”µä¿¡æ¯ï¼ˆå‡è®¾åœ¨ç‰¹å¾çš„å‰å‡ ç»´ï¼‰
        if node_features.shape[1] >= 4:
            load = node_features[:, 2]  # æœ‰åŠŸè´Ÿè·
            gen = node_features[:, 3] if node_features.shape[1] > 3 else torch.zeros_like(load)
        else:
            # å¦‚æœç‰¹å¾ç»´åº¦ä¸å¤Ÿï¼Œè¿”å›0æŸå¤±
            return torch.tensor(0.0, device=node_embeddings.device)
            
        # è®¡ç®—åŠŸç‡ç‰¹å¾
        power_feature = (load - gen).unsqueeze(-1)  # [N, 1]
        
        # é€šè¿‡ä¸€ä¸ªå°ç½‘ç»œå°†åŠŸç‡ç‰¹å¾æ˜ å°„åˆ°åµŒå…¥ç©ºé—´
        power_embedding = torch.tanh(power_feature * 0.1)  # ç®€å•çš„éçº¿æ€§å˜æ¢
        
        # ç¡®ä¿åµŒå…¥çš„æŸä¸ªç»´åº¦åæ˜ åŠŸç‡å¹³è¡¡
        embedding_power_dim = node_embeddings[:, 0:1]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªç»´åº¦
        
        # MSEæŸå¤±
        physics_loss = F.mse_loss(embedding_power_dim, power_embedding)
        
        return physics_loss


class GNNPretrainer:
    """
    GNNé¢„è®­ç»ƒå™¨ - ç”Ÿäº§çº§å®ç°
    
    ç‰¹æ€§ï¼š
    - å¤šä»»åŠ¡è‡ªç›‘ç£å­¦ä¹ 
    - è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ
    - æ—©åœå’Œå­¦ä¹ ç‡è°ƒåº¦
    - å®Œæ•´çš„æ—¥å¿—å’Œå¯è§†åŒ–
    """
    
    def __init__(self, 
                 encoder: nn.Module,
                 config: Optional[PretrainConfig] = None,
                 checkpoint_path: Optional[str] = None):
        """
        Args:
            encoder: è¦é¢„è®­ç»ƒçš„GNNç¼–ç å™¨
            config: é¢„è®­ç»ƒé…ç½®
            checkpoint_path: æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„
        """
        self.encoder = encoder
        self.config = config or PretrainConfig()
        
        # è®¾å¤‡é…ç½®
        if self.config.device == "auto":
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(self.config.device)
            
        self.encoder.to(self.device)
        # ç¡®ä¿ç¼–ç å™¨ä½¿ç”¨float32ç²¾åº¦
        self.encoder.float()
        
        # æŸå¤±å‡½æ•°
        self.smoothness_loss = SmoothnessLoss()
        self.contrastive_loss = ContrastiveLoss(temperature=self.config.temperature)
        self.physics_loss = PhysicsConsistencyLoss()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.encoder.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, 
            mode='min', 
            factor=0.5, 
            patience=10,
            min_lr=1e-6
        )
        
        # æ··åˆç²¾åº¦è®­ç»ƒ - å·²ç¦ç”¨ä»¥ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´æ€§
        self.scaler = torch.amp.GradScaler(enabled=False)
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.training_history = defaultdict(list)
        
        # åˆ›å»ºç›®å½•
        Path(self.config.checkpoint_dir).mkdir(parents=True, exist_ok=True)
        Path(self.config.log_dir).mkdir(parents=True, exist_ok=True)
        
        # TensorBoard
        self.writer = SummaryWriter(self.config.log_dir)
        
        # æ¢å¤è®­ç»ƒ
        if checkpoint_path:
            self.load_checkpoint(checkpoint_path)
            
        logger.info(f"GNNé¢„è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ (è®¾å¤‡: {self.device})")
        
    def train(self, 
              data_generator,
              val_data: Optional[List[Data]] = None) -> Dict[str, torch.Tensor]:
        """
        æ‰§è¡Œé¢„è®­ç»ƒ
        
        Args:
            data_generator: æ•°æ®ç”Ÿæˆå™¨ï¼ˆå¦‚ScaleAwareSyntheticGeneratorï¼‰
            val_data: éªŒè¯æ•°æ®é›†
            
        Returns:
            é¢„è®­ç»ƒçš„æ¨¡å‹æƒé‡å­—å…¸
        """
        # è¯¦ç»†æ—¥å¿—
        logger.info(f"å¼€å§‹GNNé¢„è®­ç»ƒï¼Œå…± {self.config.epochs} è½®")

        # ç®€æ´æç¤ºï¼ˆæ— è®ºæ—¥å¿—çº§åˆ«å¦‚ä½•éƒ½æ˜¾ç¤ºï¼‰
        print(f"ğŸ§  å¼€å§‹GNNé¢„è®­ç»ƒ ({self.config.epochs}è½®) - è¯·è€å¿ƒç­‰å¾…...")
        
        for epoch in range(self.current_epoch, self.config.epochs):
            self.current_epoch = epoch
            
            # è®­ç»ƒé˜¶æ®µ
            train_loss = self._train_epoch(data_generator, epoch)
            
            # éªŒè¯é˜¶æ®µ
            if epoch % self.config.validation_interval == 0:
                val_loss = self._validate(val_data or self._generate_val_data(data_generator))
                
                # å­¦ä¹ ç‡è°ƒåº¦
                self.scheduler.step(val_loss)
                
                # æ—©åœæ£€æŸ¥
                if self._check_early_stopping(val_loss):
                    logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
                    break
                    
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % self.config.save_interval == 0:
                self._save_checkpoint(epoch)
                
            # æ—¥å¿—è®°å½•
            if epoch % self.config.log_interval == 0:
                self._log_progress(epoch, train_loss)
                
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_final_model()
        
        # å…³é—­TensorBoard
        self.writer.close()
        
        return self.encoder.state_dict()
        
    def _train_epoch(self, data_generator, epoch: int) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.encoder.train()
        
        # æ ¹æ®è®­ç»ƒè¿›åº¦è·å–æ•°æ®åˆ†å¸ƒ
        stage_config = data_generator.get_progressive_schedule(
            epoch, 
            self.config.epochs
        )
        
        # ç”Ÿæˆè®­ç»ƒæ‰¹æ¬¡
        batch_data = data_generator.generate_batch(
            stage_config, 
            batch_size=self.config.batch_size,
            return_k_values=False  # é¢„è®­ç»ƒä¸éœ€è¦Kå€¼
        )
        
        total_loss = 0
        num_batches = 0
        
        # å°†æ•°æ®è½¬æ¢ä¸ºPyGæ ¼å¼å¹¶è®­ç»ƒ
        for i in range(0, len(batch_data), self.config.batch_size):
            batch_grids = batch_data[i:i+self.config.batch_size]

            # å¤„ç†æ•°æ®æ ¼å¼ï¼šå¦‚æœæ˜¯å…ƒç»„ï¼Œæå–å­—å…¸éƒ¨åˆ†
            processed_grids = []
            for item in batch_grids:
                if isinstance(item, tuple):
                    # (grid_data, k_value) æ ¼å¼
                    processed_grids.append(item[0])
                else:
                    # ç›´æ¥æ˜¯grid_dataå­—å…¸
                    processed_grids.append(item)

            # è½¬æ¢ä¸ºå›¾æ•°æ®
            try:
                graphs = [self._grid_to_graph(grid) for grid in processed_grids]
                batch = Batch.from_data_list(graphs).to(self.device)

                # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´æ€§ï¼ˆä½¿ç”¨float32ï¼‰
                if hasattr(batch, 'x') and batch.x is not None:
                    batch.x = batch.x.float()
                if hasattr(batch, 'edge_attr') and batch.edge_attr is not None:
                    batch.edge_attr = batch.edge_attr.float()

            except Exception as e:
                logger.warning(f"æ‰¹æ¬¡æ•°æ®è½¬æ¢å¤±è´¥: {e}ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                continue
            
            # å‰å‘ä¼ æ’­
            loss = self._compute_loss(batch)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()

            if self.scaler and self.scaler.is_enabled():
                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                self.optimizer.step()
                
            total_loss += loss.item()
            num_batches += 1
            
        avg_loss = total_loss / num_batches
        self.training_history['train_loss'].append(avg_loss)
        
        return avg_loss
        
    def _compute_loss(self, batch: Batch) -> torch.Tensor:
        """è®¡ç®—å¤šä»»åŠ¡æŸå¤±"""
        # è·å–èŠ‚ç‚¹åµŒå…¥
        if self.scaler and self.scaler.is_enabled():
            # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
            with torch.amp.autocast('cuda'):
                # æ„å»ºå¼‚æ„æ•°æ®ï¼ˆä¸å®é™…ç³»ç»Ÿå…¼å®¹ï¼‰
                hetero_batch = HeteroData()
                hetero_batch['bus'].x = batch.x
                hetero_batch['bus', 'connects', 'bus'].edge_index = batch.edge_index
                if hasattr(batch, 'edge_attr') and batch.edge_attr is not None:
                    hetero_batch['bus', 'connects', 'bus'].edge_attr = batch.edge_attr

                # é€šè¿‡ç¼–ç å™¨ - éœ€è¦ä¿æŒæ¢¯åº¦è®¡ç®—
                encoder_output = self.encoder(hetero_batch)
        else:
            # æ ‡å‡†ç²¾åº¦è®­ç»ƒ
            # æ„å»ºå¼‚æ„æ•°æ®ï¼ˆä¸å®é™…ç³»ç»Ÿå…¼å®¹ï¼‰
            hetero_batch = HeteroData()
            hetero_batch['bus'].x = batch.x.float()  # ç¡®ä¿ä½¿ç”¨float32
            hetero_batch['bus', 'connects', 'bus'].edge_index = batch.edge_index
            if hasattr(batch, 'edge_attr') and batch.edge_attr is not None:
                hetero_batch['bus', 'connects', 'bus'].edge_attr = batch.edge_attr.float()

            # é€šè¿‡ç¼–ç å™¨ - éœ€è¦ä¿æŒæ¢¯åº¦è®¡ç®—
            encoder_output = self.encoder(hetero_batch)

        # å¦‚æœç¼–ç å™¨è¿”å›å­—å…¸ï¼Œæå–busåµŒå…¥
        if isinstance(encoder_output, dict):
            node_embeddings = encoder_output['bus']
        elif isinstance(encoder_output, tuple):
            # å¦‚æœè¿”å›å…ƒç»„ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯èŠ‚ç‚¹åµŒå…¥
            node_embeddings_dict = encoder_output[0]
            node_embeddings = node_embeddings_dict['bus']
        else:
            # ç›´æ¥æ˜¯tensor
            node_embeddings = encoder_output
            
        # è®¡ç®—å„é¡¹æŸå¤±
        smooth_loss = self.smoothness_loss(
            node_embeddings, 
            batch.edge_index,
            batch.edge_attr,
            batch.batch
        )
        
        contrast_loss = self.contrastive_loss(
            node_embeddings,
            batch.edge_index,
            self.config.negative_samples,
            batch.batch
        )
        
        physics_loss = self.physics_loss(
            node_embeddings,
            batch.x,
            batch.edge_index,
            batch.edge_attr,
            batch.batch
        )
        
        # åŠ æƒç»„åˆ
        total_loss = (
            self.config.smoothness_weight * smooth_loss +
            self.config.contrastive_weight * contrast_loss +
            self.config.physics_weight * physics_loss
        )
        
        # è®°å½•å„é¡¹æŸå¤±
        if hasattr(self, 'writer'):
            global_step = self.current_epoch * 1000  # è¿‘ä¼¼çš„å…¨å±€æ­¥æ•°
            self.writer.add_scalar('Loss/Smoothness', smooth_loss.item(), global_step)
            self.writer.add_scalar('Loss/Contrastive', contrast_loss.item(), global_step)
            self.writer.add_scalar('Loss/Physics', physics_loss.item(), global_step)
            self.writer.add_scalar('Loss/Total', total_loss.item(), global_step)
            
        return total_loss
        
    def _validate(self, val_data: List[Data]) -> float:
        """éªŒè¯æ¨¡å‹"""
        self.encoder.eval()
        
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            # åˆ†æ‰¹å¤„ç†éªŒè¯æ•°æ®
            for i in range(0, len(val_data), self.config.batch_size):
                batch_graphs = val_data[i:i+self.config.batch_size]
                batch = Batch.from_data_list(batch_graphs).to(self.device)
                
                loss = self._compute_loss(batch)
                total_loss += loss.item()
                num_batches += 1
                
        avg_loss = total_loss / num_batches
        self.training_history['val_loss'].append(avg_loss)
        
        # è®°å½•åˆ°TensorBoard
        self.writer.add_scalar('Loss/Validation', avg_loss, self.current_epoch)
        
        return avg_loss
        
    def _check_early_stopping(self, val_loss: float) -> bool:
        """æ£€æŸ¥æ˜¯å¦è§¦å‘æ—©åœ"""
        if val_loss < self.best_val_loss - self.config.early_stopping_min_delta:
            self.best_val_loss = val_loss
            self.patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            self._save_best_model()
        else:
            self.patience_counter += 1
            
        return self.patience_counter >= self.config.early_stopping_patience
        
    def _save_checkpoint(self, epoch: int):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'encoder_state_dict': self.encoder.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'patience_counter': self.patience_counter,
            'training_history': dict(self.training_history),
            'config': self.config.__dict__
        }
        
        if self.scaler:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
            
        path = Path(self.config.checkpoint_dir) / f'checkpoint_epoch_{epoch}.pt'
        torch.save(checkpoint, path)
        logger.info(f"ä¿å­˜æ£€æŸ¥ç‚¹: {path}")
        
    def _save_best_model(self):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        path = Path(self.config.checkpoint_dir) / 'best_model.pt'
        torch.save({
            'encoder_state_dict': self.encoder.state_dict(),
            'val_loss': self.best_val_loss,
            'epoch': self.current_epoch
        }, path)
        
    def _save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        path = Path(self.config.checkpoint_dir) / 'final_model.pt'
        torch.save({
            'encoder_state_dict': self.encoder.state_dict(),
            'training_history': dict(self.training_history),
            'final_epoch': self.current_epoch
        }, path)
        logger.info(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹: {path}")
        
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.encoder.load_state_dict(checkpoint['encoder_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.current_epoch = checkpoint['epoch'] + 1
        self.best_val_loss = checkpoint['best_val_loss']
        self.patience_counter = checkpoint['patience_counter']
        self.training_history = defaultdict(list, checkpoint['training_history'])
        
        if self.scaler and 'scaler_state_dict' in checkpoint:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
            
        logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: epoch {self.current_epoch}")
        
    def _grid_to_graph(self, grid_data: Dict) -> Data:
        """å°†ç”µç½‘æ•°æ®è½¬æ¢ä¸ºPyGå›¾æ•°æ® - å¢å¼ºé”™è¯¯å¤„ç†"""
        try:
            # éªŒè¯è¾“å…¥æ•°æ®
            if not isinstance(grid_data, dict):
                raise ValueError(f"æœŸæœ›å­—å…¸æ ¼å¼ï¼Œå¾—åˆ° {type(grid_data)}")

            if 'bus' not in grid_data or 'branch' not in grid_data:
                raise ValueError("ç¼ºå°‘å¿…éœ€çš„ 'bus' æˆ– 'branch' é”®")

            bus_data = grid_data['bus']
            branch_data = grid_data['branch']

            # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆå¦‚æœè¿˜ä¸æ˜¯ï¼‰
            if not isinstance(bus_data, np.ndarray):
                bus_data = np.array(bus_data)
            if not isinstance(branch_data, np.ndarray):
                branch_data = np.array(branch_data)

            # èŠ‚ç‚¹ç‰¹å¾
            num_nodes = len(bus_data)
            if num_nodes == 0:
                raise ValueError("ç©ºçš„æ¯çº¿æ•°æ®")

            # æå–èŠ‚ç‚¹ç‰¹å¾ï¼Œä½¿ç”¨ä¸å®é™…ç³»ç»Ÿå…¼å®¹çš„ç‰¹å¾æå–æ–¹å¼
            # åŸºç¡€ç‰¹å¾ï¼šPd, Qd, Gs, Bs, Vm, Va, Vmax, Vmin, degree (9ç»´)
            if bus_data.shape[1] >= 9:
                # æ ‡å‡†MATPOWERæ ¼å¼ï¼šæå–åŸºç¡€ç”µæ°”ç‰¹å¾
                Pd = bus_data[:, 2] / 100.0  # æœ‰åŠŸè´Ÿè·ï¼Œæ ‡å‡†åŒ–
                Qd = bus_data[:, 3] / 100.0  # æ— åŠŸè´Ÿè·ï¼Œæ ‡å‡†åŒ–
                Gs = bus_data[:, 4]          # å¹¶è”ç”µå¯¼
                Bs = bus_data[:, 5]          # å¹¶è”ç”µçº³
                Vm = bus_data[:, 7]          # ç”µå‹å¹…å€¼
                Va = np.deg2rad(bus_data[:, 8])  # ç”µå‹ç›¸è§’

                # ç”µå‹çº¦æŸï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                Vmax = bus_data[:, 11] if bus_data.shape[1] > 11 else np.ones(num_nodes) * 1.1
                Vmin = bus_data[:, 12] if bus_data.shape[1] > 12 else np.ones(num_nodes) * 0.9

                # ç®€åŒ–çš„åº¦æ•°è®¡ç®—ï¼ˆåŸºäºæ”¯è·¯æ•°æ®ï¼‰
                degree = np.zeros(num_nodes)
                for branch in branch_data:
                    if len(branch) > 1:
                        i, j = int(branch[0]) - 1, int(branch[1]) - 1
                        if 0 <= i < num_nodes and 0 <= j < num_nodes:
                            degree[i] += 1
                            degree[j] += 1

                # ç»„åˆåŸºç¡€ç‰¹å¾
                base_features = np.column_stack([Pd, Qd, Gs, Bs, Vm, Va, Vmax, Vmin, degree])

                # æ·»åŠ bus_typeç‹¬çƒ­ç¼–ç ï¼ˆ3ç»´ï¼štype_1, type_2, type_3ï¼‰
                bus_types = bus_data[:, 1].astype(int) if bus_data.shape[1] > 1 else np.ones(num_nodes, dtype=int)
                type_1 = (bus_types == 1).astype(float)
                type_2 = (bus_types == 2).astype(float)
                type_3 = (bus_types == 3).astype(float)
                type_features = np.column_stack([type_1, type_2, type_3])

                # æ·»åŠ å‘ç”µæœºç‰¹å¾ï¼ˆ5ç»´ï¼šPg, Qg, Pg_max, Pg_min, is_genï¼‰
                gen_features = np.zeros((num_nodes, 5))
                # éšæœºè®¾ç½®ä¸€äº›èŠ‚ç‚¹ä¸ºå‘ç”µæœº
                gen_indices = np.random.choice(num_nodes, size=max(1, num_nodes//10), replace=False)
                gen_features[gen_indices, 0] = np.random.uniform(0.1, 2.0, len(gen_indices))  # Pg
                gen_features[gen_indices, 2] = gen_features[gen_indices, 0] * 1.5  # Pg_max
                gen_features[gen_indices, 3] = 0.0  # Pg_min
                gen_features[gen_indices, 4] = 1.0  # is_gen

                # åˆå¹¶æ‰€æœ‰ç‰¹å¾ (9 + 3 + 5 = 17ç»´ï¼Œä¸å®é™…pandapoweræ•°æ®åŒ¹é…)
                all_features = np.hstack([base_features, type_features, gen_features])
                node_features = torch.tensor(all_features, dtype=torch.float32)
            else:
                # å¦‚æœæ•°æ®æ ¼å¼ä¸æ ‡å‡†ï¼Œåˆ›å»ºæœ€å°å…¼å®¹ç‰¹å¾é›†
                # 17ç»´ç‰¹å¾ï¼š9ä¸ªåŸºç¡€ç‰¹å¾ + 3ä¸ªbus_typeç‹¬çƒ­ç¼–ç  + 5ä¸ªå‘ç”µæœºç‰¹å¾
                padded_features = np.zeros((num_nodes, 17))
                # å¡«å……å¯ç”¨çš„ç‰¹å¾
                available_cols = min(bus_data.shape[1] - 1, 9)
                if available_cols > 0:
                    padded_features[:, :available_cols] = bus_data[:, 1:1+available_cols]
                # é»˜è®¤bus_typeä¸º1ï¼ˆPQèŠ‚ç‚¹ï¼‰
                padded_features[:, 9] = 1.0  # type_1
                # å‘ç”µæœºç‰¹å¾ä¿æŒä¸º0ï¼ˆé»˜è®¤æ— å‘ç”µæœºï¼‰
                node_features = torch.tensor(padded_features, dtype=torch.float32)

            # è¾¹ç´¢å¼•å’Œç‰¹å¾
            edge_list = []
            edge_features = []

            for branch in branch_data:
                # æ£€æŸ¥æ”¯è·¯çŠ¶æ€ï¼ˆåœ¨çº¿/ç¦»çº¿ï¼‰
                status_idx = min(10, len(branch) - 1)  # é˜²æ­¢ç´¢å¼•è¶Šç•Œ
                if len(branch) > status_idx and branch[status_idx] == 1:  # åœ¨çº¿æ”¯è·¯
                    i, j = int(branch[0]) - 1, int(branch[1]) - 1  # è½¬æ¢ä¸º0ç´¢å¼•

                    # éªŒè¯èŠ‚ç‚¹ç´¢å¼•
                    if 0 <= i < num_nodes and 0 <= j < num_nodes and i != j:
                        edge_list.append([i, j])
                        edge_list.append([j, i])  # æ— å‘å›¾

                        # è¾¹ç‰¹å¾ï¼ˆä¸å®é™…ç³»ç»Ÿå…¼å®¹çš„9ç»´ç‰¹å¾ï¼‰
                        # ['r', 'x', 'b', '|z|', 'y', 'rateA', 'angle_diff', 'is_transformer', 'status']
                        r = branch[2] if len(branch) > 2 else 0.01
                        x = branch[3] if len(branch) > 3 else 0.01
                        b = branch[4] if len(branch) > 4 else 0.0  # ç”µçº³
                        z_magnitude = np.sqrt(r**2 + x**2)
                        y = 1.0 / z_magnitude if z_magnitude > 1e-6 else 1.0
                        rateA = branch[5] if len(branch) > 5 else 100.0  # è½½æµé™åˆ¶
                        angle_diff = 0.0  # ç›¸è§’å·®é™åˆ¶ï¼ˆç®€åŒ–ï¼‰
                        is_transformer = 0.0  # éå˜å‹å™¨
                        status = 1.0  # åœ¨çº¿çŠ¶æ€

                        edge_feat = [r, x, b, z_magnitude, y, rateA, angle_diff, is_transformer, status]
                        edge_features.extend([edge_feat, edge_feat])  # åŒå‘è¾¹

            # å¦‚æœæ²¡æœ‰è¾¹ï¼Œåˆ›å»ºä¸€ä¸ªæœ€å°è¿é€šå›¾
            if not edge_list:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆè¾¹ï¼Œåˆ›å»ºæœ€å°è¿é€šå›¾")
                for i in range(min(num_nodes - 1, 1)):
                    edge_list.extend([[i, i+1], [i+1, i]])
                    # 9ç»´è¾¹ç‰¹å¾ï¼š[r, x, b, |z|, y, rateA, angle_diff, is_transformer, status]
                    fallback_edge_feat = [0.01, 0.01, 0.0, 0.014, 70.0, 100.0, 0.0, 0.0, 1.0]
                    edge_features.extend([fallback_edge_feat, fallback_edge_feat])

            edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
            edge_attr = torch.tensor(edge_features, dtype=torch.float32)

            return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)

        except Exception as e:
            logger.error(f"ç”µç½‘æ•°æ®è½¬æ¢å¤±è´¥: {e}")
            # è¿”å›ä¸€ä¸ªæœ€å°çš„æœ‰æ•ˆå›¾ï¼Œä½¿ç”¨17ç»´ç‰¹å¾ä»¥ä¿æŒå…¼å®¹æ€§
            fallback_features = torch.zeros(2, 17)  # 2ä¸ªèŠ‚ç‚¹ï¼Œ17ä¸ªç‰¹å¾
            fallback_features[:, 0] = 0.1  # åŸºç¡€è´Ÿè·
            fallback_features[:, 4] = 1.0  # ç”µå‹å¹…å€¼
            fallback_features[:, 9] = 1.0  # bus_type_1
            # 9ç»´è¾¹ç‰¹å¾ï¼š[r, x, b, |z|, y, rateA, angle_diff, is_transformer, status]
            fallback_edge_attr = torch.tensor([
                [0.01, 0.01, 0.0, 0.014, 70.0, 100.0, 0.0, 0.0, 1.0],
                [0.01, 0.01, 0.0, 0.014, 70.0, 100.0, 0.0, 0.0, 1.0]
            ], dtype=torch.float32)
            return Data(
                x=fallback_features,
                edge_index=torch.tensor([[0, 1], [1, 0]], dtype=torch.long).t(),
                edge_attr=fallback_edge_attr
            )
        
    def _generate_val_data(self, data_generator) -> List[Data]:
        """ç”ŸæˆéªŒè¯æ•°æ®"""
        val_config = {'small': 0.5, 'medium': 0.3, 'large': 0.2}
        val_batch = data_generator.generate_batch(
            val_config,
            batch_size=int(self.config.batch_size * self.config.validation_split),
            return_k_values=False
        )

        # å¤„ç†æ•°æ®æ ¼å¼ï¼šå¦‚æœæ˜¯å…ƒç»„ï¼Œæå–å­—å…¸éƒ¨åˆ†
        processed_grids = []
        for item in val_batch:
            if isinstance(item, tuple):
                # (grid_data, k_value) æ ¼å¼
                processed_grids.append(item[0])
            else:
                # ç›´æ¥æ˜¯grid_dataå­—å…¸
                processed_grids.append(item)

        try:
            return [self._grid_to_graph(grid) for grid in processed_grids]
        except Exception as e:
            logger.error(f"éªŒè¯æ•°æ®è½¬æ¢å¤±è´¥: {e}")
            return []
        
    def _log_progress(self, epoch: int, train_loss: float):
        """è®°å½•è®­ç»ƒè¿›åº¦"""
        lr = self.optimizer.param_groups[0]['lr']

        # æ£€æŸ¥æ˜¯å¦ä¸ºç®€æ´æ¨¡å¼ï¼ˆé€šè¿‡æ—¥å¿—çº§åˆ«åˆ¤æ–­ï¼‰
        if logger.getEffectiveLevel() <= logging.INFO:
            # è¯¦ç»†æ¨¡å¼
            logger.info(
                f"Epoch {epoch}/{self.config.epochs} | "
                f"Train Loss: {train_loss:.4f} | "
                f"Best Val Loss: {self.best_val_loss:.4f} | "
                f"LR: {lr:.6f}"
            )
        else:
            # ç®€æ´æ¨¡å¼ - åªåœ¨å…³é”®èŠ‚ç‚¹æ˜¾ç¤º
            if epoch == 0 or epoch % 20 == 0 or epoch == self.config.epochs - 1:
                print(f"ğŸ§  GNNé¢„è®­ç»ƒ: {epoch}/{self.config.epochs} | æŸå¤±: {train_loss:.3f}")


# å¼‚æ„æ•°æ®æ”¯æŒï¼ˆä¸ä¸»ç³»ç»Ÿå…¼å®¹ï¼‰
from torch_geometric.data import HeteroData


def create_hetero_pretrainer(encoder, config: Optional[PretrainConfig] = None):
    """
    åˆ›å»ºæ”¯æŒå¼‚æ„å›¾çš„é¢„è®­ç»ƒå™¨
    
    è¿™æ˜¯ä¸ä¸»ç³»ç»Ÿé›†æˆçš„æ¨èæ–¹å¼
    """
    return GNNPretrainer(encoder, config)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    from code.src.gat import create_production_encoder
    
    # åˆ›å»ºç¼–ç å™¨
    encoder_config = {
        'in_channels': 32,
        'hidden_channels': 64,
        'num_layers': 3,
        'heads': 4,
        'out_channels': 128,
        'dropout': 0.1,
        'edge_dim': 4
    }
    encoder = create_production_encoder(**encoder_config)
    
    # åˆ›å»ºé¢„è®­ç»ƒå™¨
    pretrain_config = PretrainConfig(
        epochs=10,  # æµ‹è¯•ç”¨
        batch_size=4,
        learning_rate=1e-3
    )
    
    pretrainer = GNNPretrainer(encoder, pretrain_config)
    
    logger.info("GNNé¢„è®­ç»ƒå™¨æµ‹è¯•å®Œæˆ")