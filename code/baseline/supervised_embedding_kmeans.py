import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from typing import TYPE_CHECKING, Dict, List, Tuple, Optional, Any, Callable
from .baseline import BasePartitioner, set_baseline_seed

if TYPE_CHECKING:
    from ..src.rl.environment import PowerGridPartitioningEnv

try:
    import torch_geometric
    from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool
    from torch_geometric.data import Data, Batch
    TORCH_GEOMETRIC_AVAILABLE = True
except ImportError:
    TORCH_GEOMETRIC_AVAILABLE = False


class SupervisedGATEncoder(nn.Module):
    """
    æœ‰ç›‘ç£å›¾æ³¨æ„åŠ›ç½‘ç»œç¼–ç å™¨
    
    é€šè¿‡é¢„æµ‹ä¸å®æ—¶è´Ÿè·ç›¸å…³çš„ä»£ç†ä»»åŠ¡ï¼ˆå¦‚æ€»è´Ÿè·ã€èŠ‚ç‚¹ç”µå‹ç­‰ï¼‰æ¥å­¦ä¹ æœ‰æ„ä¹‰çš„èŠ‚ç‚¹è¡¨ç¤ºã€‚
    """
    
    def __init__(self, input_dim: int, hidden_dim: int = 64, embedding_dim: int = 32,
                 num_heads: int = 4, num_layers: int = 2, dropout: float = 0.1,
                 prediction_task: str = 'total_load', output_dim: int = 1):
        super(SupervisedGATEncoder, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.embedding_dim = embedding_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.dropout = dropout
        self.prediction_task = prediction_task
        self.output_dim = output_dim
        
        # æ„å»ºGATå±‚
        self.gat_layers = nn.ModuleList()
        
        # ç¬¬ä¸€å±‚
        self.gat_layers.append(GATConv(input_dim, hidden_dim, heads=num_heads, 
                                      concat=True, dropout=dropout))
        
        # ä¸­é—´å±‚
        for _ in range(num_layers - 2):
            self.gat_layers.append(GATConv(hidden_dim * num_heads, hidden_dim, 
                                          heads=num_heads, concat=True, dropout=dropout))
        
        # æœ€åä¸€å±‚ - äº§ç”ŸåµŒå…¥
        self.gat_layers.append(GATConv(hidden_dim * num_heads, embedding_dim, 
                                      heads=1, concat=False, dropout=dropout))
        
        # é¢„æµ‹å¤´
        self.prediction_head = self._build_prediction_head()
        
        # æ‰¹å½’ä¸€åŒ–
        self.batch_norms = nn.ModuleList([
            nn.BatchNorm1d(hidden_dim * num_heads) for _ in range(num_layers - 1)
        ])
    
    def _build_prediction_head(self) -> nn.Module:
        """æ„å»ºé¢„æµ‹å¤´"""
        if self.prediction_task == 'total_load':
            # é¢„æµ‹æ€»è´Ÿè·
            return nn.Sequential(
                nn.Linear(self.embedding_dim, self.hidden_dim),
                nn.ReLU(),
                nn.Dropout(self.dropout),
                nn.Linear(self.hidden_dim, self.output_dim)
            )
        elif self.prediction_task == 'node_voltage':
            # é¢„æµ‹èŠ‚ç‚¹ç”µå‹
            return nn.Sequential(
                nn.Linear(self.embedding_dim, self.hidden_dim),
                nn.ReLU(),
                nn.Dropout(self.dropout),
                nn.Linear(self.hidden_dim, self.output_dim)
            )
        elif self.prediction_task == 'load_distribution':
            # é¢„æµ‹è´Ÿè·åˆ†å¸ƒ
            return nn.Sequential(
                nn.Linear(self.embedding_dim, self.hidden_dim),
                nn.ReLU(),
                nn.Dropout(self.dropout),
                nn.Linear(self.hidden_dim, self.output_dim)
            )
        else:
            raise ValueError(f"Unknown prediction task: {self.prediction_task}")
    
    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, 
                batch: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        # é€šè¿‡GATå±‚
        h = x
        for i, gat_layer in enumerate(self.gat_layers):
            h = gat_layer(h, edge_index)
            
            # é™¤äº†æœ€åä¸€å±‚ï¼Œéƒ½ä½¿ç”¨æ‰¹å½’ä¸€åŒ–å’Œæ¿€æ´»
            if i < len(self.gat_layers) - 1:
                h = self.batch_norms[i](h)
                h = F.relu(h)
                h = F.dropout(h, p=self.dropout, training=self.training)
        
        # èŠ‚ç‚¹åµŒå…¥
        node_embeddings = h
        
        # å›¾çº§åˆ«çš„é¢„æµ‹
        if batch is not None:
            # æ‰¹å¤„ç†æ¨¡å¼
            graph_embedding = global_mean_pool(node_embeddings, batch)
        else:
            # å•å›¾æ¨¡å¼
            graph_embedding = torch.mean(node_embeddings, dim=0, keepdim=True)
        
        # é¢„æµ‹
        predictions = self.prediction_head(graph_embedding)
        
        return node_embeddings, predictions
    
    def get_node_embeddings(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        """åªè·å–èŠ‚ç‚¹åµŒå…¥ï¼ˆç”¨äºèšç±»ï¼‰"""
        node_embeddings, _ = self.forward(x, edge_index)
        return node_embeddings


class SupervisedEmbeddingKMeansPartitioner(BasePartitioner):
    """
    æœ‰ç›‘ç£åµŒå…¥ + K-Means åˆ†åŒºæ–¹æ³•
    
    ä¸¤æ­¥éª¤æ–¹æ³•ï¼š
    1. é€šè¿‡æœ‰ç›‘ç£å­¦ä¹ è®­ç»ƒGATæ¥é¢„æµ‹ä¸è´Ÿè·ç›¸å…³çš„ä»£ç†ä»»åŠ¡
    2. ä½¿ç”¨è®­ç»ƒå¥½çš„GATä¸ºå½“å‰çŠ¶æ€ç”ŸæˆåŠ¨æ€åµŒå…¥ï¼Œç„¶åè¿›è¡ŒK-Meansèšç±»
    """
    
    def __init__(self, seed: int = 42, hidden_dim: int = 64, embedding_dim: int = 32,
                 num_heads: int = 4, num_layers: int = 2, dropout: float = 0.1,
                 prediction_task: str = 'total_load', learning_rate: float = 0.01,
                 epochs: int = 200, early_stopping: int = 20, batch_size: int = 32,
                 kmeans_init: str = 'k-means++', kmeans_n_init: int = 10):
        """
        åˆå§‹åŒ–æœ‰ç›‘ç£åµŒå…¥+K-Meansåˆ†åŒºå™¨
        
        Args:
            seed: éšæœºç§å­
            hidden_dim: éšè—å±‚ç»´åº¦
            embedding_dim: åµŒå…¥ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            num_layers: GATå±‚æ•°
            dropout: Dropoutæ¦‚ç‡
            prediction_task: ä»£ç†ä»»åŠ¡ç±»å‹
            learning_rate: å­¦ä¹ ç‡
            epochs: è®­ç»ƒè½®æ•°
            early_stopping: æ—©åœè½®æ•°
            batch_size: æ‰¹å¤§å°
            kmeans_init: K-Meansåˆå§‹åŒ–æ–¹æ³•
            kmeans_n_init: K-Meansé‡å¤æ¬¡æ•°
        """
        super().__init__(seed)
        
        if not TORCH_GEOMETRIC_AVAILABLE:
            raise ImportError("torch_geometricåº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install torch-geometric")
        
        self.hidden_dim = hidden_dim
        self.embedding_dim = embedding_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.dropout = dropout
        self.prediction_task = prediction_task
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.early_stopping = early_stopping
        self.batch_size = batch_size
        self.kmeans_init = kmeans_init
        self.kmeans_n_init = kmeans_n_init
        
        # æ¨¡å‹ç›¸å…³
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.scaler = StandardScaler()
        self.is_trained = False
        
        # è®­ç»ƒæ•°æ®
        self.training_data = []
        self.training_labels = []
    
    def partition(self, env: 'PowerGridPartitioningEnv') -> np.ndarray:
        """æ‰§è¡Œæœ‰ç›‘ç£åµŒå…¥+K-Meansåˆ†åŒº"""
        # è®¾ç½®éšæœºç§å­
        self._set_seed()
        
        try:
            # å¦‚æœæ¨¡å‹æœªè®­ç»ƒï¼Œå°è¯•è‡ªè®­ç»ƒ
            if not self.is_trained:
                print("ğŸ”„ æ¨¡å‹æœªè®­ç»ƒï¼Œå°è¯•è‡ªè®­ç»ƒ...")
                success = self._self_train(env)
                if not success:
                    print("âš ï¸ è‡ªè®­ç»ƒå¤±è´¥ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–è¿›è¡Œåˆ†åŒº")
                    return self._fallback_partition(env)
            
            # å‡†å¤‡å½“å‰çŠ¶æ€æ•°æ®
            current_data = self._prepare_graph_data(env)
            
            # ç”ŸæˆåŠ¨æ€åµŒå…¥
            embeddings = self._generate_dynamic_embeddings(current_data)
            
            # K-Meansèšç±»
            labels = self._perform_kmeans(embeddings, env.num_partitions)
            
            return labels + 1  # è½¬æ¢ä¸º1-basedæ ‡ç­¾
            
        except Exception as e:
            print(f"âŒ æœ‰ç›‘ç£åµŒå…¥+K-Meansåˆ†åŒºå¤±è´¥: {str(e)}")
            return self._fallback_partition(env)
    
    def _prepare_graph_data(self, env: 'PowerGridPartitioningEnv') -> Data:
        """å‡†å¤‡å›¾æ•°æ®"""
        # è·å–è¾¹ä¿¡æ¯
        edge_index = env.edge_info['edge_index'].cpu()
        
        # è·å–åŠ¨æ€èŠ‚ç‚¹ç‰¹å¾ï¼ˆåŒ…å«å½“å‰è´Ÿè·çŠ¶æ€ï¼‰
        node_features = self._extract_dynamic_node_features(env)
        
        # åˆ›å»ºPyGæ•°æ®å¯¹è±¡
        data = Data(x=node_features, edge_index=edge_index)
        
        return data
    
    def _extract_dynamic_node_features(self, env: 'PowerGridPartitioningEnv') -> torch.Tensor:
        """æå–åŠ¨æ€èŠ‚ç‚¹ç‰¹å¾"""
        try:
            # åŸºç¡€ç‰¹å¾
            if hasattr(env, 'node_features'):
                base_features = env.node_features.cpu()
            else:
                total_nodes = env.total_nodes
                base_features = torch.randn(total_nodes, 4)
            
            # åŠ¨æ€ç‰¹å¾ï¼ˆè´Ÿè·çŠ¶æ€ï¼‰
            if hasattr(env, 'current_load_state'):
                load_features = env.current_load_state.cpu()
            else:
                # æ¨¡æ‹ŸåŠ¨æ€è´Ÿè·ç‰¹å¾
                total_nodes = env.total_nodes
                load_features = torch.randn(total_nodes, 4)
            
            # æ—¶é—´ç‰¹å¾
            if hasattr(env, 'time_features'):
                time_features = env.time_features.cpu()
            else:
                # æ¨¡æ‹Ÿæ—¶é—´ç‰¹å¾ï¼ˆå°æ—¶ã€æ˜ŸæœŸå‡ ç­‰ï¼‰
                total_nodes = env.total_nodes
                time_features = torch.randn(total_nodes, 2)
            
            # åˆå¹¶ç‰¹å¾
            dynamic_features = torch.cat([base_features, load_features, time_features], dim=1)
            
            return dynamic_features
            
        except Exception as e:
            print(f"âš ï¸ åŠ¨æ€ç‰¹å¾æå–å¤±è´¥: {e}")
            # è¿”å›éšæœºç‰¹å¾
            total_nodes = env.total_nodes
            return torch.randn(total_nodes, 10)
    
    def train_on_scenarios(self, training_scenarios: List[Dict[str, Any]]):
        """åœ¨å¤šä¸ªåœºæ™¯ä¸Šè®­ç»ƒæœ‰ç›‘ç£æ¨¡å‹"""
        print("ğŸ”„ å¼€å§‹æœ‰ç›‘ç£è®­ç»ƒ...")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        self._prepare_training_data(training_scenarios)
        
        if len(self.training_data) == 0:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
            return
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = self._create_data_loader()
        
        # ç¡®å®šè¾“å…¥ç»´åº¦å’Œè¾“å‡ºç»´åº¦
        sample_data = self.training_data[0]
        input_dim = sample_data.x.shape[1]
        output_dim = self._get_output_dimension()
        
        # åˆ›å»ºæ¨¡å‹
        self.model = SupervisedGATEncoder(
            input_dim=input_dim,
            hidden_dim=self.hidden_dim,
            embedding_dim=self.embedding_dim,
            num_heads=self.num_heads,
            num_layers=self.num_layers,
            dropout=self.dropout,
            prediction_task=self.prediction_task,
            output_dim=output_dim
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)
        criterion = self._get_loss_function()
        
        # è®­ç»ƒå¾ªç¯
        best_loss = float('inf')
        patience = 0
        
        for epoch in range(self.epochs):
            epoch_loss = 0.0
            self.model.train()
            
            for batch in train_loader:
                batch = batch.to(self.device)
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                node_embeddings, predictions = self.model(batch.x, batch.edge_index, batch.batch)
                
                # è®¡ç®—æŸå¤±
                loss = criterion(predictions, batch.y)
                
                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
            
            avg_loss = epoch_loss / len(train_loader)
            
            # æ—©åœæ£€æŸ¥
            if avg_loss < best_loss:
                best_loss = avg_loss
                patience = 0
            else:
                patience += 1
                if patience >= self.early_stopping:
                    print(f"   æ—©åœäºç¬¬{epoch+1}è½®")
                    break
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 50 == 0:
                print(f"   Epoch {epoch+1}/{self.epochs}, Loss: {avg_loss:.4f}")
        
        self.is_trained = True
        print("âœ… æœ‰ç›‘ç£è®­ç»ƒå®Œæˆ")
    
    def _prepare_training_data(self, training_scenarios: List[Dict[str, Any]]):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        self.training_data = []
        self.training_labels = []
        
        for scenario in training_scenarios:
            try:
                # æå–åœºæ™¯æ•°æ®
                env = scenario['env']
                target_value = scenario['target']
                
                # åˆ›å»ºå›¾æ•°æ®
                data = self._prepare_graph_data(env)
                
                # æ·»åŠ ç›®æ ‡å€¼
                data.y = torch.tensor([target_value], dtype=torch.float32)
                
                self.training_data.append(data)
                self.training_labels.append(target_value)
                
            except Exception as e:
                print(f"âš ï¸ åœºæ™¯æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
                continue
        
        # æ ‡å‡†åŒ–æ ‡ç­¾
        if len(self.training_labels) > 0:
            labels_array = np.array(self.training_labels).reshape(-1, 1)
            self.scaler.fit(labels_array)
            
            # æ›´æ–°æ•°æ®ä¸­çš„æ ‡ç­¾
            for i, data in enumerate(self.training_data):
                normalized_label = self.scaler.transform([[self.training_labels[i]]])
                data.y = torch.tensor(normalized_label[0], dtype=torch.float32)
    
    def _create_data_loader(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        from torch_geometric.loader import DataLoader
        
        return DataLoader(self.training_data, batch_size=self.batch_size, shuffle=True)
    
    def _get_output_dimension(self) -> int:
        """è·å–è¾“å‡ºç»´åº¦"""
        if self.prediction_task == 'total_load':
            return 1
        elif self.prediction_task == 'node_voltage':
            return 1
        elif self.prediction_task == 'load_distribution':
            return 3  # ä¾‹å¦‚ï¼šå³°å€¼ã€å‡å€¼ã€æ ‡å‡†å·®
        else:
            return 1
    
    def _get_loss_function(self) -> Callable:
        """è·å–æŸå¤±å‡½æ•°"""
        if self.prediction_task in ['total_load', 'node_voltage']:
            return nn.MSELoss()
        elif self.prediction_task == 'load_distribution':
            return nn.MSELoss()
        else:
            return nn.MSELoss()
    
    def _generate_dynamic_embeddings(self, data: Data) -> np.ndarray:
        """ç”ŸæˆåŠ¨æ€èŠ‚ç‚¹åµŒå…¥"""
        data = data.to(self.device)
        
        self.model.eval()
        with torch.no_grad():
            embeddings = self.model.get_node_embeddings(data.x, data.edge_index)
        
        return embeddings.cpu().numpy()
    
    def _perform_kmeans(self, embeddings: np.ndarray, n_clusters: int) -> np.ndarray:
        """æ‰§è¡ŒK-Meansèšç±»"""
        # å¤„ç†å¼‚å¸¸å€¼
        embeddings = np.nan_to_num(embeddings, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # K-Meansèšç±»
        kmeans = KMeans(
            n_clusters=n_clusters,
            init=self.kmeans_init,
            n_init=self.kmeans_n_init,
            random_state=self.seed
        )
        
        labels = kmeans.fit_predict(embeddings)
        
        return labels
    
    def _self_train(self, env: 'PowerGridPartitioningEnv') -> bool:
        """è‡ªè®­ç»ƒæ–¹æ³•ï¼Œä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒæ¨¡å‹"""
        try:
            print("ğŸ”„ å¼€å§‹è‡ªè®­ç»ƒ...")
            
            # ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®
            train_data, train_targets = self._generate_synthetic_training_data(env)
            
            if len(train_data) == 0:
                print("âŒ æ— æ³•ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®")
                return False
            
            # åˆå§‹åŒ–æ¨¡å‹
            input_dim = train_data[0].x.shape[1]
            self.model = SupervisedGATEncoder(
                input_dim=input_dim,
                hidden_dim=self.hidden_dim,
                embedding_dim=self.embedding_dim,
                num_heads=self.num_heads,
                num_layers=self.num_layers,
                dropout=self.dropout,
                prediction_task=self.prediction_task,
                output_dim=1
            ).to(self.device)
            
            # è®­ç»ƒæ¨¡å‹
            success = self._train_model(train_data, train_targets)
            
            if success:
                self.is_trained = True
                print("âœ… è‡ªè®­ç»ƒå®Œæˆ")
                return True
            else:
                print("âŒ è‡ªè®­ç»ƒå¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ è‡ªè®­ç»ƒè¿‡ç¨‹å¤±è´¥: {str(e)}")
            return False
    
    def _generate_synthetic_training_data(self, env: 'PowerGridPartitioningEnv') -> Tuple[List[Data], List[float]]:
        """ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®"""
        train_data = []
        train_targets = []
        
        try:
            # ç”Ÿæˆå¤šä¸ªä¸åŒçš„è´Ÿè·çŠ¶æ€
            num_samples = 100
            
            for i in range(num_samples):
                # éšæœºæ‰°åŠ¨è´Ÿè·
                data = self._prepare_graph_data(env)
                
                # æ·»åŠ éšæœºæ‰°åŠ¨åˆ°èŠ‚ç‚¹ç‰¹å¾
                noise = torch.randn_like(data.x) * 0.1
                data.x = data.x + noise
                
                # æ ¹æ®é¢„æµ‹ä»»åŠ¡ç”Ÿæˆç›®æ ‡
                if self.prediction_task == 'total_load':
                    # æ€»è´Ÿè· = èŠ‚ç‚¹ç‰¹å¾çš„å’Œ
                    target = float(data.x.sum())
                elif self.prediction_task == 'node_voltage':
                    # å¹³å‡ç”µå‹ï¼ˆæ¨¡æ‹Ÿï¼‰
                    target = float(data.x.mean())
                else:
                    # é»˜è®¤ç›®æ ‡
                    target = float(data.x.norm())
                
                train_data.append(data)
                train_targets.append(target)
            
            print(f"ğŸ“Š ç”Ÿæˆäº† {len(train_data)} ä¸ªè®­ç»ƒæ ·æœ¬")
            return train_data, train_targets
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆåˆæˆæ•°æ®å¤±è´¥: {str(e)}")
            return [], []
    
    def _train_model(self, train_data: List[Data], train_targets: List[float]) -> bool:
        """è®­ç»ƒæ¨¡å‹"""
        try:
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            train_targets = torch.tensor(train_targets, dtype=torch.float32).to(self.device)
            
            # æ ‡å‡†åŒ–ç›®æ ‡
            train_targets = (train_targets - train_targets.mean()) / (train_targets.std() + 1e-8)
            
            # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)
            criterion = nn.MSELoss()
            
            # è®­ç»ƒå¾ªç¯
            self.model.train()
            best_loss = float('inf')
            patience = 0
            
            # å‡å°‘è®­ç»ƒè½®æ•°ä»¥åŠ å¿«é€Ÿåº¦
            max_epochs = min(self.epochs, 50)
            
            for epoch in range(max_epochs):
                total_loss = 0.0
                
                # éšæœºæ‰“ä¹±æ•°æ®
                indices = torch.randperm(len(train_data))
                
                for i in indices:
                    data = train_data[i].to(self.device)
                    target = train_targets[i:i+1]
                    
                    optimizer.zero_grad()
                    
                    # å‰å‘ä¼ æ’­
                    _, pred = self.model(data.x, data.edge_index)
                    
                    # è®¡ç®—æŸå¤±
                    loss = criterion(pred, target)
                    
                    # åå‘ä¼ æ’­
                    loss.backward()
                    optimizer.step()
                    
                    total_loss += loss.item()
                
                avg_loss = total_loss / len(train_data)
                
                # æ—©åœæ£€æŸ¥
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    patience = 0
                else:
                    patience += 1
                    if patience >= self.early_stopping:
                        print(f"   æ—©åœäºç¬¬{epoch+1}è½®")
                        break
                
                if epoch % 10 == 0:
                    print(f"   è½®æ¬¡ {epoch+1}/{max_epochs}, æŸå¤±: {avg_loss:.4f}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}")
            return False
    
    def _fallback_partition(self, env: 'PowerGridPartitioningEnv') -> np.ndarray:
        """é™çº§åˆ†åŒºæ–¹æ³•"""
        print("âš ï¸ ä½¿ç”¨é™çº§åˆ†åŒºæ–¹æ³•...")
        
        # ç®€å•çš„è½®è¯¢åˆ†é…
        labels = np.zeros(env.total_nodes, dtype=int)
        for i in range(env.total_nodes):
            labels[i] = (i % env.num_partitions) + 1
        
        return labels
    
    def predict_target(self, env: 'PowerGridPartitioningEnv') -> float:
        """é¢„æµ‹ç›®æ ‡å€¼ï¼ˆç”¨äºéªŒè¯æ¨¡å‹æ€§èƒ½ï¼‰"""
        try:
            if not self.is_trained:
                return 0.0
            
            data = self._prepare_graph_data(env)
            data = data.to(self.device)
            
            self.model.eval()
            with torch.no_grad():
                _, predictions = self.model(data.x, data.edge_index)
            
            # åæ ‡å‡†åŒ–
            pred_value = predictions.cpu().numpy()[0]
            if hasattr(self.scaler, 'inverse_transform'):
                pred_value = self.scaler.inverse_transform([pred_value])[0]
            
            return float(pred_value)
            
        except Exception as e:
            print(f"âš ï¸ é¢„æµ‹å¤±è´¥: {e}")
            return 0.0
    
    def evaluate_prediction_accuracy(self, test_scenarios: List[Dict[str, Any]]) -> Dict[str, float]:
        """è¯„ä¼°é¢„æµ‹å‡†ç¡®æ€§"""
        if not self.is_trained:
            return {'error': 'Model not trained'}
        
        try:
            predictions = []
            targets = []
            
            for scenario in test_scenarios:
                env = scenario['env']
                target = scenario['target']
                
                pred = self.predict_target(env)
                predictions.append(pred)
                targets.append(target)
            
            # è®¡ç®—æŒ‡æ ‡
            predictions = np.array(predictions)
            targets = np.array(targets)
            
            mse = np.mean((predictions - targets) ** 2)
            mae = np.mean(np.abs(predictions - targets))
            
            # ç›¸å…³ç³»æ•°
            correlation = np.corrcoef(predictions, targets)[0, 1]
            
            return {
                'mse': float(mse),
                'mae': float(mae),
                'correlation': float(correlation),
                'num_samples': len(predictions)
            }
            
        except Exception as e:
            return {'error': str(e)}
    
    def save_model(self, filepath: str):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if self.model is not None:
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'model_config': {
                    'input_dim': self.model.input_dim,
                    'hidden_dim': self.hidden_dim,
                    'embedding_dim': self.embedding_dim,
                    'num_heads': self.num_heads,
                    'num_layers': self.num_layers,
                    'dropout': self.dropout,
                    'prediction_task': self.prediction_task,
                    'output_dim': self.model.output_dim
                },
                'scaler': self.scaler,
                'is_trained': self.is_trained
            }, filepath)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
        else:
            print("âŒ æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥ä¿å­˜")
    
    def load_model(self, filepath: str):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            checkpoint = torch.load(filepath, map_location=self.device)
            
            # é‡å»ºæ¨¡å‹
            config = checkpoint['model_config']
            self.model = SupervisedGATEncoder(
                input_dim=config['input_dim'],
                hidden_dim=config['hidden_dim'],
                embedding_dim=config['embedding_dim'],
                num_heads=config['num_heads'],
                num_layers=config['num_layers'],
                dropout=config['dropout'],
                prediction_task=config['prediction_task'],
                output_dim=config['output_dim']
            ).to(self.device)
            
            # åŠ è½½æƒé‡
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.scaler = checkpoint['scaler']
            self.is_trained = checkpoint['is_trained']
            
            print(f"âœ… æ¨¡å‹å·²ä» {filepath} åŠ è½½")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise