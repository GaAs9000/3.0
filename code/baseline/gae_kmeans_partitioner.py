import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.cluster import KMeans
from typing import TYPE_CHECKING, Dict, List, Tuple, Optional, Any
from .baseline import BasePartitioner, set_baseline_seed

if TYPE_CHECKING:
    from ..src.rl.environment import PowerGridPartitioningEnv

try:
    import torch_geometric
    from torch_geometric.nn import GCNConv, GATConv, global_mean_pool
    from torch_geometric.data import Data, Batch
    from torch_geometric.utils import negative_sampling
    TORCH_GEOMETRIC_AVAILABLE = True
except ImportError:
    TORCH_GEOMETRIC_AVAILABLE = False


class GraphAutoEncoder(nn.Module):
    """
    å›¾è‡ªç¼–ç å™¨æ¨¡å‹
    
    ä½¿ç”¨å›¾å·ç§¯ç½‘ç»œå­¦ä¹ èŠ‚ç‚¹çš„ä½ç»´è¡¨ç¤ºï¼Œç„¶åé‡æ„å›¾çš„é‚»æ¥å…³ç³»ã€‚
    """
    
    def __init__(self, input_dim: int, hidden_dim: int = 64, embedding_dim: int = 32,
                 encoder_type: str = 'gcn', num_layers: int = 2, dropout: float = 0.1):
        super(GraphAutoEncoder, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.embedding_dim = embedding_dim
        self.encoder_type = encoder_type
        self.num_layers = num_layers
        self.dropout = dropout
        
        # æ„å»ºç¼–ç å™¨
        if encoder_type == 'gcn':
            self.encoder = self._build_gcn_encoder()
        elif encoder_type == 'gat':
            self.encoder = self._build_gat_encoder()
        else:
            raise ValueError(f"Unknown encoder type: {encoder_type}")
        
        # è§£ç å™¨ï¼ˆå†…ç§¯è§£ç å™¨ï¼‰
        self.decoder = InnerProductDecoder()
    
    def _build_gcn_encoder(self) -> nn.ModuleList:
        """æ„å»ºGCNç¼–ç å™¨"""
        layers = nn.ModuleList()
        
        # è¾“å…¥å±‚
        layers.append(GCNConv(self.input_dim, self.hidden_dim))
        
        # éšè—å±‚
        for _ in range(self.num_layers - 2):
            layers.append(GCNConv(self.hidden_dim, self.hidden_dim))
        
        # è¾“å‡ºå±‚
        layers.append(GCNConv(self.hidden_dim, self.embedding_dim))
        
        return layers
    
    def _build_gat_encoder(self) -> nn.ModuleList:
        """æ„å»ºGATç¼–ç å™¨"""
        layers = nn.ModuleList()
        
        # è¾“å…¥å±‚
        layers.append(GATConv(self.input_dim, self.hidden_dim, heads=4, concat=False))
        
        # éšè—å±‚
        for _ in range(self.num_layers - 2):
            layers.append(GATConv(self.hidden_dim, self.hidden_dim, heads=4, concat=False))
        
        # è¾“å‡ºå±‚
        layers.append(GATConv(self.hidden_dim, self.embedding_dim, heads=1, concat=False))
        
        return layers
    
    def encode(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        """ç¼–ç èŠ‚ç‚¹ç‰¹å¾"""
        h = x
        
        for i, layer in enumerate(self.encoder):
            h = layer(h, edge_index)
            
            # é™¤äº†æœ€åä¸€å±‚ï¼Œéƒ½ä½¿ç”¨ReLUå’ŒDropout
            if i < len(self.encoder) - 1:
                h = F.relu(h)
                h = F.dropout(h, p=self.dropout, training=self.training)
        
        return h
    
    def decode(self, z: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        """è§£ç è¾¹æ¦‚ç‡"""
        return self.decoder(z, edge_index)
    
    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        # ç¼–ç 
        z = self.encode(x, edge_index)
        
        # è§£ç 
        edge_prob = self.decode(z, edge_index)
        
        return z, edge_prob


class InnerProductDecoder(nn.Module):
    """å†…ç§¯è§£ç å™¨"""
    
    def __init__(self):
        super(InnerProductDecoder, self).__init__()
    
    def forward(self, z: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—è¾¹çš„é‡æ„æ¦‚ç‡"""
        # è·å–è¾¹çš„ç«¯ç‚¹
        row, col = edge_index
        
        # è®¡ç®—å†…ç§¯
        inner_product = (z[row] * z[col]).sum(dim=1)
        
        # åº”ç”¨sigmoidæ¿€æ´»
        return torch.sigmoid(inner_product)


class GAEKMeansPartitioner(BasePartitioner):
    """
    å›¾è‡ªç¼–ç å™¨ + K-Means åˆ†åŒºæ–¹æ³•
    
    ä¸¤æ­¥éª¤æ–¹æ³•ï¼š
    1. ä½¿ç”¨å›¾è‡ªç¼–ç å™¨å­¦ä¹ èŠ‚ç‚¹çš„ä½ç»´è¡¨ç¤º
    2. å¯¹å­¦åˆ°çš„åµŒå…¥å‘é‡è¿›è¡ŒK-Meansèšç±»
    """
    
    def __init__(self, seed: int = 42, hidden_dim: int = 64, embedding_dim: int = 32,
                 encoder_type: str = 'gcn', num_layers: int = 2, dropout: float = 0.1,
                 learning_rate: float = 0.01, epochs: int = 200, early_stopping: int = 20,
                 kmeans_init: str = 'k-means++', kmeans_n_init: int = 10):
        """
        åˆå§‹åŒ–GAE+K-Meansåˆ†åŒºå™¨
        
        Args:
            seed: éšæœºç§å­
            hidden_dim: éšè—å±‚ç»´åº¦
            embedding_dim: åµŒå…¥ç»´åº¦
            encoder_type: ç¼–ç å™¨ç±»å‹ ('gcn', 'gat')
            num_layers: ç¼–ç å™¨å±‚æ•°
            dropout: Dropoutæ¦‚ç‡
            learning_rate: å­¦ä¹ ç‡
            epochs: è®­ç»ƒè½®æ•°
            early_stopping: æ—©åœè½®æ•°
            kmeans_init: K-Meansåˆå§‹åŒ–æ–¹æ³•
            kmeans_n_init: K-Meansé‡å¤æ¬¡æ•°
        """
        super().__init__(seed)
        
        if not TORCH_GEOMETRIC_AVAILABLE:
            raise ImportError("torch_geometricåº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install torch-geometric")
        
        self.hidden_dim = hidden_dim
        self.embedding_dim = embedding_dim
        self.encoder_type = encoder_type
        self.num_layers = num_layers
        self.dropout = dropout
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.early_stopping = early_stopping
        self.kmeans_init = kmeans_init
        self.kmeans_n_init = kmeans_n_init
        
        # æ¨¡å‹ç›¸å…³
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.is_trained = False
    
    def partition(self, env: 'PowerGridPartitioningEnv') -> np.ndarray:
        """æ‰§è¡ŒGAE+K-Meansåˆ†åŒº"""
        # è®¾ç½®éšæœºç§å­
        self._set_seed()
        
        try:
            # å‡†å¤‡æ•°æ®
            graph_data = self._prepare_graph_data(env)
            
            # è®­ç»ƒGAEï¼ˆå¦‚æœå°šæœªè®­ç»ƒï¼‰
            if not self.is_trained:
                self._train_gae(graph_data)
            
            # ç”ŸæˆèŠ‚ç‚¹åµŒå…¥
            embeddings = self._generate_embeddings(graph_data)
            
            # K-Meansèšç±»
            labels = self._perform_kmeans(embeddings, env.num_partitions)
            
            return labels + 1  # è½¬æ¢ä¸º1-basedæ ‡ç­¾
            
        except Exception as e:
            print(f"âŒ GAE+K-Meansåˆ†åŒºå¤±è´¥: {str(e)}")
            return self._fallback_partition(env)
    
    def _prepare_graph_data(self, env: 'PowerGridPartitioningEnv') -> Data:
        """å‡†å¤‡å›¾æ•°æ®"""
        # è·å–è¾¹ä¿¡æ¯
        edge_index = env.edge_info['edge_index'].cpu()
        
        # è·å–èŠ‚ç‚¹ç‰¹å¾
        if hasattr(env, 'node_features'):
            node_features = env.node_features.cpu()
        else:
            # å¦‚æœæ²¡æœ‰èŠ‚ç‚¹ç‰¹å¾ï¼Œä½¿ç”¨ç®€å•çš„ç‰¹å¾
            total_nodes = env.total_nodes
            node_features = torch.randn(total_nodes, 8)  # 8ç»´éšæœºç‰¹å¾
        
        # åˆ›å»ºPyGæ•°æ®å¯¹è±¡
        data = Data(x=node_features, edge_index=edge_index)
        
        return data
    
    def _train_gae(self, data: Data):
        """è®­ç»ƒå›¾è‡ªç¼–ç å™¨"""
        print("ğŸ”„ å¼€å§‹è®­ç»ƒå›¾è‡ªç¼–ç å™¨...")
        
        # è®¾ç½®è®¾å¤‡
        data = data.to(self.device)
        
        # åˆ›å»ºæ¨¡å‹
        self.model = GraphAutoEncoder(
            input_dim=data.x.shape[1],
            hidden_dim=self.hidden_dim,
            embedding_dim=self.embedding_dim,
            encoder_type=self.encoder_type,
            num_layers=self.num_layers,
            dropout=self.dropout
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)
        
        # è®­ç»ƒå¾ªç¯
        best_loss = float('inf')
        patience = 0
        
        self.model.train()
        for epoch in range(self.epochs):
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            z, edge_prob = self.model(data.x, data.edge_index)
            
            # è®¡ç®—æŸå¤±
            loss = self._compute_gae_loss(edge_prob, data.edge_index, data.x.shape[0], data)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            # æ—©åœæ£€æŸ¥
            if loss.item() < best_loss:
                best_loss = loss.item()
                patience = 0
            else:
                patience += 1
                if patience >= self.early_stopping:
                    print(f"   æ—©åœäºç¬¬{epoch+1}è½®")
                    break
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 50 == 0:
                print(f"   Epoch {epoch+1}/{self.epochs}, Loss: {loss.item():.4f}")
        
        self.is_trained = True
        print("âœ… å›¾è‡ªç¼–ç å™¨è®­ç»ƒå®Œæˆ")
    
    def _compute_gae_loss(self, edge_prob: torch.Tensor, edge_index: torch.Tensor, 
                         num_nodes: int, data: Any) -> torch.Tensor:
        """è®¡ç®—GAEæŸå¤±"""
        # æ­£æ ·æœ¬æŸå¤±
        pos_loss = -torch.log(edge_prob + 1e-15).mean()
        
        # è´Ÿæ ·æœ¬
        neg_edge_index = negative_sampling(edge_index, num_nodes=num_nodes, 
                                          num_neg_samples=edge_index.shape[1])
        
        # è®¡ç®—è´Ÿæ ·æœ¬æ¦‚ç‡
        neg_edge_prob = self.model.decode(self.model.encode(data.x, edge_index), 
                                         neg_edge_index)
        
        # è´Ÿæ ·æœ¬æŸå¤±
        neg_loss = -torch.log(1 - neg_edge_prob + 1e-15).mean()
        
        return pos_loss + neg_loss
    
    def _generate_embeddings(self, data: Data) -> np.ndarray:
        """ç”ŸæˆèŠ‚ç‚¹åµŒå…¥"""
        data = data.to(self.device)
        
        self.model.eval()
        with torch.no_grad():
            embeddings = self.model.encode(data.x, data.edge_index)
        
        return embeddings.cpu().numpy()
    
    def _perform_kmeans(self, embeddings: np.ndarray, n_clusters: int) -> np.ndarray:
        """æ‰§è¡ŒK-Meansèšç±»"""
        # å¤„ç†å¼‚å¸¸å€¼
        embeddings = np.nan_to_num(embeddings, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # K-Meansèšç±»
        kmeans = KMeans(
            n_clusters=n_clusters,
            init=self.kmeans_init,
            n_init=self.kmeans_n_init,
            random_state=self.seed
        )
        
        labels = kmeans.fit_predict(embeddings)
        
        return labels
    
    def _fallback_partition(self, env: 'PowerGridPartitioningEnv') -> np.ndarray:
        """é™çº§åˆ†åŒºæ–¹æ³•"""
        print("âš ï¸ ä½¿ç”¨é™çº§åˆ†åŒºæ–¹æ³•...")
        
        # ç®€å•çš„è½®è¯¢åˆ†é…
        labels = np.zeros(env.total_nodes, dtype=int)
        for i in range(env.total_nodes):
            labels[i] = (i % env.num_partitions) + 1
        
        return labels
    
    def pretrain_on_multiple_networks(self, envs: List['PowerGridPartitioningEnv']):
        """åœ¨å¤šä¸ªç½‘ç»œä¸Šé¢„è®­ç»ƒGAE"""
        print("ğŸ”„ å¼€å§‹åœ¨å¤šä¸ªç½‘ç»œä¸Šé¢„è®­ç»ƒGAE...")
        
        # å‡†å¤‡æ‰¹é‡æ•°æ®
        batch_data = []
        for env in envs:
            data = self._prepare_graph_data(env)
            batch_data.append(data)
        
        # åˆ›å»ºæ‰¹é‡æ•°æ®
        batch = Batch.from_data_list(batch_data)
        
        # è®­ç»ƒ
        self._train_gae(batch)
        
        print("âœ… å¤šç½‘ç»œé¢„è®­ç»ƒå®Œæˆ")
    
    def get_embedding_quality(self, env: 'PowerGridPartitioningEnv') -> Dict[str, float]:
        """è¯„ä¼°åµŒå…¥è´¨é‡"""
        try:
            if not self.is_trained:
                return {'error': 'Model not trained'}
            
            # ç”ŸæˆåµŒå…¥
            data = self._prepare_graph_data(env)
            embeddings = self._generate_embeddings(data)
            
            # è®¡ç®—åµŒå…¥è´¨é‡æŒ‡æ ‡
            # 1. åµŒå…¥æ–¹å·®
            embedding_variance = np.var(embeddings, axis=0).mean()
            
            # 2. åµŒå…¥èŒƒæ•°
            embedding_norm = np.linalg.norm(embeddings, axis=1).mean()
            
            # 3. åµŒå…¥åˆ†ç¦»åº¦
            from sklearn.metrics.pairwise import pairwise_distances
            distances = pairwise_distances(embeddings)
            avg_distance = np.mean(distances[np.triu_indices_from(distances, k=1)])
            
            quality = {
                'embedding_variance': float(embedding_variance),
                'embedding_norm': float(embedding_norm),
                'avg_pairwise_distance': float(avg_distance),
                'embedding_dim': self.embedding_dim
            }
            
            return quality
            
        except Exception as e:
            return {'error': str(e)}
    
    def save_model(self, filepath: str):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if self.model is not None:
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'model_config': {
                    'input_dim': self.model.input_dim,
                    'hidden_dim': self.hidden_dim,
                    'embedding_dim': self.embedding_dim,
                    'encoder_type': self.encoder_type,
                    'num_layers': self.num_layers,
                    'dropout': self.dropout
                },
                'is_trained': self.is_trained
            }, filepath)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
        else:
            print("âŒ æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥ä¿å­˜")
    
    def load_model(self, filepath: str):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            checkpoint = torch.load(filepath, map_location=self.device)
            
            # é‡å»ºæ¨¡å‹
            config = checkpoint['model_config']
            self.model = GraphAutoEncoder(
                input_dim=config['input_dim'],
                hidden_dim=config['hidden_dim'],
                embedding_dim=config['embedding_dim'],
                encoder_type=config['encoder_type'],
                num_layers=config['num_layers'],
                dropout=config['dropout']
            ).to(self.device)
            
            # åŠ è½½æƒé‡
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.is_trained = checkpoint['is_trained']
            
            print(f"âœ… æ¨¡å‹å·²ä» {filepath} åŠ è½½")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise