#!/usr/bin/env python3
"""
ç®€å•ç›¸å¯¹æ”¹è¿›å¥–åŠ±æµ‹è¯•è„šæœ¬

éªŒè¯æ–°çš„ç›¸å¯¹æ”¹è¿›å¥–åŠ±è®¡ç®—æ˜¯å¦æ­£ç¡®è§£å†³è·¨åœºæ™¯è®­ç»ƒåå‘é—®é¢˜
"""

import sys
import os
# æ·»åŠ æ­£ç¡®çš„è·¯å¾„ï¼šä»code/testså›åˆ°2.0æ ¹ç›®å½•ï¼Œç„¶åè¿›å…¥code/src
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

import numpy as np
import torch
from code.src.rl.reward import RewardFunction

def test_relative_reward_calculation():
    """æµ‹è¯•ç›¸å¯¹å¥–åŠ±è®¡ç®—çš„æ­£ç¡®æ€§"""
    print("ğŸ§ª æµ‹è¯•ç›¸å¯¹æ”¹è¿›å¥–åŠ±è®¡ç®—")
    print("=" * 50)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„RewardFunctionå®ä¾‹ç”¨äºæµ‹è¯•
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªæµ‹è¯•_compute_simple_relative_rewardæ–¹æ³•
    class TestRewardFunction:
        def _compute_simple_relative_reward(self, prev_quality: float, curr_quality: float) -> float:
            """ç®€å•ç›¸å¯¹æ”¹è¿›å¥–åŠ± - è§£å†³è·¨åœºæ™¯è®­ç»ƒåå‘é—®é¢˜"""
            try:
                if prev_quality > 0.01:  # é¿å…é™¤é›¶ï¼Œå¤„ç†è¾¹ç•Œæƒ…å†µ
                    relative_improvement = (curr_quality - prev_quality) / prev_quality
                else:
                    # ä»é›¶å¼€å§‹çš„æƒ…å†µï¼Œç›´æ¥ç”¨ç»å¯¹æ”¹è¿›
                    relative_improvement = curr_quality - prev_quality
                
                # è½»å¾®è£å‰ªé¿å…æç«¯å€¼ï¼Œä¿æŒè®­ç»ƒç¨³å®šæ€§
                return np.clip(relative_improvement, -1.0, 1.0)
                
            except Exception as e:
                print(f"è­¦å‘Šï¼šç›¸å¯¹å¥–åŠ±è®¡ç®—å‡ºç°å¼‚å¸¸: {e}")
                return 0.0
    
    reward_func = TestRewardFunction()
    
    # æµ‹è¯•æ¡ˆä¾‹1: è·¨åœºæ™¯å…¬å¹³æ€§
    print("ğŸ“Š æµ‹è¯•æ¡ˆä¾‹1: è·¨åœºæ™¯å…¬å¹³æ€§")
    print("-" * 30)
    
    # æ•…éšœåœºæ™¯ï¼šä»ä½è´¨é‡åŸºçº¿æ”¹è¿›
    fault_prev = 0.40
    fault_curr = 0.42
    fault_reward = reward_func._compute_simple_relative_reward(fault_prev, fault_curr)
    fault_improvement = (fault_curr - fault_prev) / fault_prev * 100
    
    # æ­£å¸¸åœºæ™¯ï¼šä»é«˜è´¨é‡åŸºçº¿æ”¹è¿›
    normal_prev = 0.68
    normal_curr = 0.714  # ç›¸åŒçš„5%ç›¸å¯¹æ”¹è¿›
    normal_reward = reward_func._compute_simple_relative_reward(normal_prev, normal_curr)
    normal_improvement = (normal_curr - normal_prev) / normal_prev * 100
    
    print(f"æ•…éšœåœºæ™¯: {fault_prev:.3f} â†’ {fault_curr:.3f} ({fault_improvement:.1f}%æ”¹è¿›) â†’ å¥–åŠ±: {fault_reward:.4f}")
    print(f"æ­£å¸¸åœºæ™¯: {normal_prev:.3f} â†’ {normal_curr:.3f} ({normal_improvement:.1f}%æ”¹è¿›) â†’ å¥–åŠ±: {normal_reward:.4f}")
    print(f"å¥–åŠ±å·®å¼‚: {abs(fault_reward - normal_reward):.6f} (åº”è¯¥æ¥è¿‘0)")
    
    # æµ‹è¯•æ¡ˆä¾‹2: è¾¹ç•Œæƒ…å†µ
    print("\nğŸ“Š æµ‹è¯•æ¡ˆä¾‹2: è¾¹ç•Œæƒ…å†µ")
    print("-" * 30)
    
    # ä»é›¶å¼€å§‹
    zero_reward = reward_func._compute_simple_relative_reward(0.0, 0.5)
    print(f"ä»é›¶å¼€å§‹: 0.0 â†’ 0.5 â†’ å¥–åŠ±: {zero_reward:.4f}")
    
    # æå°åŸºçº¿
    small_reward = reward_func._compute_simple_relative_reward(0.005, 0.01)
    print(f"æå°åŸºçº¿: 0.005 â†’ 0.01 â†’ å¥–åŠ±: {small_reward:.4f}")
    
    # è´Ÿå‘æ”¹è¿›
    negative_reward = reward_func._compute_simple_relative_reward(0.6, 0.5)
    negative_improvement = (0.5 - 0.6) / 0.6 * 100
    print(f"è´Ÿå‘æ”¹è¿›: 0.6 â†’ 0.5 ({negative_improvement:.1f}%é€€åŒ–) â†’ å¥–åŠ±: {negative_reward:.4f}")
    
    # æµ‹è¯•æ¡ˆä¾‹3: æç«¯å€¼å¤„ç†
    print("\nğŸ“Š æµ‹è¯•æ¡ˆä¾‹3: æç«¯å€¼å¤„ç†")
    print("-" * 30)
    
    # æå¤§æ”¹è¿›ï¼ˆåº”è¯¥è¢«è£å‰ªï¼‰
    extreme_reward = reward_func._compute_simple_relative_reward(0.1, 0.5)
    extreme_improvement = (0.5 - 0.1) / 0.1 * 100
    print(f"æå¤§æ”¹è¿›: 0.1 â†’ 0.5 ({extreme_improvement:.0f}%æ”¹è¿›) â†’ å¥–åŠ±: {extreme_reward:.4f} (è£å‰ªåˆ°1.0)")
    
    # æå¤§é€€åŒ–ï¼ˆåº”è¯¥è¢«è£å‰ªï¼‰
    extreme_negative = reward_func._compute_simple_relative_reward(0.8, 0.1)
    extreme_negative_improvement = (0.1 - 0.8) / 0.8 * 100
    print(f"æå¤§é€€åŒ–: 0.8 â†’ 0.1 ({extreme_negative_improvement:.0f}%é€€åŒ–) â†’ å¥–åŠ±: {extreme_negative:.4f} (è£å‰ªåˆ°-1.0)")

def test_scenario_comparison():
    """å¯¹æ¯”ä¸åŒåœºæ™¯ä¸‹çš„å¥–åŠ±å…¬å¹³æ€§"""
    print("\nğŸ¯ åœºæ™¯å¯¹æ¯”åˆ†æ")
    print("=" * 50)
    
    class TestRewardFunction:
        def _compute_simple_relative_reward(self, prev_quality: float, curr_quality: float) -> float:
            try:
                if prev_quality > 0.01:
                    relative_improvement = (curr_quality - prev_quality) / prev_quality
                else:
                    relative_improvement = curr_quality - prev_quality
                return np.clip(relative_improvement, -1.0, 1.0)
            except:
                return 0.0
    
    reward_func = TestRewardFunction()
    
    scenarios = [
        ("æ­£å¸¸è¿è¡Œ", 0.75, 0.7875),  # 5%æ”¹è¿›
        ("è½»å¾®æ•…éšœ", 0.60, 0.63),    # 5%æ”¹è¿›  
        ("ä¸¥é‡æ•…éšœ", 0.35, 0.3675),  # 5%æ”¹è¿›
        ("é«˜è´Ÿè·", 0.50, 0.525),     # 5%æ”¹è¿›
        ("å‘ç”µæ³¢åŠ¨", 0.45, 0.4725),  # 5%æ”¹è¿›
    ]
    
    print("æ‰€æœ‰åœºæ™¯éƒ½å®ç°5%çš„ç›¸å¯¹æ”¹è¿›:")
    print("-" * 40)
    
    rewards = []
    for name, prev, curr in scenarios:
        reward = reward_func._compute_simple_relative_reward(prev, curr)
        improvement = (curr - prev) / prev * 100
        rewards.append(reward)
        print(f"{name:8s}: {prev:.4f} â†’ {curr:.4f} ({improvement:.1f}%) â†’ å¥–åŠ±: {reward:.4f}")
    
    # è®¡ç®—å¥–åŠ±çš„æ ‡å‡†å·®ï¼Œåº”è¯¥å¾ˆå°
    reward_std = np.std(rewards)
    print(f"\nå¥–åŠ±æ ‡å‡†å·®: {reward_std:.6f} (è¶Šå°è¶Šå¥½ï¼Œè¡¨ç¤ºå…¬å¹³æ€§)")
    
    if reward_std < 0.001:
        print("âœ… æµ‹è¯•é€šè¿‡ï¼šè·¨åœºæ™¯å¥–åŠ±å…¬å¹³æ€§è‰¯å¥½")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼šè·¨åœºæ™¯å¥–åŠ±å­˜åœ¨åå‘")

if __name__ == "__main__":
    print("ğŸš€ ç®€å•ç›¸å¯¹æ”¹è¿›å¥–åŠ±ç³»ç»Ÿæµ‹è¯•")
    print("è§£å†³è·¨åœºæ™¯è®­ç»ƒåå‘é—®é¢˜")
    print("=" * 60)
    
    test_relative_reward_calculation()
    test_scenario_comparison()
    
    print("\nâœ¨ æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ’¡ å…³é”®ä¼˜åŠ¿:")
    print("1. ç›¸åŒç›¸å¯¹åŠªåŠ› â†’ ç›¸åŒå¥–åŠ±å¹…åº¦")
    print("2. å›°éš¾åœºæ™¯ä¸è¢«å¿½è§†")
    print("3. è®­ç»ƒè¿‡ç¨‹æ›´åŠ å‡è¡¡")
    print("4. å®ç°ç®€å•ï¼Œæ€§èƒ½æ— æŸ")
