#!/usr/bin/env python3
"""
ç”µåŠ›ç½‘ç»œåˆ†åŒºæ™ºèƒ½ä½“ç»¼åˆè¯„ä¼°ç³»ç»Ÿ

é›†æˆæ‰€æœ‰è¯„ä¼°åŠŸèƒ½ï¼š
- åŒºåŸŸå†…å¹³è¡¡æŒ‡æ ‡è®¡ç®—
- å¤šç§baselineæ–¹æ³•å¯¹æ¯”
- è·¨åœºæ™¯æµ‹è¯•å’Œæ³›åŒ–è¯„ä¼°
- è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import time
from sklearn.cluster import SpectralClustering
import sys

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / 'src'))
sys.path.append(str(Path(__file__).parent.parent))

from train import UnifiedTrainingSystem, load_power_grid_data
from data_processing import PowerGridDataProcessor
from gat import create_hetero_graph_encoder
from rl.environment import PowerGridPartitioningEnv
from rl.agent import PPOAgent


class BaselinePartitioner:
    """Baselineåˆ†åŒºæ–¹æ³•é›†åˆ"""
    
    @staticmethod
    def random_partition(adjacency_matrix: np.ndarray, num_partitions: int, seed: int = 42) -> np.ndarray:
        """éšæœºåˆ†åŒºæ–¹æ³•"""
        np.random.seed(seed)
        num_nodes = adjacency_matrix.shape[0]
        partition = np.random.randint(1, num_partitions + 1, size=num_nodes)
        return partition
    
    @staticmethod
    def degree_based_partition(adjacency_matrix: np.ndarray, num_partitions: int) -> np.ndarray:
        """åŸºäºåº¦ä¸­å¿ƒæ€§çš„åˆ†åŒºæ–¹æ³•"""
        # è®¡ç®—èŠ‚ç‚¹åº¦æ•°
        degrees = np.sum(adjacency_matrix, axis=1)
        
        # æŒ‰åº¦æ•°æ’åº
        sorted_indices = np.argsort(degrees)[::-1]  # é™åº
        
        # è½®æµåˆ†é…åˆ°å„åˆ†åŒº
        partition = np.zeros(len(degrees), dtype=int)
        for i, node_idx in enumerate(sorted_indices):
            partition[node_idx] = (i % num_partitions) + 1
            
        return partition
    
    @staticmethod
    def spectral_partition(adjacency_matrix: np.ndarray, num_partitions: int, random_state: int = 42) -> np.ndarray:
        """è°±èšç±»åˆ†åŒºæ–¹æ³•"""
        try:
            # ç¡®ä¿é‚»æ¥çŸ©é˜µæ˜¯å¯¹ç§°çš„
            adj_symmetric = (adjacency_matrix + adjacency_matrix.T) / 2
            
            # è°±èšç±»
            spectral = SpectralClustering(
                n_clusters=num_partitions,
                affinity='precomputed',
                random_state=random_state,
                assign_labels='discretize'
            )
            
            labels = spectral.fit_predict(adj_symmetric)
            # è½¬æ¢ä¸º1-basedç´¢å¼•
            partition = labels + 1
            
            return partition
            
        except Exception as e:
            print(f"âš ï¸ è°±èšç±»å¤±è´¥ï¼Œä½¿ç”¨éšæœºåˆ†åŒº: {e}")
            return BaselinePartitioner.random_partition(adjacency_matrix, num_partitions, random_state)


class ScenarioGenerator:
    """æµ‹è¯•åœºæ™¯ç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_test_scenarios(original_mpc: Dict, network_name: str) -> Dict[str, Dict]:
        """ç”Ÿæˆ4ç§æµ‹è¯•åœºæ™¯"""
        scenarios = {}
        
        # 1. æ­£å¸¸åœºæ™¯
        scenarios['normal'] = original_mpc.copy()
        
        # 2. é«˜è´Ÿè½½åœºæ™¯ (æ‰€æœ‰è´Ÿè½½Ã—1.5)
        high_load_mpc = original_mpc.copy()
        high_load_mpc['bus'] = high_load_mpc['bus'].copy()
        high_load_mpc['bus'][:, [2, 3]] *= 1.5  # PD, QDåˆ—
        scenarios['high_load'] = high_load_mpc
        
        # 3. ä¸å¹³è¡¡åœºæ™¯ (éšæœº30%èŠ‚ç‚¹è´Ÿè½½Ã—2ï¼Œå…¶ä»–Ã—0.8)
        unbalanced_mpc = original_mpc.copy()
        unbalanced_mpc['bus'] = unbalanced_mpc['bus'].copy()
        num_nodes = unbalanced_mpc['bus'].shape[0]
        np.random.seed(42)  # ç¡®ä¿å¯é‡å¤
        high_load_nodes = np.random.choice(num_nodes, size=int(0.3 * num_nodes), replace=False)
        
        # é«˜è´Ÿè½½èŠ‚ç‚¹
        for node in high_load_nodes:
            unbalanced_mpc['bus'][node, [2, 3]] *= 2.0
        # å…¶ä»–èŠ‚ç‚¹
        other_nodes = np.setdiff1d(np.arange(num_nodes), high_load_nodes)
        for node in other_nodes:
            unbalanced_mpc['bus'][node, [2, 3]] *= 0.8
        scenarios['unbalanced'] = unbalanced_mpc
        
        # 4. æ•…éšœåœºæ™¯ (ç§»é™¤åº¦æ•°æœ€é«˜çš„1-2æ¡è¾¹)
        fault_mpc = original_mpc.copy()
        fault_mpc['branch'] = fault_mpc['branch'].copy()
        
        # è®¡ç®—è¾¹çš„é‡è¦æ€§ï¼ˆè¿æ¥åº¦æ•°é«˜çš„èŠ‚ç‚¹ï¼‰
        branch_importance = []
        for i, branch in enumerate(fault_mpc['branch']):
            from_bus = int(branch[0]) - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•
            to_bus = int(branch[1]) - 1
            # ç®€å•çš„é‡è¦æ€§è¯„ä¼°ï¼šè¿æ¥èŠ‚ç‚¹çš„åº¦æ•°ä¹‹å’Œ
            importance = from_bus + to_bus  # ç®€åŒ–ç‰ˆæœ¬
            branch_importance.append((importance, i))
        
        # ç§»é™¤æœ€é‡è¦çš„1æ¡è¾¹
        branch_importance.sort(reverse=True)
        if len(branch_importance) > 1:
            fault_branch_idx = branch_importance[0][1]
            fault_mpc['branch'] = np.delete(fault_mpc['branch'], fault_branch_idx, axis=0)
        
        scenarios['fault'] = fault_mpc
        
        return scenarios


class MetricsCalculator:
    """æŒ‡æ ‡è®¡ç®—å™¨ - åŒ…å«åŒºåŸŸå†…å¹³è¡¡æŒ‡æ ‡"""
    
    @staticmethod
    def calculate_intra_region_balance(partition: np.ndarray, node_loads: np.ndarray) -> float:
        """è®¡ç®—åŒºåŸŸå†…éƒ¨å¹³è¡¡åº¦ - æ–°å¢æŒ‡æ ‡"""
        unique_partitions = np.unique(partition[partition > 0])
        intra_cvs = []
        
        for pid in unique_partitions:
            mask = (partition == pid)
            region_loads = np.abs(node_loads[mask])
            
            if len(region_loads) <= 1:
                intra_cvs.append(0.0)  # å•èŠ‚ç‚¹åˆ†åŒºå®Œå…¨å‡è¡¡
            else:
                mean_load = np.mean(region_loads)
                std_load = np.std(region_loads)
                cv = std_load / (mean_load + 1e-8)
                intra_cvs.append(cv)
        
        return np.mean(intra_cvs) if intra_cvs else 1.0
    
    @staticmethod
    def calculate_inter_region_balance(partition: np.ndarray, node_loads: np.ndarray) -> float:
        """è®¡ç®—åˆ†åŒºé—´å¹³è¡¡åº¦ - ä¼ ç»ŸCVæŒ‡æ ‡"""
        unique_partitions = np.unique(partition[partition > 0])
        partition_loads = []
        
        for pid in unique_partitions:
            mask = (partition == pid)
            total_load = np.sum(np.abs(node_loads[mask]))
            partition_loads.append(total_load)
        
        if len(partition_loads) <= 1:
            return 0.0
        
        mean_load = np.mean(partition_loads)
        std_load = np.std(partition_loads)
        cv = std_load / (mean_load + 1e-8)
        
        return cv
    
    @staticmethod
    def calculate_decoupling(partition: np.ndarray, edge_index: np.ndarray) -> float:
        """è®¡ç®—ç”µæ°”è§£è€¦åº¦"""
        if edge_index.shape[1] == 0:
            return 1.0
        
        cross_partition_edges = 0
        total_edges = edge_index.shape[1]
        
        for i in range(total_edges):
            from_node = edge_index[0, i]
            to_node = edge_index[1, i]
            
            if partition[from_node] != partition[to_node]:
                cross_partition_edges += 1
        
        coupling_ratio = cross_partition_edges / total_edges
        decoupling_ratio = 1.0 - coupling_ratio
        
        return decoupling_ratio
    
    @staticmethod
    def calculate_comprehensive_score(inter_cv: float, intra_cv: float, decoupling: float,
                                    weights: Dict[str, float] = None) -> float:
        """è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°"""
        if weights is None:
            weights = {'inter_balance': 0.3, 'intra_balance': 0.3, 'decoupling': 0.4}
        
        # è½¬æ¢ä¸ºåˆ†æ•°ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
        inter_score = 1.0 / (1.0 + inter_cv)
        intra_score = 1.0 / (1.0 + intra_cv)
        decoupling_score = decoupling
        
        comprehensive_score = (
            weights['inter_balance'] * inter_score +
            weights['intra_balance'] * intra_score +
            weights['decoupling'] * decoupling_score
        )
        
        return comprehensive_score

    @staticmethod
    def convert_to_scores(load_b: float, decoupling: float, power_b: float):
        """å°†åŸå§‹(è¶Šå°è¶Šå¥½)æŒ‡æ ‡è½¬æ¢ä¸ºåˆ†æ•°(è¶Šå¤§è¶Šå¥½)"""
        load_b_score = 1.0 / (1.0 + load_b)
        decoupling_score = 1.0 - decoupling
        power_b_score = 1.0 / (1.0 + power_b)
        return load_b_score, decoupling_score, power_b_score

    @staticmethod
    def calculate_comprehensive_score_v2(load_b_score: float, decoupling_score: float, power_b_score: float,
                                         weights: Dict[str, float] = None) -> float:
        """æ ¹æ®ä¸‰ä¸ªåˆ†æ•°è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•° (è¶Šå¤§è¶Šå¥½)"""
        if weights is None:
            weights = {'load_b': 0.4, 'decoupling': 0.4, 'power_b': 0.2}
        # å½’ä¸€åŒ–æƒé‡
        total_w = sum(weights.values()) + 1e-8
        w_load = weights['load_b'] / total_w
        w_dec = weights['decoupling'] / total_w
        w_pow = weights['power_b'] / total_w
        return w_load * load_b_score + w_dec * decoupling_score + w_pow * power_b_score


class ComprehensiveAgentEvaluator:
    """ç»¼åˆæ™ºèƒ½ä½“è¯„ä¼°å™¨"""

    def __init__(self, config_path: Optional[str] = None, model_path: Optional[str] = None):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.system = UnifiedTrainingSystem(config_path)
        self.config = self.system.config
        self.device = self.system.device
        self.model_path = model_path

        # è¯„ä¼°é…ç½®
        self.evaluation_config = {
            'baseline_comparison': {
                'enabled': True,
                'test_networks': ['ieee14', 'ieee30'],
                'scenarios': ['normal', 'high_load', 'unbalanced', 'fault'],
                'runs_per_test': 10
            },
            'generalization_test': {
                'enabled': True,
                'train_network': 'ieee14',
                'test_networks': ['ieee30', 'ieee57', 'ieee118'],
                'runs_per_network': 20
            },
            'metrics_weights': {
                'inter_balance': 0.3,
                'intra_balance': 0.3,
                'decoupling': 0.4
            },
            # æ–°æŒ‡æ ‡æƒé‡ (score è¶Šå¤§è¶Šå¥½)
            'metrics_weights_v2': {
                'load_b': 0.4,
                'decoupling': 0.4,
                'power_b': 0.2
            },
            'thresholds': {
                'excellent_generalization': 10,  # <10%æ€§èƒ½ä¸‹é™
                'good_generalization': 20       # 10-20%æ€§èƒ½ä¸‹é™
            }
        }
        
        print(f"ğŸ”¬ ç»¼åˆè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   è®­ç»ƒç½‘ç»œ: {self.config['data']['case_name']}")
        print(f"   è®¾å¤‡: {self.device}")
    
    def get_evaluation_config(self, mode: str = 'standard') -> Dict[str, Any]:
        """è·å–æŒ‡å®šæ¨¡å¼çš„è¯„ä¼°é…ç½®"""
        modes = {
            'quick': {
                'baseline_comparison': {
                    'enabled': True,
                    'test_networks': ['ieee14', 'ieee30'],
                    'scenarios': ['normal', 'high_load'],
                    'runs_per_test': 5
                },
                'generalization_test': {
                    'enabled': True,
                    'train_network': 'ieee14',
                    'test_networks': ['ieee30'],
                    'runs_per_network': 10
                }
            },
            'standard': {
                'baseline_comparison': {
                    'enabled': True,
                    'test_networks': ['ieee14', 'ieee30', 'ieee57'],
                    'scenarios': ['normal', 'high_load', 'unbalanced'],
                    'runs_per_test': 10
                },
                'generalization_test': {
                    'enabled': True,
                    'train_network': 'ieee14',
                    'test_networks': ['ieee30', 'ieee57'],
                    'runs_per_network': 20
                }
            },
            'comprehensive': {
                'baseline_comparison': {
                    'enabled': True,
                    'test_networks': ['ieee14', 'ieee30', 'ieee57'],
                    'scenarios': ['normal', 'high_load', 'unbalanced', 'fault'],
                    'runs_per_test': 15
                },
                'generalization_test': {
                    'enabled': True,
                    'train_network': 'ieee14',
                    'test_networks': ['ieee30', 'ieee57', 'ieee118'],
                    'runs_per_network': 30
                }
            }
        }
        
        return modes.get(mode, modes['standard'])
    
    def run_baseline_comparison(self, network_name: str = 'ieee30') -> Dict[str, Any]:
        """è¿è¡Œbaselineå¯¹æ¯”æµ‹è¯•"""
        print(f"\nğŸ” å¼€å§‹baselineå¯¹æ¯”æµ‹è¯• - {network_name.upper()}")
        print("=" * 50)

        # 1. å¿…é¡»ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
        if not self.model_path:
            print("âŒ é”™è¯¯ï¼šå¿…é¡»æä¾›é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
            print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•: python test.py --baseline --model <æ¨¡å‹è·¯å¾„>")
            return {'success': False, 'error': 'No pretrained model provided'}

        print(f"ğŸ“‚ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {self.model_path}")
        agent, env = self._create_test_environment(network_name)
        try:
            self._load_pretrained_model(agent, self.model_path)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return {'success': False, 'error': f'Model loading failed: {e}'}

        # 2. å‡†å¤‡æµ‹è¯•æ•°æ®
        test_mpc = load_power_grid_data(network_name)
        scenarios = ScenarioGenerator.generate_test_scenarios(test_mpc, network_name)
        
        # 4. è¿è¡Œå¯¹æ¯”æµ‹è¯•
        results = {}
        
        for scenario_name, mpc_data in scenarios.items():
            print(f"\n   ğŸ” æµ‹è¯•åœºæ™¯: {scenario_name}")
            
            scenario_results = {}
            
            # æµ‹è¯•æˆ‘çš„æ™ºèƒ½ä½“
            agent_metrics = self._test_agent_on_scenario(agent, mpc_data, scenario_name)
            scenario_results['my_agent'] = agent_metrics
            
            # æµ‹è¯•baselineæ–¹æ³•
            baseline_methods = {
                'random': BaselinePartitioner.random_partition,
                'degree_based': BaselinePartitioner.degree_based_partition,
                'spectral': BaselinePartitioner.spectral_partition
            }
            
            for baseline_name, baseline_method in baseline_methods.items():
                baseline_metrics = self._test_baseline_on_scenario(
                    baseline_method, mpc_data, scenario_name
                )
                scenario_results[baseline_name] = baseline_metrics
            
            results[scenario_name] = scenario_results
        
        # 5. æ ¼å¼åŒ–ç»“æœ
        comparison_results = self._format_comparison_results(results)
        
        return {
            'success': True,
            'network': network_name,
            'results': comparison_results,
            'summary': self._generate_comparison_summary(comparison_results)
        }

    def run_generalization_test(self, train_network: str = 'ieee14') -> Dict[str, Any]:
        """è¿è¡Œæ³›åŒ–èƒ½åŠ›æµ‹è¯•"""
        print(f"\nğŸŒ å¼€å§‹æ³›åŒ–èƒ½åŠ›æµ‹è¯•")
        print("=" * 50)

        # 1. å¿…é¡»ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
        if not self.model_path:
            print("âŒ é”™è¯¯ï¼šå¿…é¡»æä¾›é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
            print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•: python test.py --generalization --model <æ¨¡å‹è·¯å¾„>")
            return {'success': False, 'error': 'No pretrained model provided'}

        print(f"ğŸ“‚ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {self.model_path}")
        train_agent, train_env = self._create_test_environment(train_network)
        try:
            self._load_pretrained_model(train_agent, self.model_path)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return {'success': False, 'error': f'Model loading failed: {e}'}

        # 2. è·å–è®­ç»ƒç½‘ç»œbaselineæ€§èƒ½
        train_mpc = load_power_grid_data(train_network)
        train_score = self._test_agent_on_scenario(train_agent, train_mpc, 'normal')

        print(f"âœ… è®­ç»ƒç½‘ç»œæ€§èƒ½: {train_score['comprehensive_score']:.3f}")

        # 3. æµ‹è¯•æ³›åŒ–èƒ½åŠ›
        test_networks = self.evaluation_config['generalization_test']['test_networks']
        test_networks = [net for net in test_networks if net != train_network]

        results = {train_network: train_score}

        for test_network in test_networks:
            print(f"\n   ğŸ” æµ‹è¯•æ³›åŒ–åˆ° {test_network.upper()}...")

            try:
                test_mpc = load_power_grid_data(test_network)
                test_score = self._test_agent_cross_network(train_agent, test_mpc, test_network)

                # è®¡ç®—æ€§èƒ½ä¸‹é™
                degradation = (train_score['comprehensive_score'] - test_score['comprehensive_score']) / train_score['comprehensive_score'] * 100

                results[test_network] = {
                    **test_score,
                    'degradation_pct': degradation,
                    'status': self._classify_generalization(degradation)
                }

                print(f"     ç»“æœ: ç»¼åˆåˆ†æ•° {test_score['comprehensive_score']:.3f}, æ€§èƒ½ä¸‹é™ {degradation:.1f}% - {results[test_network]['status']}")

            except Exception as e:
                print(f"     âŒ æµ‹è¯•å¤±è´¥: {e}")
                results[test_network] = {
                    'comprehensive_score': 0.0,
                    'degradation_pct': 100.0,
                    'status': 'æµ‹è¯•å¤±è´¥',
                    'error': str(e)
                }

        return {
            'success': True,
            'train_network': train_network,
            'results': results,
            'summary': self._generate_generalization_summary(results, train_network)
        }

    def _create_test_environment(self, network_name: str):
        """åˆ›å»ºæµ‹è¯•ç¯å¢ƒ"""
        # åŠ è½½æ•°æ®
        mpc = load_power_grid_data(network_name)

        # åˆ›å»ºæµ‹è¯•é…ç½®
        test_config = self.config.copy()
        test_config['data']['case_name'] = network_name

        # åŠ¨æ€è°ƒæ•´åˆ†åŒºæ•° - ç»Ÿä¸€ä½¿ç”¨3åˆ†åŒºä»¥åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹
        test_bus_count = mpc['bus'].shape[0]
        if test_bus_count <= 57:
            test_partitions = 3  # IEEE14, IEEE30, IEEE57éƒ½ä½¿ç”¨3åˆ†åŒºä»¥åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹
        else:
            test_partitions = 5  # IEEE118ç­‰å¤§å‹ç½‘ç»œä½¿ç”¨5åˆ†åŒº

        test_config['environment']['num_partitions'] = test_partitions

        # æ•°æ®å¤„ç†
        processor = PowerGridDataProcessor(
            normalize=test_config['data']['normalize'],
            cache_dir=test_config['data']['cache_dir']
        )
        hetero_data = processor.graph_from_mpc(mpc, test_config).to(self.device)

        # GATç¼–ç å™¨
        gat_config = test_config['gat']
        encoder = create_hetero_graph_encoder(
            hetero_data,
            hidden_channels=gat_config['hidden_channels'],
            gnn_layers=gat_config['gnn_layers'],
            heads=gat_config['heads'],
            output_dim=gat_config['output_dim']
        ).to(self.device)

        with torch.no_grad():
            node_embeddings, attention_weights = encoder.encode_nodes_with_attention(hetero_data, test_config)

        # ç¯å¢ƒ
        env_config = test_config['environment']
        env = PowerGridPartitioningEnv(
            hetero_data=hetero_data,
            node_embeddings=node_embeddings,
            num_partitions=test_partitions,
            reward_weights=env_config['reward_weights'],
            max_steps=env_config['max_steps'],
            device=self.device,
            attention_weights=attention_weights,
            config=test_config
        )

        # æ™ºèƒ½ä½“
        agent_config = test_config['agent']
        node_embedding_dim = env.state_manager.embedding_dim
        region_embedding_dim = node_embedding_dim * 2

        agent = PPOAgent(
            node_embedding_dim=node_embedding_dim,
            region_embedding_dim=region_embedding_dim,
            num_partitions=env.num_partitions,
            lr_actor=agent_config['lr_actor'],
            lr_critic=agent_config['lr_critic'],
            gamma=agent_config['gamma'],
            eps_clip=agent_config['eps_clip'],
            k_epochs=agent_config['k_epochs'],
            entropy_coef=agent_config['entropy_coef'],
            value_coef=agent_config['value_coef'],
            device=self.device
        )

        return agent, env

    def _load_pretrained_model(self, agent, model_path: str):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        import torch
        from pathlib import Path

        model_file = Path(model_path)
        if not model_file.exists():
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

        # åŠ è½½æ¨¡å‹çŠ¶æ€ (ä½¿ç”¨å®‰å…¨çš„weights_onlyæ¨¡å¼)
        try:
            checkpoint = torch.load(model_file, map_location=self.device, weights_only=False)
        except Exception as e:
            # å¦‚æœweights_only=Trueå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ¨¡å¼ä½†å‘å‡ºè­¦å‘Š
            print(f"Warning: Failed to load with weights_only=True, falling back to legacy mode: {e}")
            checkpoint = torch.load(model_file, map_location=self.device, weights_only=False)

        # æ£€æŸ¥checkpointæ ¼å¼
        if 'actor_state_dict' in checkpoint and 'critic_state_dict' in checkpoint:
            # æ ‡å‡†æ ¼å¼
            agent.actor.load_state_dict(checkpoint['actor_state_dict'])
            agent.critic.load_state_dict(checkpoint['critic_state_dict'])

            # å¦‚æœæœ‰ä¼˜åŒ–å™¨çŠ¶æ€ä¹ŸåŠ è½½
            if 'actor_optimizer_state_dict' in checkpoint:
                agent.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
            if 'critic_optimizer_state_dict' in checkpoint:
                agent.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼: {model_path}")

        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        agent.actor.eval()
        agent.critic.eval()

        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")

    def _test_agent_on_scenario(self, agent, mpc_data: Dict, scenario_name: str, num_runs: int = 10) -> Dict[str, float]:
        """æµ‹è¯•æ™ºèƒ½ä½“åœ¨ç‰¹å®šåœºæ™¯ä¸Šçš„æ€§èƒ½"""
        # åˆ›å»ºåœºæ™¯ç¯å¢ƒ
        env = self._create_scenario_environment(mpc_data, scenario_name)

        metrics_list = []

        for run in range(num_runs):
            state, _ = env.reset()
            episode_reward = 0

            for step in range(200):  # æœ€å¤§æ­¥æ•°
                action, _, _ = agent.select_action(state, training=False)
                if action is None:
                    break

                state, reward, terminated, truncated, info = env.step(action)
                episode_reward += reward

                if terminated or truncated:
                    break

            # è·å–æœ€ç»ˆåˆ†åŒº
            final_partition = env.state_manager.current_partition.cpu().numpy()

            # è®¡ç®—æŒ‡æ ‡
            metrics = self._calculate_all_metrics(final_partition, env)
            metrics_list.append(metrics)

        # å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        for key in metrics_list[0].keys():
            avg_metrics[key] = np.mean([m[key] for m in metrics_list])

        return avg_metrics

    def _test_baseline_on_scenario(self, baseline_method, mpc_data: Dict, scenario_name: str, num_runs: int = 10) -> Dict[str, float]:
        """æµ‹è¯•baselineæ–¹æ³•åœ¨ç‰¹å®šåœºæ™¯ä¸Šçš„æ€§èƒ½"""
        # åˆ›å»ºåœºæ™¯ç¯å¢ƒ
        env = self._create_scenario_environment(mpc_data, scenario_name)

        # è·å–é‚»æ¥çŸ©é˜µ
        edge_index = env.edge_info['edge_index'].cpu().numpy()
        num_nodes = env.total_nodes
        adjacency_matrix = np.zeros((num_nodes, num_nodes))

        for i in range(edge_index.shape[1]):
            u, v = edge_index[0, i], edge_index[1, i]
            adjacency_matrix[u, v] = 1
            adjacency_matrix[v, u] = 1

        metrics_list = []

        for run in range(num_runs):
            # ç”Ÿæˆåˆ†åŒº
            partition = baseline_method(adjacency_matrix, env.num_partitions)

            # è®¡ç®—æŒ‡æ ‡
            metrics = self._calculate_all_metrics(partition, env)
            metrics_list.append(metrics)

        # å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        for key in metrics_list[0].keys():
            avg_metrics[key] = np.mean([m[key] for m in metrics_list])

        return avg_metrics

    def _test_agent_cross_network(self, agent, mpc_data: Dict, network_name: str) -> Dict[str, float]:
        """æµ‹è¯•æ™ºèƒ½ä½“è·¨ç½‘ç»œæ€§èƒ½"""
        return self._test_agent_on_scenario(agent, mpc_data, 'cross_network', num_runs=20)

    def _create_scenario_environment(self, mpc_data: Dict, scenario_name: str):
        """ä¸ºç‰¹å®šåœºæ™¯åˆ›å»ºç¯å¢ƒ"""
        # åˆ›å»ºä¸´æ—¶é…ç½®
        temp_config = self.config.copy()

        # åŠ¨æ€è°ƒæ•´åˆ†åŒºæ•° - ç»Ÿä¸€ä½¿ç”¨3åˆ†åŒºä»¥åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹
        test_bus_count = mpc_data['bus'].shape[0]
        if test_bus_count <= 57:
            test_partitions = 3  # IEEE14, IEEE30, IEEE57éƒ½ä½¿ç”¨3åˆ†åŒºä»¥åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹
        else:
            test_partitions = 5  # IEEE118ç­‰å¤§å‹ç½‘ç»œä½¿ç”¨5åˆ†åŒº

        temp_config['environment']['num_partitions'] = test_partitions

        # æ•°æ®å¤„ç†
        processor = PowerGridDataProcessor(
            normalize=temp_config['data']['normalize'],
            cache_dir=temp_config['data']['cache_dir']
        )
        hetero_data = processor.graph_from_mpc(mpc_data, temp_config).to(self.device)

        # GATç¼–ç å™¨
        gat_config = temp_config['gat']
        encoder = create_hetero_graph_encoder(
            hetero_data,
            hidden_channels=gat_config['hidden_channels'],
            gnn_layers=gat_config['gnn_layers'],
            heads=gat_config['heads'],
            output_dim=gat_config['output_dim']
        ).to(self.device)

        with torch.no_grad():
            node_embeddings, attention_weights = encoder.encode_nodes_with_attention(hetero_data, temp_config)

        # ç¯å¢ƒ
        env_config = temp_config['environment']
        env = PowerGridPartitioningEnv(
            hetero_data=hetero_data,
            node_embeddings=node_embeddings,
            num_partitions=test_partitions,
            reward_weights=env_config['reward_weights'],
            max_steps=env_config['max_steps'],
            device=self.device,
            attention_weights=attention_weights,
            config=temp_config
        )

        return env

    def _calculate_all_metrics(self, partition: np.ndarray, env) -> Dict[str, float]:
        """è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ (æ–°ç‰ˆæœ¬)"""
        # å°†åˆ†åŒºè½¬æ¢ä¸ºtorchå¼ é‡ä»¥ä½¿ç”¨reward_function
        import torch
        partition_tensor = torch.tensor(partition, dtype=torch.long, device=env.device)

        # ä½¿ç”¨ç»Ÿä¸€å¥–åŠ±ç³»ç»Ÿçš„æ ¸å¿ƒæŒ‡æ ‡æ¥å£
        core_metrics = env.reward_function.get_current_metrics(partition_tensor)

        load_b = core_metrics.get('cv', 1.0)
        decoupling_raw = core_metrics.get('coupling_ratio', 1.0)  # è¶Šå°è¶Šå¥½
        power_b = core_metrics.get('power_imbalance_normalized', 1.0)

        # åˆ†æ•°è½¬æ¢
        load_b_score, decoupling_score, power_b_score = MetricsCalculator.convert_to_scores(
            load_b, decoupling_raw, power_b)

        # ç»¼åˆåˆ†æ•°
        comprehensive_score = MetricsCalculator.calculate_comprehensive_score_v2(
            load_b_score, decoupling_score, power_b_score, self.evaluation_config.get('metrics_weights_v2', None))

        return {
            'load_b': load_b,
            'decoupling': decoupling_raw,
            'power_b': power_b,
            'load_b_score': load_b_score,
            'decoupling_score': decoupling_score,
            'power_b_score': power_b_score,
            'comprehensive_score': comprehensive_score
        }

    def _classify_generalization(self, degradation_pct: float) -> str:
        """åˆ†ç±»æ³›åŒ–èƒ½åŠ›"""
        if degradation_pct < self.evaluation_config['thresholds']['excellent_generalization']:
            return "ğŸ‰ ä¼˜ç§€æ³›åŒ–"
        elif degradation_pct < self.evaluation_config['thresholds']['good_generalization']:
            return "âœ… è‰¯å¥½æ³›åŒ–"
        else:
            return "âš ï¸ æ³›åŒ–ä¸è¶³"

    def _format_comparison_results(self, results: Dict) -> Dict[str, Any]:
        """æ ¼å¼åŒ–å¯¹æ¯”ç»“æœ (updated)"""
        formatted_results = {}

        for scenario_name, scenario_data in results.items():
            formatted_scenario = {}

            for method_name, metrics in scenario_data.items():
                formatted_scenario[method_name] = {
                    'load_b': metrics['load_b'],
                    'decoupling': metrics['decoupling'],
                    'power_b': metrics['power_b'],
                    'load_b_score': metrics['load_b_score'],
                    'decoupling_score': metrics['decoupling_score'],
                    'power_b_score': metrics['power_b_score'],
                    'comprehensive_score': metrics['comprehensive_score']
                }

            formatted_results[scenario_name] = formatted_scenario

        return formatted_results

    def _generate_comparison_summary(self, results: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆå¯¹æ¯”æ€»ç»“"""
        summary = {
            'overall_improvement': {},
            'best_scenarios': {},
            'method_rankings': {}
        }

        # è®¡ç®—æ€»ä½“æ”¹å–„å¹…åº¦
        all_improvements = []

        for scenario_name, scenario_data in results.items():
            my_agent_score = scenario_data['my_agent']['comprehensive_score']

            # æ‰¾åˆ°æœ€ä½³baseline
            baseline_scores = {k: v['comprehensive_score'] for k, v in scenario_data.items() if k != 'my_agent'}
            best_baseline_score = max(baseline_scores.values()) if baseline_scores else 0
            best_baseline_name = max(baseline_scores, key=baseline_scores.get) if baseline_scores else 'none'

            # è®¡ç®—æ”¹å–„å¹…åº¦
            if best_baseline_score > 0:
                improvement = (my_agent_score - best_baseline_score) / best_baseline_score * 100
            else:
                improvement = 100.0

            all_improvements.append(improvement)

            summary['best_scenarios'][scenario_name] = {
                'my_agent_score': my_agent_score,
                'best_baseline': best_baseline_name,
                'best_baseline_score': best_baseline_score,
                'improvement_pct': improvement
            }

        summary['overall_improvement']['average_improvement'] = np.mean(all_improvements)
        summary['overall_improvement']['min_improvement'] = np.min(all_improvements)
        summary['overall_improvement']['max_improvement'] = np.max(all_improvements)

        return summary

    def _generate_generalization_summary(self, results: Dict, train_network: str) -> Dict[str, Any]:
        """ç”Ÿæˆæ³›åŒ–èƒ½åŠ›æ€»ç»“"""
        summary = {
            'train_network': train_network,
            'train_score': results[train_network]['comprehensive_score'],
            'test_results': {},
            'overall_generalization': {}
        }

        degradations = []

        for network, result in results.items():
            if network != train_network and 'error' not in result:
                summary['test_results'][network] = {
                    'score': result['comprehensive_score'],
                    'degradation_pct': result['degradation_pct'],
                    'status': result['status']
                }
                degradations.append(result['degradation_pct'])

        if degradations:
            avg_degradation = np.mean(degradations)
            summary['overall_generalization'] = {
                'average_degradation': avg_degradation,
                'status': self._classify_generalization(avg_degradation),
                'tested_networks': len(degradations)
            }

        return summary

    def create_comparison_visualization(self, results: Dict, save_path: Optional[str] = None) -> None:
        """Create comparison visualization chart (updated version)"""
        print("ğŸ“Š Generating comparison visualization chart...")

        # Prepare data
        scenarios = list(results.keys())
        methods = list(results[scenarios[0]].keys())

        # ã€æ–°ã€‘æŒ‡æ ‡åˆ—è¡¨
        score_metrics = ['load_b_score', 'decoupling_score', 'power_b_score', 'comprehensive_score']
        raw_metrics = ['load_b', 'decoupling', 'power_b']

        fig, axes = plt.subplots(2, 4, figsize=(22, 10))
        fig.suptitle('Agent vs Baseline Performance Comparison', fontsize=16, fontweight='bold')

        # Colors
        colors = plt.cm.Set3(np.linspace(0, 1, len(methods)))

        # --- ç»˜åˆ¶scoreæŒ‡æ ‡ (ç¬¬ä¸€è¡Œ) ---
        for idx, metric in enumerate(score_metrics):
            ax = axes[0, idx]
            width = 0.8 / len(methods)
            x = np.arange(len(scenarios))

            for j, method in enumerate(methods):
                values = [results[sc][method][metric] for sc in scenarios]
                ax.bar(x + j * width, values, width, label=method, color=colors[j], alpha=0.85)

            title_map = {
                'load_b_score': 'Load Balance Score',
                'decoupling_score': 'Decoupling Score',
                'power_b_score': 'Power Balance Score',
                'comprehensive_score': 'Comprehensive Quality Score'
            }
            ax.set_title(title_map.get(metric, metric), fontsize=11, fontweight='bold')
            ax.set_xticks(x + width * (len(methods) - 1) / 2)
            ax.set_xticklabels(scenarios, rotation=45)
            ax.set_ylabel('Score (â†‘ Better)')
            ax.grid(True, alpha=0.3)
            if idx == 0:
                ax.legend()

        # --- ç»˜åˆ¶åŸå§‹æŒ‡æ ‡ (ç¬¬äºŒè¡Œ) ---
        for idx, metric in enumerate(raw_metrics):
            ax = axes[1, idx]
            width = 0.8 / len(methods)
            x = np.arange(len(scenarios))

            for j, method in enumerate(methods):
                values = [results[sc][method][metric] for sc in scenarios]
                ax.bar(x + j * width, values, width, label=method, color=colors[j], alpha=0.85)

            title_map_raw = {
                'load_b': 'Load CV (raw)',
                'decoupling': 'Coupling Ratio (raw)',
                'power_b': 'Power Imbalance (raw)'
            }
            ax.set_title(title_map_raw.get(metric, metric), fontsize=11, fontweight='bold')
            ax.set_xticks(x + width * (len(methods) - 1) / 2)
            ax.set_xticklabels(scenarios, rotation=45)
            ax.set_ylabel('Raw Value (â†“ Better)')
            ax.grid(True, alpha=0.3)
            if idx == 0:
                ax.legend()

        # ç§»é™¤å¤šä½™å­å›¾ (axes[1,3])
        fig.delaxes(axes[1, 3])

        plt.tight_layout(rect=[0, 0, 1, 0.96])

        # Save figure
        if save_path is None:
            from path_helper import create_timestamp_directory
            eval_dir = create_timestamp_directory('evaluation', ['reports', 'metrics', 'comparisons'])
            save_path = f'{eval_dir}/comparisons/comparison_visualization.png'

        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… Comparison chart saved to: {save_path}")
        plt.close()

    def create_generalization_visualization(self, results: Dict, train_network: str, save_path: Optional[str] = None) -> None:
        """Create generalization capability visualization chart"""
        print("ğŸŒ Generating generalization capability chart...")

        # Prepare data
        networks = []
        scores = []
        degradations = []

        for network, result in results.items():
            if 'error' not in result:
                networks.append(network)
                scores.append(result['comprehensive_score'])
                if network != train_network:
                    degradations.append(result['degradation_pct'])
                else:
                    degradations.append(0.0)

        # Create chart
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        fig.suptitle('Cross-Network Generalization Analysis', fontsize=16, fontweight='bold')

        # Subplot 1: Performance score comparison
        colors = ['red' if net == train_network else 'blue' for net in networks]
        bars1 = ax1.bar(networks, scores, color=colors, alpha=0.7)
        ax1.set_title('Performance Score Comparison Across Networks')
        ax1.set_ylabel('Comprehensive Quality Score')
        ax1.set_xlabel('Networks')
        ax1.grid(True, alpha=0.3)

        # Add value labels
        for bar, score in zip(bars1, scores):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{score:.3f}', ha='center', va='bottom')

        # Subplot 2: Performance degradation trend
        test_networks = [net for net in networks if net != train_network]
        test_degradations = [deg for net, deg in zip(networks, degradations) if net != train_network]

        if test_degradations:
            colors2 = ['green' if deg < 10 else 'orange' if deg < 20 else 'red' for deg in test_degradations]
            bars2 = ax2.bar(test_networks, test_degradations, color=colors2, alpha=0.7)
            ax2.set_title('Performance Degradation Percentage')
            ax2.set_ylabel('Performance Degradation (%)')
            ax2.set_xlabel('Test Networks')
            ax2.grid(True, alpha=0.3)

            # Add threshold lines
            ax2.axhline(y=10, color='orange', linestyle='--', alpha=0.7, label='Good Threshold (10%)')
            ax2.axhline(y=20, color='red', linestyle='--', alpha=0.7, label='Acceptable Threshold (20%)')
            ax2.legend()

            # Add value labels
            for bar, deg in zip(bars2, test_degradations):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                        f'{deg:.1f}%', ha='center', va='bottom')

        plt.tight_layout()

        # Save figure
        if save_path is None:
            from path_helper import create_timestamp_directory
            eval_dir = create_timestamp_directory('evaluation', ['reports', 'metrics', 'comparisons'])
            save_path = f'{eval_dir}/comparisons/generalization_visualization.png'

        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… Generalization chart saved to: {save_path}")
        plt.close()

    def generate_evaluation_report(self, comparison_results: Dict = None, generalization_results: Dict = None,
                                 save_path: Optional[str] = None) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")

        report_lines = []
        report_lines.append("ğŸ¯ æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°æŠ¥å‘Š")
        report_lines.append("=" * 50)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")

        # Baselineå¯¹æ¯”ç»“æœ
        if comparison_results:
            report_lines.append("ğŸ“Š Baselineå¯¹æ¯”ç»“æœ")
            report_lines.append("-" * 30)

            network = comparison_results['network']
            results = comparison_results['results']
            summary = comparison_results['summary']

            report_lines.append(f"æµ‹è¯•ç½‘ç»œ: {network.upper()}")
            report_lines.append("")

            # è¯¦ç»†å¯¹æ¯”è¡¨æ ¼
            report_lines.append("æŒ‡æ ‡å¯¹æ¯”è¯¦æƒ…:")
            report_lines.append("")

            # è¡¨å¤´
            header = f"{'åœºæ™¯':<10} {'æ–¹æ³•':<12} {'è´Ÿè½½CV':<10} {'è€¦åˆç‡':<10} {'åŠŸç‡ä¸å¹³è¡¡':<12} {'è´Ÿè½½S':<8} {'è§£è€¦S':<8} {'åŠŸç‡S':<8} {'ç»¼åˆåˆ†':<8}"
            report_lines.append(header)
            report_lines.append("-" * len(header))

            for scenario_name, scenario_data in results.items():
                for i, (method_name, metrics) in enumerate(scenario_data.items()):
                    scenario_display = scenario_name if i == 0 else ""
                    line = (
                        f"{scenario_display:<10} {method_name:<12} "
                        f"{metrics['load_b']:<10.3f} {metrics['decoupling']:<10.3f} {metrics['power_b']:<12.3f} "
                        f"{metrics['load_b_score']:<8.3f} {metrics['decoupling_score']:<8.3f} {metrics['power_b_score']:<8.3f} "
                        f"{metrics['comprehensive_score']:<8.3f}"
                    )
                    report_lines.append(line)
                report_lines.append("")

            # æ”¹å–„å¹…åº¦æ€»ç»“
            report_lines.append("ğŸ† æ€§èƒ½æ”¹å–„æ€»ç»“:")
            avg_improvement = summary['overall_improvement']['average_improvement']
            min_improvement = summary['overall_improvement']['min_improvement']
            max_improvement = summary['overall_improvement']['max_improvement']

            report_lines.append(f"å¹³å‡æ€§èƒ½æå‡: {avg_improvement:.1f}%")
            report_lines.append(f"æœ€å°æ€§èƒ½æå‡: {min_improvement:.1f}%")
            report_lines.append(f"æœ€å¤§æ€§èƒ½æå‡: {max_improvement:.1f}%")
            report_lines.append("")

            # å„åœºæ™¯æœ€ä½³è¡¨ç°
            for scenario_name, scenario_summary in summary['best_scenarios'].items():
                improvement = scenario_summary['improvement_pct']
                status = "ğŸ‰ ä¼˜ç§€" if improvement > 15 else "âœ… è‰¯å¥½" if improvement > 5 else "âš ï¸ æœ‰é™"
                report_lines.append(f"{scenario_name}: æå‡ {improvement:.1f}% - {status}")

            report_lines.append("")

        # æ³›åŒ–èƒ½åŠ›ç»“æœ
        if generalization_results:
            report_lines.append("ğŸŒ æ³›åŒ–èƒ½åŠ›æµ‹è¯•ç»“æœ")
            report_lines.append("-" * 30)

            train_network = generalization_results['train_network']
            results = generalization_results['results']
            summary = generalization_results['summary']

            report_lines.append(f"è®­ç»ƒç½‘ç»œ: {train_network.upper()}")
            report_lines.append(f"è®­ç»ƒç½‘ç»œæ€§èƒ½: {summary['train_score']:.3f}")
            report_lines.append("")

            # æ³›åŒ–æµ‹è¯•è¯¦æƒ…
            report_lines.append("æ³›åŒ–æµ‹è¯•è¯¦æƒ…:")
            report_lines.append("")

            header = f"{'æµ‹è¯•ç½‘ç»œ':<10} {'è´¨é‡åˆ†æ•°':<10} {'æ€§èƒ½ä¸‹é™':<10} {'æ³›åŒ–è¯„çº§':<12}"
            report_lines.append(header)
            report_lines.append("-" * len(header))

            for network, result in summary['test_results'].items():
                line = f"{network.upper():<10} {result['score']:<10.3f} {result['degradation_pct']:<10.1f}% {result['status']:<12}"
                report_lines.append(line)

            report_lines.append("")

            # æ€»ä½“æ³›åŒ–è¯„ä»·
            overall = summary['overall_generalization']
            report_lines.append("ğŸ† æ€»ä½“æ³›åŒ–èƒ½åŠ›:")
            report_lines.append(f"å¹³å‡æ€§èƒ½ä¸‹é™: {overall['average_degradation']:.1f}%")
            report_lines.append(f"æµ‹è¯•ç½‘ç»œæ•°é‡: {overall['tested_networks']}")
            report_lines.append(f"æ€»ä½“è¯„çº§: {overall['status']}")
            report_lines.append("")

        # æ€»ä½“ç»“è®º
        report_lines.append("ğŸ¯ æ€»ä½“ç»“è®º")
        report_lines.append("-" * 30)

        if comparison_results and generalization_results:
            avg_improvement = comparison_results['summary']['overall_improvement']['average_improvement']
            avg_degradation = generalization_results['summary']['overall_generalization']['average_degradation']

            if avg_improvement > 10 and avg_degradation < 15:
                conclusion = "ğŸŒŸ ä¼˜ç§€ - åœ¨æ‰€æœ‰æµ‹è¯•ä¸­è¡¨ç°å“è¶Š"
            elif avg_improvement > 5 and avg_degradation < 25:
                conclusion = "âœ… è‰¯å¥½ - æ•´ä½“æ€§èƒ½ä»¤äººæ»¡æ„"
            else:
                conclusion = "âš ï¸ éœ€è¦æ”¹è¿› - å­˜åœ¨æ€§èƒ½æå‡ç©ºé—´"

            report_lines.append(f"âœ… åœ¨baselineå¯¹æ¯”ä¸­å¹³å‡æå‡: {avg_improvement:.1f}%")
            report_lines.append(f"ğŸŒ è·¨ç½‘ç»œæ³›åŒ–å¹³å‡ä¸‹é™: {avg_degradation:.1f}%")
            report_lines.append(f"â­ ç»¼åˆè¯„çº§: {conclusion}")

        elif comparison_results:
            avg_improvement = comparison_results['summary']['overall_improvement']['average_improvement']
            if avg_improvement > 10:
                conclusion = "ğŸŒŸ ä¼˜ç§€æ€§èƒ½æå‡"
            elif avg_improvement > 5:
                conclusion = "âœ… è‰¯å¥½æ€§èƒ½æå‡"
            else:
                conclusion = "âš ï¸ æœ‰é™æ€§èƒ½æå‡"

            report_lines.append(f"âœ… å¹³å‡æ€§èƒ½æå‡: {avg_improvement:.1f}%")
            report_lines.append(f"â­ è¯„çº§: {conclusion}")

        elif generalization_results:
            avg_degradation = generalization_results['summary']['overall_generalization']['average_degradation']
            if avg_degradation < 10:
                conclusion = "ğŸŒŸ ä¼˜ç§€æ³›åŒ–èƒ½åŠ›"
            elif avg_degradation < 20:
                conclusion = "âœ… è‰¯å¥½æ³›åŒ–èƒ½åŠ›"
            else:
                conclusion = "âš ï¸ æœ‰é™æ³›åŒ–èƒ½åŠ›"

            report_lines.append(f"ğŸŒ å¹³å‡æ€§èƒ½ä¸‹é™: {avg_degradation:.1f}%")
            report_lines.append(f"â­ è¯„çº§: {conclusion}")

        # ç”ŸæˆæŠ¥å‘Šæ–‡æœ¬
        report_text = "\n".join(report_lines)

        # ä¿å­˜æŠ¥å‘Š
        if save_path is None:
            from path_helper import create_timestamp_directory
            eval_dir = create_timestamp_directory('evaluation', ['reports', 'metrics', 'comparisons'])
            save_path = f'{eval_dir}/reports/evaluation_report.txt'

        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report_text)

        print(f"âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")

        return report_text
