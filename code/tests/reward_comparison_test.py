#!/usr/bin/env python3
"""
å¥–åŠ±ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•è„šæœ¬

æ¨¡æ‹Ÿå¯¹æ¯”å®éªŒï¼ŒéªŒè¯ç®€å•ç›¸å¯¹æ”¹è¿›å¥–åŠ±æ˜¯å¦çœŸçš„è§£å†³äº†è·¨åœºæ™¯åå‘é—®é¢˜

æµ‹è¯•å†…å®¹ï¼š
1. æ¨¡æ‹Ÿä¸åŒåœºæ™¯ä¸‹çš„è®­ç»ƒè¿‡ç¨‹
2. å¯¹æ¯”ä¼ ç»Ÿç»å¯¹å¥–åŠ± vs ç®€å•ç›¸å¯¹å¥–åŠ±
3. åˆ†æåœºæ™¯è®¿é—®åˆ†å¸ƒå’Œæ€§èƒ½å·®å¼‚
4. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import json
import os
from collections import defaultdict, Counter
from typing import Dict, List, Tuple
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

class ScenarioSimulator:
    """åœºæ™¯æ¨¡æ‹Ÿå™¨ - æ¨¡æ‹Ÿä¸åŒéš¾åº¦çš„ç”µåŠ›ç³»ç»Ÿåœºæ™¯"""

    def __init__(self):
        self.scenarios = {
            'normal': {'base_quality': 0.75, 'difficulty': 1.0, 'weight': 0.4},
            'light_fault': {'base_quality': 0.60, 'difficulty': 1.5, 'weight': 0.25},
            'severe_fault': {'base_quality': 0.35, 'difficulty': 2.5, 'weight': 0.15},
            'high_load': {'base_quality': 0.50, 'difficulty': 2.0, 'weight': 0.15},
            'generation_fluctuation': {'base_quality': 0.45, 'difficulty': 2.2, 'weight': 0.05}
        }

    def sample_scenario(self) -> str:
        """æ ¹æ®æƒé‡éšæœºé‡‡æ ·åœºæ™¯"""
        scenarios = list(self.scenarios.keys())
        weights = [self.scenarios[s]['weight'] for s in scenarios]
        return np.random.choice(scenarios, p=weights)

    def get_scenario_info(self, scenario: str) -> Dict:
        """è·å–åœºæ™¯ä¿¡æ¯"""
        return self.scenarios[scenario]

    def simulate_quality_improvement(self, scenario: str, action_quality: float) -> float:
        """
        æ¨¡æ‹Ÿåœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„è´¨é‡æ”¹è¿›

        Args:
            scenario: åœºæ™¯ç±»å‹
            action_quality: åŠ¨ä½œè´¨é‡ [0, 1]ï¼Œè¡¨ç¤ºç®—æ³•é€‰æ‹©çš„åŠ¨ä½œå¥½å

        Returns:
            æ”¹è¿›åçš„è´¨é‡åˆ†æ•°
        """
        info = self.scenarios[scenario]
        base = info['base_quality']
        difficulty = info['difficulty']

        # åœ¨å›°éš¾åœºæ™¯ä¸‹ï¼Œç›¸åŒçš„åŠ¨ä½œè´¨é‡äº§ç”Ÿæ›´å°çš„æ”¹è¿›
        max_improvement = 0.3 / difficulty  # å›°éš¾åœºæ™¯æ”¹è¿›ç©ºé—´æ›´å°
        actual_improvement = action_quality * max_improvement

        # æ·»åŠ å™ªå£°
        noise = np.random.normal(0, 0.02)
        new_quality = base + actual_improvement + noise

        return np.clip(new_quality, 0.0, 1.0)

class RewardComparator:
    """å¥–åŠ±ç³»ç»Ÿå¯¹æ¯”å™¨"""

    def __init__(self):
        self.simulator = ScenarioSimulator()

    def absolute_reward(self, prev_quality: float, curr_quality: float) -> float:
        """ä¼ ç»Ÿç»å¯¹å¥–åŠ±"""
        gamma = 0.99
        return gamma * curr_quality - prev_quality

    def relative_reward(self, prev_quality: float, curr_quality: float) -> float:
        """ç®€å•ç›¸å¯¹æ”¹è¿›å¥–åŠ±"""
        if prev_quality > 0.01:
            relative_improvement = (curr_quality - prev_quality) / prev_quality
        else:
            relative_improvement = curr_quality - prev_quality
        return np.clip(relative_improvement, -1.0, 1.0)

    def simulate_training_episode(self, reward_type: str, steps: int = 50) -> Dict:
        """
        æ¨¡æ‹Ÿä¸€ä¸ªè®­ç»ƒepisode

        Args:
            reward_type: 'absolute' æˆ– 'relative'
            steps: episodeæ­¥æ•°

        Returns:
            episodeç»“æœå­—å…¸
        """
        scenario = self.simulator.sample_scenario()
        scenario_info = self.simulator.get_scenario_info(scenario)

        # åˆå§‹è´¨é‡
        current_quality = scenario_info['base_quality'] + np.random.normal(0, 0.05)
        current_quality = np.clip(current_quality, 0.1, 0.9)

        rewards = []
        qualities = [current_quality]
        actions = []

        for step in range(steps):
            # æ¨¡æ‹Ÿæ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œï¼ˆç®€åŒ–ä¸ºéšæœºåŠ¨ä½œè´¨é‡ï¼‰
            action_quality = np.random.beta(2, 2)  # åå‘ä¸­ç­‰è´¨é‡çš„åŠ¨ä½œ
            actions.append(action_quality)

            # è®¡ç®—æ–°çš„è´¨é‡åˆ†æ•°
            prev_quality = current_quality
            current_quality = self.simulator.simulate_quality_improvement(scenario, action_quality)
            qualities.append(current_quality)

            # è®¡ç®—å¥–åŠ±
            if reward_type == 'absolute':
                reward = self.absolute_reward(prev_quality, current_quality)
            else:  # relative
                reward = self.relative_reward(prev_quality, current_quality)

            rewards.append(reward)

        return {
            'scenario': scenario,
            'scenario_difficulty': scenario_info['difficulty'],
            'initial_quality': qualities[0],
            'final_quality': qualities[-1],
            'quality_improvement': qualities[-1] - qualities[0],
            'relative_improvement': (qualities[-1] - qualities[0]) / qualities[0] if qualities[0] > 0 else 0,
            'total_reward': sum(rewards),
            'avg_reward': np.mean(rewards),
            'rewards': rewards,
            'qualities': qualities,
            'actions': actions
        }

    def run_comparison_experiment(self, num_episodes: int = 1000) -> Dict:
        """è¿è¡Œå¯¹æ¯”å®éªŒ"""
        print(f"ğŸ§ª è¿è¡Œå¯¹æ¯”å®éªŒ ({num_episodes} episodes)")

        results = {
            'absolute': [],
            'relative': []
        }

        # è¿è¡Œå®éªŒ
        for reward_type in ['absolute', 'relative']:
            print(f"   æµ‹è¯• {reward_type} å¥–åŠ±...")
            for i in range(num_episodes):
                if (i + 1) % 200 == 0:
                    print(f"     å®Œæˆ {i + 1}/{num_episodes} episodes")

                episode_result = self.simulate_training_episode(reward_type)
                results[reward_type].append(episode_result)

        return results

def analyze_scenario_bias(results: Dict) -> Dict:
    """åˆ†æåœºæ™¯åå‘é—®é¢˜"""
    print("\nğŸ“Š åˆ†æåœºæ™¯åå‘é—®é¢˜")

    analysis = {}

    for reward_type in ['absolute', 'relative']:
        episodes = results[reward_type]

        # ç»Ÿè®¡å„åœºæ™¯çš„è®¿é—®å’Œæ€§èƒ½
        scenario_stats = defaultdict(list)
        scenario_counts = Counter()

        for episode in episodes:
            scenario = episode['scenario']
            scenario_counts[scenario] += 1
            scenario_stats[scenario].append({
                'final_quality': episode['final_quality'],
                'quality_improvement': episode['quality_improvement'],
                'relative_improvement': episode['relative_improvement'],
                'total_reward': episode['total_reward']
            })

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        scenario_analysis = {}
        for scenario, stats in scenario_stats.items():
            scenario_analysis[scenario] = {
                'count': scenario_counts[scenario],
                'avg_final_quality': np.mean([s['final_quality'] for s in stats]),
                'avg_quality_improvement': np.mean([s['quality_improvement'] for s in stats]),
                'avg_relative_improvement': np.mean([s['relative_improvement'] for s in stats]),
                'avg_total_reward': np.mean([s['total_reward'] for s in stats]),
                'std_final_quality': np.std([s['final_quality'] for s in stats])
            }

        analysis[reward_type] = scenario_analysis

    return analysis

def generate_visualizations(results: Dict, analysis: Dict):
    """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
    print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('data/experiments/reward_comparison', exist_ok=True)

    # 1. åœºæ™¯æ€§èƒ½å¯¹æ¯”å›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('å¥–åŠ±ç³»ç»Ÿå¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')

    # 1.1 å„åœºæ™¯æœ€ç»ˆè´¨é‡å¯¹æ¯”
    scenarios = list(analysis['absolute'].keys())
    absolute_qualities = [analysis['absolute'][s]['avg_final_quality'] for s in scenarios]
    relative_qualities = [analysis['relative'][s]['avg_final_quality'] for s in scenarios]

    x = np.arange(len(scenarios))
    width = 0.35

    axes[0,0].bar(x - width/2, absolute_qualities, width, label='ç»å¯¹å¥–åŠ±', alpha=0.8)
    axes[0,0].bar(x + width/2, relative_qualities, width, label='ç›¸å¯¹å¥–åŠ±', alpha=0.8)
    axes[0,0].set_xlabel('åœºæ™¯ç±»å‹')
    axes[0,0].set_ylabel('å¹³å‡æœ€ç»ˆè´¨é‡')
    axes[0,0].set_title('å„åœºæ™¯æœ€ç»ˆè´¨é‡å¯¹æ¯”')
    axes[0,0].set_xticks(x)
    axes[0,0].set_xticklabels(scenarios, rotation=45)
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)

    # 1.2 ç›¸å¯¹æ”¹è¿›å¯¹æ¯”
    absolute_improvements = [analysis['absolute'][s]['avg_relative_improvement'] for s in scenarios]
    relative_improvements = [analysis['relative'][s]['avg_relative_improvement'] for s in scenarios]

    axes[0,1].bar(x - width/2, absolute_improvements, width, label='ç»å¯¹å¥–åŠ±', alpha=0.8)
    axes[0,1].bar(x + width/2, relative_improvements, width, label='ç›¸å¯¹å¥–åŠ±', alpha=0.8)
    axes[0,1].set_xlabel('åœºæ™¯ç±»å‹')
    axes[0,1].set_ylabel('å¹³å‡ç›¸å¯¹æ”¹è¿›ç‡')
    axes[0,1].set_title('å„åœºæ™¯ç›¸å¯¹æ”¹è¿›å¯¹æ¯”')
    axes[0,1].set_xticks(x)
    axes[0,1].set_xticklabels(scenarios, rotation=45)
    axes[0,1].legend()
    axes[0,1].grid(True, alpha=0.3)

    # 1.3 åœºæ™¯è®¿é—®åˆ†å¸ƒ
    absolute_counts = [analysis['absolute'][s]['count'] for s in scenarios]
    relative_counts = [analysis['relative'][s]['count'] for s in scenarios]

    axes[1,0].bar(x - width/2, absolute_counts, width, label='ç»å¯¹å¥–åŠ±', alpha=0.8)
    axes[1,0].bar(x + width/2, relative_counts, width, label='ç›¸å¯¹å¥–åŠ±', alpha=0.8)
    axes[1,0].set_xlabel('åœºæ™¯ç±»å‹')
    axes[1,0].set_ylabel('è®¿é—®æ¬¡æ•°')
    axes[1,0].set_title('åœºæ™¯è®¿é—®åˆ†å¸ƒ')
    axes[1,0].set_xticks(x)
    axes[1,0].set_xticklabels(scenarios, rotation=45)
    axes[1,0].legend()
    axes[1,0].grid(True, alpha=0.3)

    # 1.4 å¥–åŠ±åˆ†å¸ƒå¯¹æ¯”
    absolute_rewards = [episode['total_reward'] for episode in results['absolute']]
    relative_rewards = [episode['total_reward'] for episode in results['relative']]

    axes[1,1].hist(absolute_rewards, bins=30, alpha=0.7, label='ç»å¯¹å¥–åŠ±', density=True)
    axes[1,1].hist(relative_rewards, bins=30, alpha=0.7, label='ç›¸å¯¹å¥–åŠ±', density=True)
    axes[1,1].set_xlabel('æ€»å¥–åŠ±')
    axes[1,1].set_ylabel('å¯†åº¦')
    axes[1,1].set_title('å¥–åŠ±åˆ†å¸ƒå¯¹æ¯”')
    axes[1,1].legend()
    axes[1,1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('data/experiments/reward_comparison/comparison_overview.png', dpi=300, bbox_inches='tight')
    plt.close()

    print("   âœ… ä¿å­˜å¯¹æ¯”æ¦‚è§ˆå›¾: data/experiments/reward_comparison/comparison_overview.png")

def generate_detailed_report(results: Dict, analysis: Dict):
    """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
    print("\nğŸ“„ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š")

    report = {
        'experiment_summary': {
            'total_episodes_per_method': len(results['absolute']),
            'scenarios_tested': list(analysis['absolute'].keys()),
            'timestamp': pd.Timestamp.now().isoformat()
        },
        'key_findings': [],
        'scenario_analysis': analysis,
        'recommendations': []
    }

    # åˆ†æå…³é”®å‘ç°
    for scenario in analysis['absolute'].keys():
        abs_quality = analysis['absolute'][scenario]['avg_final_quality']
        rel_quality = analysis['relative'][scenario]['avg_final_quality']
        improvement = (rel_quality - abs_quality) / abs_quality * 100

        if improvement > 1:  # è¶…è¿‡1%æ”¹è¿›
            report['key_findings'].append(
                f"{scenario}åœºæ™¯: ç›¸å¯¹å¥–åŠ±æ¯”ç»å¯¹å¥–åŠ±æ€§èƒ½æå‡{improvement:.1f}%"
            )
        elif improvement < -1:  # è¶…è¿‡1%é€€åŒ–
            report['key_findings'].append(
                f"{scenario}åœºæ™¯: ç›¸å¯¹å¥–åŠ±æ¯”ç»å¯¹å¥–åŠ±æ€§èƒ½ä¸‹é™{abs(improvement):.1f}%"
            )

    # è®¡ç®—å›°éš¾åœºæ™¯çš„æ•´ä½“è¡¨ç°
    difficult_scenarios = ['severe_fault', 'high_load', 'generation_fluctuation']
    abs_difficult_avg = np.mean([analysis['absolute'][s]['avg_final_quality']
                                for s in difficult_scenarios if s in analysis['absolute']])
    rel_difficult_avg = np.mean([analysis['relative'][s]['avg_final_quality']
                                for s in difficult_scenarios if s in analysis['relative']])

    difficult_improvement = (rel_difficult_avg - abs_difficult_avg) / abs_difficult_avg * 100

    if difficult_improvement > 0:
        report['key_findings'].append(
            f"å›°éš¾åœºæ™¯æ•´ä½“: ç›¸å¯¹å¥–åŠ±æå‡{difficult_improvement:.1f}%"
        )
        report['recommendations'].append("å»ºè®®é‡‡ç”¨ç®€å•ç›¸å¯¹å¥–åŠ±ç³»ç»Ÿ")
    else:
        report['key_findings'].append(
            f"å›°éš¾åœºæ™¯æ•´ä½“: ç›¸å¯¹å¥–åŠ±ä¸‹é™{abs(difficult_improvement):.1f}%"
        )
        report['recommendations'].append("éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ç›¸å¯¹å¥–åŠ±ç®—æ³•")

    # ä¿å­˜æŠ¥å‘Š
    with open('data/experiments/reward_comparison/detailed_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print("   âœ… ä¿å­˜è¯¦ç»†æŠ¥å‘Š: data/experiments/reward_comparison/detailed_report.json")

    return report

def print_summary(report: Dict):
    """æ‰“å°æ€»ç»“"""
    print("\n" + "="*60)
    print("ğŸ¯ å®éªŒæ€»ç»“")
    print("="*60)

    print(f"ğŸ“Š å®éªŒè§„æ¨¡: æ¯ç§æ–¹æ³•æµ‹è¯• {report['experiment_summary']['total_episodes_per_method']} episodes")
    print(f"ğŸ­ æµ‹è¯•åœºæ™¯: {', '.join(report['experiment_summary']['scenarios_tested'])}")

    print("\nğŸ” å…³é”®å‘ç°:")
    for finding in report['key_findings']:
        print(f"   â€¢ {finding}")

    print("\nğŸ’¡ å»ºè®®:")
    for rec in report['recommendations']:
        print(f"   â€¢ {rec}")

    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print("   â€¢ data/experiments/reward_comparison/comparison_overview.png")
    print("   â€¢ data/experiments/reward_comparison/detailed_report.json")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¥–åŠ±ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•")
    print("è§£å†³è·¨åœºæ™¯è®­ç»ƒåå‘é—®é¢˜")
    print("="*60)

    # 1. è¿è¡Œå¯¹æ¯”å®éªŒ
    comparator = RewardComparator()
    results = comparator.run_comparison_experiment(num_episodes=1000)

    # 2. åˆ†æç»“æœ
    analysis = analyze_scenario_bias(results)

    # 3. ç”Ÿæˆå¯è§†åŒ–
    generate_visualizations(results, analysis)

    # 4. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report = generate_detailed_report(results, analysis)

    # 5. æ‰“å°æ€»ç»“
    print_summary(report)

    print("\nâœ¨ æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main()