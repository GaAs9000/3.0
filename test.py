#!/usr/bin/env python3
"""
ç”µåŠ›ç½‘ç»œåˆ†åŒºæ™ºèƒ½ä½“ç°ä»£åŒ–æµ‹è¯•ç³»ç»Ÿ - ä¸»å…¥å£

åŸºäºpytestçš„ç°ä»£åŒ–æµ‹è¯•æ¡†æ¶ï¼Œæ”¯æŒï¼š
- å•å…ƒæµ‹è¯• (Unit Tests)
- é›†æˆæµ‹è¯• (Integration Tests)
- æ€§èƒ½æµ‹è¯• (Performance Tests)
- ç«¯åˆ°ç«¯æµ‹è¯• (E2E Tests)
- æµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Š
- CI/CDé›†æˆ
- è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ
- å¯è§†åŒ–åˆ†æ
"""

import argparse
import sys
import time
import subprocess
import json
import numpy as np
import torch
from pathlib import Path
from typing import Optional, List, Dict, Any

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent / 'code' / 'src'))
sys.path.append(str(Path(__file__).parent / 'code'))
sys.path.append(str(Path(__file__).parent / 'code' / 'test'))

try:
    from comprehensive_evaluator import ComprehensiveAgentEvaluator
except ImportError:
    print("âš ï¸ è­¦å‘Š: æ— æ³•å¯¼å…¥comprehensive_evaluatorï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
    ComprehensiveAgentEvaluator = None


class RealTimePerformanceTester:
    """å®æ—¶æ€§èƒ½æµ‹è¯•å™¨ - Agent vs Baselineç®—æ³•æ‰§è¡Œæ—¶é—´å¯¹æ¯”æµ‹è¯•"""

    def __init__(self):
        self.project_root = Path(__file__).parent
        self.results_dir = self.project_root / 'performance_results'
        self.results_dir.mkdir(exist_ok=True)

        # æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥baselineæ¨¡å—
        sys.path.append(str(self.project_root / 'code'))
        sys.path.append(str(self.project_root / 'code' / 'baseline'))

    def run_engineering_performance_test(self, model_path: str, network: str = 'ieee57', num_runs: int = 100) -> Dict[str, Any]:
        """è¿è¡Œå·¥ç¨‹çº§æ€§èƒ½æµ‹è¯• - åŒ…å«Agent vs Baselineæ€§èƒ½å¯¹æ¯”"""
        print(f"ğŸ­ å·¥ç¨‹çº§Agent vs Baselineå®æ—¶æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        print(f"   ç½‘ç»œ: {network.upper()}")
        print(f"   æ¨¡å‹: {model_path}")
        print(f"   è¿è¡Œæ¬¡æ•°: {num_runs}")
        print("=" * 60)

        try:
            # 1. è¿è¡ŒAgentæ€§èƒ½æµ‹è¯•
            print("ğŸ¤– ç¬¬ä¸€é˜¶æ®µ: Agentæ€§èƒ½æµ‹è¯•")
            agent_results = self._run_agent_performance_test(model_path, network, num_runs)

            if not agent_results['success']:
                return agent_results

            # 2. è¿è¡ŒBaselineç®—æ³•æ€§èƒ½æµ‹è¯•
            print("\nğŸ“Š ç¬¬äºŒé˜¶æ®µ: Baselineç®—æ³•æ€§èƒ½æµ‹è¯•")
            baseline_results = self._run_baseline_performance_test(network, num_runs)

            # 3. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
            print("\nğŸ“ˆ ç¬¬ä¸‰é˜¶æ®µ: ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
            comparison_results = self._generate_performance_comparison(
                agent_results, baseline_results, model_path, network, num_runs
            )

            # 4. ä¿å­˜ç»“æœ
            self._save_comparison_results(comparison_results, network)

            return comparison_results

        except Exception as e:
            print(f"âŒ æ€§èƒ½å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _run_agent_performance_test(self, model_path: str, network: str, num_runs: int) -> Dict[str, Any]:
        """è¿è¡ŒAgentæ€§èƒ½æµ‹è¯• - ç›´æ¥åœ¨å†…éƒ¨å®ç°é¿å…å¤–éƒ¨è„šæœ¬è°ƒç”¨é—®é¢˜"""
        try:
            print("ğŸš€ å¯åŠ¨Agentæ€§èƒ½æµ‹è¯•...")

            # å¯¼å…¥å¿…è¦çš„æ¨¡å—
            from train import load_power_grid_data, create_environment_from_config
            from data_processing import PowerGridDataProcessor
            from gat import create_production_encoder
            from rl.agent import PPOAgent
            import yaml
            import psutil
            import os

            # åŠ è½½é…ç½®
            config_path = self.project_root / 'config.yaml'
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
            else:
                config = {
                    'system': {'device': 'cpu', 'seed': 42},
                    'data': {'normalize': True},
                    'environment': {
                        'num_partitions': 4,
                        'max_steps': 200,
                        'reward_weights': {'balance': 1.0, 'connectivity': 0.5, 'size': 0.3}
                    },
                    'gat': {'hidden_dim': 64, 'num_heads': 4, 'num_layers': 2},
                    'agent': {'learning_rate': 3e-4, 'gamma': 0.99}
                }

            print("   åˆå§‹åŒ–Agentæµ‹è¯•ç¯å¢ƒ...")

            # åŠ è½½ç½‘ç»œæ•°æ®å¹¶åˆ›å»ºç¯å¢ƒ
            mpc_data = load_power_grid_data(network)
            processor = PowerGridDataProcessor()
            hetero_data = processor.graph_from_mpc(mpc_data, config=config)
            encoder = create_production_encoder(hetero_data, config.get('gat', {}))
            hetero_data = hetero_data.to(torch.device('cpu'))

            with torch.no_grad():
                node_embeddings_dict, attention_weights = encoder.encode_nodes_with_attention(hetero_data, config)

            env, _ = create_environment_from_config(
                config, hetero_data, node_embeddings_dict, attention_weights, mpc_data,
                torch.device('cpu'), is_normalized=config['data'].get('normalize', True)
            )

            # è·å–ç¯å¢ƒä¿¡æ¯
            init_state, init_info = env.reset()
            node_dim = init_state['node_embeddings'].shape[1]
            region_dim = init_state['region_embeddings'].shape[1]
            num_partitions = init_state['region_embeddings'].shape[0]
            num_nodes = init_state['node_embeddings'].shape[0]

            print(f"   ç¯å¢ƒè§„æ¨¡: {num_nodes}èŠ‚ç‚¹, {num_partitions}åˆ†åŒº")

            # åˆ›å»ºæ™ºèƒ½ä½“
            agent = PPOAgent(
                node_embedding_dim=node_dim,
                region_embedding_dim=region_dim,
                num_partitions=num_partitions,
                agent_config=config.get('agent', {}),
                device=torch.device('cpu')
            )

            # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            print("   åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
            checkpoint = torch.load(model_path, map_location='cpu')

            if 'actor_state_dict' not in checkpoint:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼: {model_path}")

            # åŠ è½½æ¨¡å‹çŠ¶æ€
            agent.actor.load_state_dict(checkpoint['actor_state_dict'])
            agent.critic.load_state_dict(checkpoint['critic_state_dict'])
            agent.actor.eval()
            agent.critic.eval()

            print("   å¼€å§‹Agentæ€§èƒ½æµ‹è¯•...")

            # ç³»ç»Ÿé¢„çƒ­
            for i in range(10):
                state, _ = env.reset()
                action, _, _ = agent.select_action(state, training=False)
                if action is not None:
                    env.step(action)

            # æ€§èƒ½æµ‹è¯•
            agent_times = []
            process = psutil.Process(os.getpid())
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB

            successful_runs = 0
            for i in range(num_runs):
                try:
                    state, _ = env.reset()

                    # æµ‹é‡Agentå†³ç­–æ—¶é—´
                    start_time = time.perf_counter()
                    action, _, _ = agent.select_action(state, training=False)
                    end_time = time.perf_counter()

                    if action is not None:
                        execution_time = (end_time - start_time) * 1000  # æ¯«ç§’
                        agent_times.append(execution_time)
                        successful_runs += 1

                except Exception as e:
                    print(f"       ç¬¬{i+1}æ¬¡è¿è¡Œå¤±è´¥: {e}")
                    continue

            if not agent_times:
                return {'success': False, 'error': 'Agentæµ‹è¯•å…¨éƒ¨å¤±è´¥'}

            # ç»Ÿè®¡åˆ†æ
            agent_times = np.array(agent_times)
            current_memory = process.memory_info().rss / 1024 / 1024

            results = {
                'success': True,
                'data': {
                    'test_info': {
                        'network': network,
                        'model_path': model_path,
                        'num_runs': num_runs,
                        'successful_runs': successful_runs,
                        'num_nodes': num_nodes,
                        'num_partitions': num_partitions,
                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                    },
                    'performance_metrics': {
                        'agent_decision': {
                            'mean_ms': float(np.mean(agent_times)),
                            'std_ms': float(np.std(agent_times)),
                            'min_ms': float(np.min(agent_times)),
                            'max_ms': float(np.max(agent_times)),
                            'p50_ms': float(np.percentile(agent_times, 50)),
                            'p95_ms': float(np.percentile(agent_times, 95)),
                            'p99_ms': float(np.percentile(agent_times, 99)),
                            'p999_ms': float(np.percentile(agent_times, 99.9))
                        },
                        'memory': {
                            'increase_mb': float(current_memory - initial_memory)
                        }
                    }
                }
            }

            print(f"   âœ… Agentæµ‹è¯•å®Œæˆ: å¹³å‡ {np.mean(agent_times):.2f}ms")
            return results

        except Exception as e:
            print(f"âŒ Agentæ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _get_latest_performance_result(self, network: str) -> Dict[str, Any]:
        """è·å–æœ€æ–°çš„æ€§èƒ½æµ‹è¯•ç»“æœ"""
        try:
            import glob
            import json

            # æŸ¥æ‰¾æœ€æ–°çš„ç»“æœæ–‡ä»¶
            pattern = str(self.results_dir / f"engineering_benchmark_{network}_*.json")
            files = glob.glob(pattern)

            if not files:
                return None

            # è·å–æœ€æ–°æ–‡ä»¶
            latest_file = max(files, key=lambda x: Path(x).stat().st_mtime)

            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)

        except Exception as e:
            print(f"âš ï¸ æ— æ³•è¯»å–Agentæ€§èƒ½ç»“æœ: {e}")
            return None



    def _run_baseline_performance_test(self, network: str, num_runs: int) -> Dict[str, Any]:
        """è¿è¡ŒBaselineç®—æ³•æ€§èƒ½æµ‹è¯•"""
        try:
            # å¯¼å…¥å¿…è¦çš„æ¨¡å—
            from train import load_power_grid_data, create_environment_from_config
            from data_processing import PowerGridDataProcessor
            from gat import create_production_encoder
            from baseline import SpectralPartitioner, KMeansPartitioner
            import yaml
            import time
            import psutil
            import os

            print("   åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ...")

            # åŠ è½½é…ç½®
            config_path = self.project_root / 'config.yaml'
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
            else:
                config = {
                    'system': {'device': 'cpu', 'seed': 42},
                    'data': {'normalize': True},
                    'environment': {
                        'num_partitions': 4,
                        'max_steps': 200,
                        'reward_weights': {'balance': 1.0, 'connectivity': 0.5, 'size': 0.3}
                    },
                    'gat': {'hidden_dim': 64, 'num_heads': 4, 'num_layers': 2}
                }

            # åŠ è½½ç½‘ç»œæ•°æ®å¹¶åˆ›å»ºç¯å¢ƒ
            mpc_data = load_power_grid_data(network)
            processor = PowerGridDataProcessor()
            hetero_data = processor.graph_from_mpc(mpc_data, config=config)
            encoder = create_production_encoder(hetero_data, config.get('gat', {}))
            hetero_data = hetero_data.to(torch.device('cpu'))

            with torch.no_grad():
                node_embeddings_dict, attention_weights = encoder.encode_nodes_with_attention(hetero_data, config)

            env, _ = create_environment_from_config(
                config, hetero_data, node_embeddings_dict, attention_weights, mpc_data,
                torch.device('cpu'), is_normalized=config['data'].get('normalize', True)
            )

            print("   å¼€å§‹Baselineç®—æ³•æ€§èƒ½æµ‹è¯•...")

            # æµ‹è¯•ä¸åŒçš„baselineç®—æ³•
            baseline_methods = {
                'Spectral Clustering': SpectralPartitioner(seed=42),
                'K-means Clustering': KMeansPartitioner(seed=42)
            }

            baseline_results = {}
            process = psutil.Process(os.getpid())

            for method_name, partitioner in baseline_methods.items():
                print(f"     æµ‹è¯• {method_name}...")

                method_times = []
                memory_usage = []
                initial_memory = process.memory_info().rss / 1024 / 1024  # MB

                successful_runs = 0
                for i in range(num_runs):
                    try:
                        # é‡ç½®ç¯å¢ƒ
                        env.reset()

                        # æµ‹é‡ç®—æ³•æ‰§è¡Œæ—¶é—´
                        start_time = time.perf_counter()
                        partition_result = partitioner.partition(env)
                        end_time = time.perf_counter()

                        # è®°å½•æ—¶é—´ (æ¯«ç§’)
                        execution_time = (end_time - start_time) * 1000
                        method_times.append(execution_time)
                        successful_runs += 1

                        # æ¯50æ¬¡æµ‹é‡å†…å­˜ä½¿ç”¨
                        if i % 50 == 0:
                            current_memory = process.memory_info().rss / 1024 / 1024
                            memory_usage.append(current_memory - initial_memory)

                    except Exception as e:
                        print(f"       ç¬¬{i+1}æ¬¡è¿è¡Œå¤±è´¥: {e}")
                        continue

                if method_times:
                    method_times = np.array(method_times)
                    baseline_results[method_name] = {
                        'successful_runs': successful_runs,
                        'mean_ms': float(np.mean(method_times)),
                        'std_ms': float(np.std(method_times)),
                        'min_ms': float(np.min(method_times)),
                        'max_ms': float(np.max(method_times)),
                        'p50_ms': float(np.percentile(method_times, 50)),
                        'p95_ms': float(np.percentile(method_times, 95)),
                        'p99_ms': float(np.percentile(method_times, 99)),
                        'peak_memory_mb': float(np.max(memory_usage)) if memory_usage else 0,
                        'raw_times': method_times.tolist()
                    }
                    print(f"       âœ… {method_name}: å¹³å‡ {np.mean(method_times):.2f}ms")
                else:
                    baseline_results[method_name] = {
                        'successful_runs': 0,
                        'error': 'All runs failed'
                    }
                    print(f"       âŒ {method_name}: æ‰€æœ‰è¿è¡Œéƒ½å¤±è´¥")

            return {
                'success': True,
                'data': {
                    'network': network,
                    'num_runs': num_runs,
                    'methods': baseline_results,
                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                }
            }

        except Exception as e:
            print(f"   âŒ Baselineæ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _generate_performance_comparison(self, agent_results: Dict[str, Any], baseline_results: Dict[str, Any],
                                       model_path: str, network: str, num_runs: int) -> Dict[str, Any]:
        """ç”ŸæˆAgent vs Baselineæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š"""
        try:
            print("   åˆ†ææ€§èƒ½å¯¹æ¯”æ•°æ®...")

            comparison = {
                'success': True,
                'test_info': {
                    'network': network,
                    'model_path': model_path,
                    'num_runs': num_runs,
                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                },
                'agent_performance': {},
                'baseline_performance': {},
                'comparison_analysis': {},
                'summary': {}
            }

            # æå–Agentæ€§èƒ½æ•°æ®
            if agent_results.get('success') and 'data' in agent_results:
                agent_data = agent_results['data']
                if 'performance_metrics' in agent_data:
                    agent_perf = agent_data['performance_metrics']['agent_decision']
                    comparison['agent_performance'] = {
                        'method': 'RL Agent (PPO)',
                        'mean_ms': agent_perf['mean_ms'],
                        'std_ms': agent_perf['std_ms'],
                        'p50_ms': agent_perf['p50_ms'],
                        'p95_ms': agent_perf['p95_ms'],
                        'p99_ms': agent_perf['p99_ms'],
                        'max_ms': agent_perf['max_ms']
                    }

            # æå–Baselineæ€§èƒ½æ•°æ®
            if baseline_results.get('success') and 'data' in baseline_results:
                comparison['baseline_performance'] = baseline_results['data']['methods']

            # ç”Ÿæˆå¯¹æ¯”åˆ†æ
            if comparison['agent_performance'] and comparison['baseline_performance']:
                comparison['comparison_analysis'] = self._analyze_performance_comparison(
                    comparison['agent_performance'], comparison['baseline_performance']
                )

                # ç”Ÿæˆæ€»ç»“
                comparison['summary'] = self._generate_performance_summary(comparison['comparison_analysis'])

            # æ‰“å°å¯¹æ¯”ç»“æœ
            self._print_performance_comparison(comparison)

            return comparison

        except Exception as e:
            print(f"   âŒ æ€§èƒ½å¯¹æ¯”åˆ†æå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'agent_results': agent_results,
                'baseline_results': baseline_results
            }

    def _analyze_performance_comparison(self, agent_perf: Dict[str, Any], baseline_perf: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½å¯¹æ¯”æ•°æ®"""
        analysis = {}

        agent_mean = agent_perf['mean_ms']
        agent_p99 = agent_perf['p99_ms']

        for method_name, method_data in baseline_perf.items():
            if 'mean_ms' not in method_data:
                continue

            baseline_mean = method_data['mean_ms']
            baseline_p99 = method_data['p99_ms']

            # è®¡ç®—æ€§èƒ½æ¯”è¾ƒ
            mean_speedup = baseline_mean / agent_mean if agent_mean > 0 else float('inf')
            p99_speedup = baseline_p99 / agent_p99 if agent_p99 > 0 else float('inf')

            analysis[method_name] = {
                'baseline_mean_ms': baseline_mean,
                'baseline_p99_ms': baseline_p99,
                'mean_speedup_ratio': mean_speedup,  # >1 è¡¨ç¤ºAgentæ›´å¿«
                'p99_speedup_ratio': p99_speedup,
                'mean_performance': 'Agentæ›´å¿«' if mean_speedup > 1 else 'Baselineæ›´å¿«',
                'p99_performance': 'Agentæ›´å¿«' if p99_speedup > 1 else 'Baselineæ›´å¿«',
                'mean_improvement_pct': (mean_speedup - 1) * 100 if mean_speedup > 1 else (1 - mean_speedup) * -100,
                'p99_improvement_pct': (p99_speedup - 1) * 100 if p99_speedup > 1 else (1 - p99_speedup) * -100
            }

        return analysis

    def _generate_performance_summary(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æ€»ç»“"""
        if not analysis:
            return {'status': 'No valid comparison data'}

        # ç»Ÿè®¡Agentç›¸å¯¹äºå„baselineçš„è¡¨ç°
        faster_count = sum(1 for data in analysis.values() if data['mean_speedup_ratio'] > 1)
        total_methods = len(analysis)

        avg_speedup = np.mean([data['mean_speedup_ratio'] for data in analysis.values()])
        best_speedup = max([data['mean_speedup_ratio'] for data in analysis.values()])
        worst_speedup = min([data['mean_speedup_ratio'] for data in analysis.values()])

        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡Spectral Clusteringï¼ˆå…³é”®æŒ‡æ ‡ï¼‰
        spectral_speedup = analysis.get('Spectral Clustering', {}).get('mean_speedup_ratio', 0)
        beats_spectral = spectral_speedup > 1

        return {
            'agent_faster_than_baselines': f"{faster_count}/{total_methods}",
            'average_speedup_ratio': avg_speedup,
            'best_speedup_ratio': best_speedup,
            'worst_speedup_ratio': worst_speedup,
            'beats_spectral_clustering': beats_spectral,
            'spectral_speedup_ratio': spectral_speedup,
            'overall_performance': self._evaluate_overall_performance(beats_spectral, spectral_speedup, avg_speedup),
            'recommendation': self._get_performance_recommendation(beats_spectral, spectral_speedup, faster_count, total_methods)
        }

    def _evaluate_overall_performance(self, beats_spectral: bool, spectral_speedup: float, avg_speedup: float) -> str:
        """è¯„ä¼°æ•´ä½“æ€§èƒ½ - åªæœ‰è¶…è¿‡Spectral Clusteringæ‰ç®—ä¼˜ç§€"""
        if beats_spectral:
            if spectral_speedup > 2.0:
                return 'Agentè¡¨ç°ä¼˜ç§€'
            elif spectral_speedup > 1.5:
                return 'Agentè¡¨ç°è‰¯å¥½'
            else:
                return 'Agentè¡¨ç°ä¸€èˆ¬'
        else:
            return 'Agentéœ€è¦ä¼˜åŒ–'

    def _get_performance_recommendation(self, beats_spectral: bool, spectral_speedup: float, faster_count: int, total_methods: int) -> str:
        """è·å–æ€§èƒ½å»ºè®® - åŸºäºæ˜¯å¦è¶…è¿‡Spectral Clustering"""
        if beats_spectral:
            if spectral_speedup > 3.0:
                return "ğŸŸ¢ Agentæ€§èƒ½ä¼˜ç§€ï¼Œæ˜¾è‘—è¶…è¿‡Spectral Clusteringï¼Œé€‚åˆå®æ—¶éƒ¨ç½²"
            elif spectral_speedup > 2.0:
                return "ğŸŸ¡ Agentæ€§èƒ½è‰¯å¥½ï¼Œè¶…è¿‡Spectral Clusteringï¼Œå¯è€ƒè™‘å®æ—¶éƒ¨ç½²"
            elif spectral_speedup > 1.5:
                return "ğŸŸ  Agentæ€§èƒ½ä¸€èˆ¬ï¼Œç•¥è¶…è¿‡Spectral Clusteringï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–"
            else:
                return "ğŸŸ  Agentå‹‰å¼ºè¶…è¿‡Spectral Clusteringï¼Œå»ºè®®ä¼˜åŒ–ä»¥è·å¾—æ›´å¤§ä¼˜åŠ¿"
        else:
            return "ğŸ”´ Agentæœªèƒ½è¶…è¿‡Spectral Clusteringï¼Œéœ€è¦æ˜¾è‘—æ”¹è¿›æ‰èƒ½ç”¨äºå®æ—¶éƒ¨ç½²"

    def generate_test_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report_path = self.results_dir / 'test_report.json'

        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'results': results,
            'summary': {
                'total_tests': len(results),
                'passed': sum(1 for r in results.values() if r.get('status') == 'success'),
                'failed': sum(1 for r in results.values() if r.get('status') == 'failed'),
                'errors': sum(1 for r in results.values() if r.get('status') == 'error'),
                'timeouts': sum(1 for r in results.values() if r.get('status') == 'timeout')
            }
        }

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return str(report_path)

    def _print_performance_comparison(self, comparison: Dict[str, Any]):
        """æ‰“å°æ€§èƒ½å¯¹æ¯”ç»“æœ"""
        if not comparison.get('success'):
            print("âŒ æ— æ³•ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
            return

        print(f"\nğŸ† Agent vs Baseline æ€§èƒ½å¯¹æ¯”ç»“æœ")
        print("=" * 80)

        info = comparison['test_info']
        print(f"ç½‘ç»œ: {info['network'].upper()}")
        print(f"æµ‹è¯•æ¬¡æ•°: {info['num_runs']}")
        print(f"æ—¶é—´: {info['timestamp']}")
        print()

        # Agentæ€§èƒ½
        if comparison['agent_performance']:
            agent = comparison['agent_performance']
            print(f"ğŸ¤– RL Agent (PPO) æ€§èƒ½:")
            print(f"   å¹³å‡æ—¶é—´: {agent['mean_ms']:.2f} Â± {agent['std_ms']:.2f} ms")
            print(f"   ä¸­ä½æ•°: {agent['p50_ms']:.2f} ms")
            print(f"   95%åˆ†ä½: {agent['p95_ms']:.2f} ms")
            print(f"   99%åˆ†ä½: {agent['p99_ms']:.2f} ms")
            print()

        # Baselineæ€§èƒ½
        if comparison['baseline_performance']:
            print(f"ğŸ“Š Baselineç®—æ³•æ€§èƒ½:")
            for method_name, method_data in comparison['baseline_performance'].items():
                if 'mean_ms' in method_data:
                    print(f"   {method_name}:")
                    print(f"     å¹³å‡æ—¶é—´: {method_data['mean_ms']:.2f} Â± {method_data['std_ms']:.2f} ms")
                    print(f"     99%åˆ†ä½: {method_data['p99_ms']:.2f} ms")
                    print(f"     æˆåŠŸç‡: {method_data['successful_runs']}/{info['num_runs']}")
                else:
                    print(f"   {method_name}: âŒ æµ‹è¯•å¤±è´¥")
            print()

        # å¯¹æ¯”åˆ†æ
        if comparison['comparison_analysis']:
            print(f"ğŸ“ˆ æ€§èƒ½å¯¹æ¯”åˆ†æ:")
            agent_perf = comparison['agent_performance']

            for method_name, analysis in comparison['comparison_analysis'].items():
                speedup = analysis['mean_speedup_ratio']
                improvement = analysis['mean_improvement_pct']

                if speedup > 1:
                    status = "ğŸŸ¢ Agentæ›´å¿«"
                    desc = f"å¿« {speedup:.1f}x ({improvement:+.1f}%)"
                else:
                    status = "ğŸ”´ Baselineæ›´å¿«"
                    desc = f"æ…¢ {1/speedup:.1f}x ({improvement:+.1f}%)"

                print(f"   vs {method_name}: {status}")
                print(f"     Agent: {agent_perf['mean_ms']:.2f}ms vs Baseline: {analysis['baseline_mean_ms']:.2f}ms")
                print(f"     æ€§èƒ½å·®å¼‚: {desc}")
            print()

        # æ€»ç»“
        if comparison['summary']:
            summary = comparison['summary']
            print(f"ğŸ¯ æ€»ä½“è¯„ä¼°:")
            print(f"   Agentä¼˜äºBaseline: {summary['agent_faster_than_baselines']}")
            print(f"   å¹³å‡åŠ é€Ÿæ¯”: {summary['average_speedup_ratio']:.2f}x")
            print(f"   æœ€ä½³åŠ é€Ÿæ¯”: {summary['best_speedup_ratio']:.2f}x")

            # çªå‡ºæ˜¾ç¤ºä¸Spectral Clusteringçš„å¯¹æ¯”ï¼ˆå…³é”®æŒ‡æ ‡ï¼‰
            if 'beats_spectral_clustering' in summary:
                spectral_status = "âœ… æ˜¯" if summary['beats_spectral_clustering'] else "âŒ å¦"
                spectral_ratio = summary.get('spectral_speedup_ratio', 0)
                print(f"   ğŸ”‘ è¶…è¿‡Spectral Clustering: {spectral_status} ({spectral_ratio:.2f}x)")

            print(f"   æ•´ä½“è¡¨ç°: {summary['overall_performance']}")
            print(f"   å»ºè®®: {summary['recommendation']}")

    def _save_comparison_results(self, comparison: Dict[str, Any], network: str):
        """ä¿å­˜æ€§èƒ½å¯¹æ¯”ç»“æœ"""
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        filename = f"agent_vs_baseline_comparison_{network}_{timestamp}.json"
        filepath = self.results_dir / filename

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(comparison, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ æ€§èƒ½å¯¹æ¯”ç»“æœå·²ä¿å­˜: {filepath}")

        # åŒæ—¶ä¿å­˜ç®€åŒ–çš„CSVæ ¼å¼ä¾¿äºåˆ†æ
        if comparison.get('success') and comparison.get('comparison_analysis'):
            csv_filename = f"performance_summary_{network}_{timestamp}.csv"
            csv_filepath = self.results_dir / csv_filename

            try:
                import pandas as pd

                # å‡†å¤‡CSVæ•°æ®
                csv_data = []
                agent_perf = comparison['agent_performance']

                # Agentæ•°æ®
                csv_data.append({
                    'Method': 'RL Agent (PPO)',
                    'Mean_ms': agent_perf['mean_ms'],
                    'Std_ms': agent_perf['std_ms'],
                    'P99_ms': agent_perf['p99_ms'],
                    'Speedup_vs_Agent': 1.0,
                    'Performance_Status': 'Reference'
                })

                # Baselineæ•°æ®
                for method_name, analysis in comparison['comparison_analysis'].items():
                    csv_data.append({
                        'Method': method_name,
                        'Mean_ms': analysis['baseline_mean_ms'],
                        'Std_ms': comparison['baseline_performance'][method_name].get('std_ms', 0),
                        'P99_ms': analysis['baseline_p99_ms'],
                        'Speedup_vs_Agent': 1.0 / analysis['mean_speedup_ratio'],  # Baselineç›¸å¯¹äºAgentçš„é€Ÿåº¦
                        'Performance_Status': analysis['mean_performance']
                    })

                df = pd.DataFrame(csv_data)
                df.to_csv(csv_filepath, index=False)
                print(f"ğŸ“Š CSVæ‘˜è¦å·²ä¿å­˜: {csv_filepath}")

            except ImportError:
                print("âš ï¸ pandasæœªå®‰è£…ï¼Œè·³è¿‡CSVæ ¼å¼ä¿å­˜")
            except Exception as e:
                print(f"âš ï¸ CSVä¿å­˜å¤±è´¥: {e}")



def run_baseline_comparison(config_path: Optional[str] = None, network: str = 'ieee57', mode: str = 'standard', model_path: Optional[str] = None, custom_scenarios: Optional[List[str]] = None, custom_runs: Optional[int] = None):
    """è¿è¡Œbaselineå¯¹æ¯”æµ‹è¯•"""
    print(f"ğŸ” å¯åŠ¨Baselineå¯¹æ¯”æµ‹è¯•")
    print(f"   æµ‹è¯•ç½‘ç»œ: {network.upper()}")
    print(f"   è¯„ä¼°æ¨¡å¼: {mode}")
    if model_path:
        print(f"   é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
    print("=" * 50)

    if ComprehensiveAgentEvaluator is None:
        print("âŒ é”™è¯¯: ComprehensiveAgentEvaluatorä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ä¾èµ–")
        return False

    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = ComprehensiveAgentEvaluator(config_path, model_path)

        # è¿è¡Œå¯¹æ¯”æµ‹è¯•
        results = evaluator.run_baseline_comparison(network, custom_scenarios, custom_runs)

        if results['success']:
            print(f"\nâœ… Baselineå¯¹æ¯”æµ‹è¯•å®Œæˆï¼")

            # ç”Ÿæˆå¯è§†åŒ–
            evaluator.create_comparison_visualization(results['results'])

            # ç”ŸæˆæŠ¥å‘Š
            evaluator.generate_evaluation_report(comparison_results=results)

            # æ‰“å°ç®€è¦æ€»ç»“
            summary = results['summary']
            avg_improvement = summary['overall_improvement']['average_improvement']
            print(f"\nğŸ† æµ‹è¯•æ€»ç»“:")
            print(f"   å¹³å‡æ€§èƒ½æå‡: {avg_improvement:.1f}%")

            for scenario, scenario_summary in summary['best_scenarios'].items():
                improvement = scenario_summary['improvement_pct']
                status = "ğŸ‰ ä¼˜ç§€" if improvement > 15 else "âœ… è‰¯å¥½" if improvement > 5 else "âš ï¸ æœ‰é™"
                print(f"   {scenario}: {improvement:.1f}% - {status}")

            return True
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return False

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def run_generalization_test(config_path: Optional[str] = None, train_network: str = 'ieee14', mode: str = 'standard', model_path: Optional[str] = None, custom_runs: Optional[int] = None):
    """è¿è¡Œæ³›åŒ–èƒ½åŠ›æµ‹è¯•"""
    print(f"ğŸŒ å¯åŠ¨æ³›åŒ–èƒ½åŠ›æµ‹è¯•")
    print(f"   è®­ç»ƒç½‘ç»œ: {train_network.upper()}")
    print(f"   è¯„ä¼°æ¨¡å¼: {mode}")
    if model_path:
        print(f"   é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
    print("=" * 50)

    if ComprehensiveAgentEvaluator is None:
        print("âŒ é”™è¯¯: ComprehensiveAgentEvaluatorä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ä¾èµ–")
        return False

    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = ComprehensiveAgentEvaluator(config_path, model_path)

        # è¿è¡Œæ³›åŒ–æµ‹è¯•
        results = evaluator.run_generalization_test(train_network, custom_runs=custom_runs)

        if results['success']:
            print(f"\nâœ… æ³›åŒ–èƒ½åŠ›æµ‹è¯•å®Œæˆï¼")

            # ç”Ÿæˆå¯è§†åŒ–
            evaluator.create_generalization_visualization(results['results'], train_network)

            # ç”ŸæˆæŠ¥å‘Š
            evaluator.generate_evaluation_report(generalization_results=results)

            # æ‰“å°ç®€è¦æ€»ç»“
            summary = results['summary']
            overall = summary['overall_generalization']
            avg_degradation = overall['average_degradation']
            status = overall['status']

            print(f"\nğŸ† æµ‹è¯•æ€»ç»“:")
            print(f"   è®­ç»ƒç½‘ç»œæ€§èƒ½: {summary['train_score']:.3f}")
            print(f"   å¹³å‡æ€§èƒ½ä¸‹é™: {avg_degradation:.1f}%")
            print(f"   æ³›åŒ–è¯„çº§: {status}")

            for network, result in summary['test_results'].items():
                print(f"   {network.upper()}: {result['score']:.3f} ({result['degradation_pct']:.1f}% ä¸‹é™) - {result['status']}")

            return True
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return False

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def run_comprehensive_evaluation(config_path: Optional[str] = None, mode: str = 'standard', model_path: Optional[str] = None):
    """è¿è¡Œå®Œæ•´è¯„ä¼°ï¼ˆåŒ…å«baselineå¯¹æ¯”å’Œæ³›åŒ–æµ‹è¯•ï¼‰"""
    print(f"ğŸ¯ å¯åŠ¨å®Œæ•´è¯„ä¼°")
    print(f"   è¯„ä¼°æ¨¡å¼: {mode}")
    if model_path:
        print(f"   é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
    else:
        print("âŒ é”™è¯¯ï¼šå¿…é¡»æä¾›é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
        print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•: python test.py --comprehensive --model <æ¨¡å‹è·¯å¾„>")
        return False
    print("=" * 50)

    if ComprehensiveAgentEvaluator is None:
        print("âŒ é”™è¯¯: ComprehensiveAgentEvaluatorä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ä¾èµ–")
        return False

    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = ComprehensiveAgentEvaluator(config_path, model_path)

        # è·å–è¯„ä¼°é…ç½®
        eval_config = evaluator.get_evaluation_config(mode)

        comparison_results = None
        generalization_results = None

        # 1. Baselineå¯¹æ¯”æµ‹è¯•
        if eval_config['baseline_comparison']['enabled']:
            print(f"\nğŸ“Š ç¬¬ä¸€é˜¶æ®µ: Baselineå¯¹æ¯”æµ‹è¯•")
            test_networks = eval_config['baseline_comparison']['test_networks']

            for network in test_networks[:1]:  # åªæµ‹è¯•ç¬¬ä¸€ä¸ªç½‘ç»œä»¥èŠ‚çœæ—¶é—´
                print(f"\n   æµ‹è¯•ç½‘ç»œ: {network.upper()}")
                results = evaluator.run_baseline_comparison(network)
                if results['success']:
                    comparison_results = results
                    print(f"   âœ… {network.upper()} å¯¹æ¯”æµ‹è¯•å®Œæˆ")
                    break

        # 2. æ³›åŒ–èƒ½åŠ›æµ‹è¯•
        if eval_config['generalization_test']['enabled']:
            print(f"\nğŸŒ ç¬¬äºŒé˜¶æ®µ: æ³›åŒ–èƒ½åŠ›æµ‹è¯•")
            train_network = eval_config['generalization_test']['train_network']

            results = evaluator.run_generalization_test(train_network)
            if results['success']:
                generalization_results = results
                print(f"   âœ… æ³›åŒ–æµ‹è¯•å®Œæˆ")

        # 3. ç”Ÿæˆç»¼åˆæŠ¥å‘Šå’Œå¯è§†åŒ–
        if comparison_results or generalization_results:
            print(f"\nğŸ“ ç¬¬ä¸‰é˜¶æ®µ: ç”Ÿæˆç»¼åˆæŠ¥å‘Š")

            # ç”Ÿæˆå¯è§†åŒ–
            if comparison_results:
                evaluator.create_comparison_visualization(comparison_results['results'])

            if generalization_results:
                evaluator.create_generalization_visualization(
                    generalization_results['results'],
                    generalization_results['train_network']
                )

            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            evaluator.generate_evaluation_report(
                comparison_results=comparison_results,
                generalization_results=generalization_results
            )

            print(f"\nğŸ‰ å®Œæ•´è¯„ä¼°å®Œæˆï¼")
            print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: evaluation_results/ ç›®å½•")

            return True
        else:
            print(f"âŒ æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥äº†")
            return False

    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def print_available_modes():
    """æ‰“å°å¯ç”¨çš„è¯„ä¼°æ¨¡å¼"""
    print("ğŸ“‹ å¯ç”¨çš„è¯„ä¼°æ¨¡å¼:")
    
    modes = {
        'quick': {
            'description': 'å¿«é€Ÿè¯„ä¼°æ¨¡å¼',
            'baseline_comparison': {
                'enabled': True,
                'test_networks': ['ieee14', 'ieee30'],
                'scenarios': ['normal', 'high_load'],
                'runs_per_test': 5
            },
            'generalization_test': {
                'enabled': True,
                'test_networks': ['ieee30'],
                'runs_per_network': 10
            }
        },
        'standard': {
            'description': 'æ ‡å‡†è¯„ä¼°æ¨¡å¼',
            'baseline_comparison': {
                'enabled': True,
                'test_networks': ['ieee14', 'ieee30', 'ieee57'],
                'scenarios': ['normal', 'high_load', 'unbalanced'],
                'runs_per_test': 10
            },
            'generalization_test': {
                'enabled': True,
                'test_networks': ['ieee30', 'ieee57'],
                'runs_per_network': 20
            }
        },
        'comprehensive': {
            'description': 'å…¨é¢è¯„ä¼°æ¨¡å¼',
            'baseline_comparison': {
                'enabled': True,
                'test_networks': ['ieee14', 'ieee30', 'ieee57'],
                'scenarios': ['normal', 'high_load', 'unbalanced', 'fault'],
                'runs_per_test': 15
            },
            'generalization_test': {
                'enabled': True,
                'test_networks': ['ieee30', 'ieee57', 'ieee118'],
                'runs_per_network': 30
            }
        }
    }
    
    for mode_name, mode_config in modes.items():
        print(f"   {mode_name}: {mode_config['description']}")

        # Baselineå¯¹æ¯”é…ç½®
        baseline_config = mode_config.get('baseline_comparison', {})
        if baseline_config.get('enabled', False):
            networks = baseline_config.get('test_networks', [])
            scenarios = baseline_config.get('scenarios', [])
            runs = baseline_config.get('runs_per_test', 0)
            print(f"     - Baselineå¯¹æ¯”: {len(networks)}ä¸ªç½‘ç»œ, {len(scenarios)}ç§åœºæ™¯, æ¯ä¸ª{runs}æ¬¡è¿è¡Œ")

        # æ³›åŒ–æµ‹è¯•é…ç½®
        gen_config = mode_config.get('generalization_test', {})
        if gen_config.get('enabled', False):
            test_networks = gen_config.get('test_networks', [])
            runs = gen_config.get('runs_per_network', 0)
            print(f"     - æ³›åŒ–æµ‹è¯•: {len(test_networks)}ä¸ªæµ‹è¯•ç½‘ç»œ, æ¯ä¸ª{runs}æ¬¡è¿è¡Œ")

        print()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ç”µåŠ›ç½‘ç»œåˆ†åŒºæ™ºèƒ½ä½“ç°ä»£åŒ–æµ‹è¯•ç³»ç»Ÿ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä¼ ç»Ÿè¯„ä¼°æµ‹è¯•
  python test.py --baseline --network ieee30 --model models/agent_ieee57_adaptive_best.pth
  python test.py --generalization --train-network ieee14 --model models/agent_ieee57_adaptive_best.pth
  python test.py --comprehensive --mode standard --model models/agent_ieee57_adaptive_best.pth

  # å®æ—¶æ€§èƒ½æµ‹è¯•ï¼ˆé‡ç‚¹åŠŸèƒ½ - Agent vs Baselineç®—æ³•æ€§èƒ½å¯¹æ¯”ï¼‰
  python test.py --benchmark --model models/agent_ieee57_adaptive_best.pth --network ieee57 --runs 100

  # æ€§èƒ½æµ‹è¯•è¯´æ˜ï¼š
  # - æµ‹è¯•Agentå†³ç­–æ—¶é—´ vs Baselineç®—æ³•ï¼ˆè°±èšç±»ã€K-meansï¼‰æ‰§è¡Œæ—¶é—´
  # - ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½å¯¹æ¯”æŠ¥å‘Šå’Œå¯è§†åŒ–ç»“æœ
  # - è¯„ä¼°Agentåœ¨å®æ—¶ç”µç½‘æ§åˆ¶åœºæ™¯ä¸‹çš„æ€§èƒ½ä¼˜åŠ¿

  # å…¶ä»–
  python test.py --list-modes                                  # æŸ¥çœ‹å¯ç”¨æ¨¡å¼
        """
    )

    # æµ‹è¯•ç±»å‹é€‰æ‹©
    test_group = parser.add_mutually_exclusive_group(required=True)
    test_group.add_argument('--baseline', action='store_true', help='è¿è¡ŒBaselineå¯¹æ¯”æµ‹è¯•')
    test_group.add_argument('--generalization', action='store_true', help='è¿è¡Œæ³›åŒ–èƒ½åŠ›æµ‹è¯•')
    test_group.add_argument('--comprehensive', action='store_true', help='è¿è¡Œå®Œæ•´è¯„ä¼°')
    test_group.add_argument('--benchmark', action='store_true', help='è¿è¡Œå®æ—¶æ€§èƒ½æµ‹è¯•ï¼ˆAgent vs Baselineç®—æ³•æ‰§è¡Œæ—¶é—´å¯¹æ¯”ï¼ŒåŒ…å«è°±èšç±»å’ŒK-meansï¼‰')
    test_group.add_argument('--list-modes', action='store_true', help='åˆ—å‡ºå¯ç”¨çš„è¯„ä¼°æ¨¡å¼')

    # é…ç½®å‚æ•°
    parser.add_argument('--config', type=str, default=None, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--mode', type=str, default='standard',
                       choices=['quick', 'standard', 'comprehensive'],
                       help='è¯„ä¼°æ¨¡å¼ (é»˜è®¤: standard)')
    parser.add_argument('--model', type=str, help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ (ä¼ ç»Ÿæµ‹è¯•å¿…éœ€)')



    # è‡ªå®šä¹‰æµ‹è¯•å‚æ•°
    parser.add_argument('--scenarios', type=str, nargs='+',
                       choices=['normal', 'high_load', 'unbalanced', 'fault'],
                       help='(è‡ªå®šä¹‰) æŒ‡å®šä¸€ä¸ªæˆ–å¤šä¸ªæµ‹è¯•åœºæ™¯')
    parser.add_argument('--runs', type=int, help='(è‡ªå®šä¹‰) æŒ‡å®šæ¯ä¸ªæµ‹è¯•çš„è¿è¡Œæ¬¡æ•°')

    # Baselineæµ‹è¯•å‚æ•°
    parser.add_argument('--network', type=str, default='ieee30',
                       choices=['ieee14', 'ieee30', 'ieee57', 'ieee118'],
                       help='Baselineæµ‹è¯•ç½‘ç»œ (é»˜è®¤: ieee30)')

    # æ³›åŒ–æµ‹è¯•å‚æ•°
    parser.add_argument('--train-network', type=str, default='ieee14',
                       choices=['ieee14', 'ieee30', 'ieee57'],
                       help='æ³›åŒ–æµ‹è¯•çš„è®­ç»ƒç½‘ç»œ (é»˜è®¤: ieee14)')

    args = parser.parse_args()

    try:
        print("ğŸ¯ ç”µåŠ›ç½‘ç»œåˆ†åŒºæ™ºèƒ½ä½“ç°ä»£åŒ–æµ‹è¯•ç³»ç»Ÿ")
        print("=" * 60)
        print(f"å¯åŠ¨æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print()

        # å¤„ç†ä¸éœ€è¦æ¨¡å‹çš„å‘½ä»¤
        if args.list_modes:
            print_available_modes()
            return 0



        # å¤„ç†å®æ—¶æ€§èƒ½æµ‹è¯•
        if args.benchmark:
            if not args.model:
                print("âŒ é”™è¯¯ï¼šæ€§èƒ½æµ‹è¯•éœ€è¦æä¾›æ¨¡å‹è·¯å¾„")
                print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•: python test.py --benchmark --model <æ¨¡å‹è·¯å¾„>")
                return 1

            performance_tester = RealTimePerformanceTester()

            print("âš¡ è¿è¡ŒAgent vs Baselineå®æ—¶æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
            results = performance_tester.run_engineering_performance_test(
                model_path=args.model,
                network=args.network,
                num_runs=args.runs if args.runs else 100
            )

            if results['success']:
                print(f"\nğŸ‰ Agent vs Baselineæ€§èƒ½å¯¹æ¯”æµ‹è¯•å®Œæˆï¼")
                print(f"ğŸ“ è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹: performance_results/ ç›®å½•")
                print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š: agent_vs_baseline_comparison_{args.network}_*.json")
                print(f"ğŸ“ˆ CSVæ‘˜è¦æ–‡ä»¶: performance_summary_{args.network}_*.csv")
                return 0
            else:
                print(f"\nâŒ æ€§èƒ½å¯¹æ¯”æµ‹è¯•å¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return 1

        # æ£€æŸ¥ä¼ ç»Ÿæµ‹è¯•çš„æ¨¡å‹å‚æ•°
        if not args.model and (args.baseline or args.generalization or args.comprehensive):
            print("âŒ é”™è¯¯ï¼šä¼ ç»Ÿæµ‹è¯•å¿…é¡»æä¾›é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
            print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•: python test.py <æµ‹è¯•ç±»å‹> --model <æ¨¡å‹è·¯å¾„>")
            print("ğŸ“‹ æŸ¥çœ‹å¸®åŠ©: python test.py --help")
            return 1

        success = False

        if args.baseline:
            success = run_baseline_comparison(
                config_path=args.config,
                network=args.network,
                mode=args.mode,
                model_path=args.model,
                custom_scenarios=args.scenarios,
                custom_runs=args.runs
            )

        elif args.generalization:
            success = run_generalization_test(
                config_path=args.config,
                train_network=args.train_network,
                mode=args.mode,
                model_path=args.model,
                custom_runs=args.runs
            )

        elif args.comprehensive:
            success = run_comprehensive_evaluation(args.config, args.mode, args.model)

        if success:
            print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼")
            print(f"ğŸ“ è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹: evaluation_results/ ç›®å½•")
            print(f"ğŸ’¡ æç¤º: å¯ä»¥ä½¿ç”¨ä¸åŒçš„ --mode å‚æ•°æ¥è°ƒæ•´è¯„ä¼°è¯¦ç»†ç¨‹åº¦")
            return 0
        else:
            print(f"\nâŒ è¯„ä¼°å¤±è´¥")
            return 1

    except KeyboardInterrupt:
        print(f"\nâš ï¸ ç”¨æˆ·ä¸­æ–­è¯„ä¼°")
        return 1
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°ç³»ç»Ÿå‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
