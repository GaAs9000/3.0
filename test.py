#!/usr/bin/env python3
"""
ç”µåŠ›ç½‘ç»œåˆ†åŒºæ™ºèƒ½ä½“ç»¼åˆè¯„ä¼°ç³»ç»Ÿ - ä¸»å…¥å£

é›†æˆæ‰€æœ‰è¯„ä¼°åŠŸèƒ½ï¼š
- Baselineæ–¹æ³•å¯¹æ¯”æµ‹è¯•
- è·¨ç½‘ç»œæ³›åŒ–èƒ½åŠ›è¯„ä¼°
- è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ
- å¯è§†åŒ–åˆ†æ
"""

import argparse
import sys
import time
from pathlib import Path
from typing import Optional

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent / 'code' / 'src'))
sys.path.append(str(Path(__file__).parent / 'code'))
sys.path.append(str(Path(__file__).parent / 'code' / 'test'))

from comprehensive_evaluator import ComprehensiveAgentEvaluator


def run_baseline_comparison(config_path: Optional[str] = None, network: str = 'ieee57', mode: str = 'standard', model_path: Optional[str] = None):
    """è¿è¡Œbaselineå¯¹æ¯”æµ‹è¯•"""
    print(f"ğŸ” å¯åŠ¨Baselineå¯¹æ¯”æµ‹è¯•")
    print(f"   æµ‹è¯•ç½‘ç»œ: {network.upper()}")
    print(f"   è¯„ä¼°æ¨¡å¼: {mode}")
    if model_path:
        print(f"   é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
    print("=" * 50)

    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = ComprehensiveAgentEvaluator(config_path, model_path)

        # è¿è¡Œå¯¹æ¯”æµ‹è¯•
        results = evaluator.run_baseline_comparison(network)

        if results['success']:
            print(f"\nâœ… Baselineå¯¹æ¯”æµ‹è¯•å®Œæˆï¼")

            # ç”Ÿæˆå¯è§†åŒ–
            evaluator.create_comparison_visualization(results['results'])

            # ç”ŸæˆæŠ¥å‘Š
            report = evaluator.generate_evaluation_report(comparison_results=results)

            # æ‰“å°ç®€è¦æ€»ç»“
            summary = results['summary']
            avg_improvement = summary['overall_improvement']['average_improvement']
            print(f"\nğŸ† æµ‹è¯•æ€»ç»“:")
            print(f"   å¹³å‡æ€§èƒ½æå‡: {avg_improvement:.1f}%")

            for scenario, scenario_summary in summary['best_scenarios'].items():
                improvement = scenario_summary['improvement_pct']
                status = "ğŸ‰ ä¼˜ç§€" if improvement > 15 else "âœ… è‰¯å¥½" if improvement > 5 else "âš ï¸ æœ‰é™"
                print(f"   {scenario}: {improvement:.1f}% - {status}")

            return True
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return False

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def run_generalization_test(config_path: Optional[str] = None, train_network: str = 'ieee14', mode: str = 'standard', model_path: Optional[str] = None):
    """è¿è¡Œæ³›åŒ–èƒ½åŠ›æµ‹è¯•"""
    print(f"ğŸŒ å¯åŠ¨æ³›åŒ–èƒ½åŠ›æµ‹è¯•")
    print(f"   è®­ç»ƒç½‘ç»œ: {train_network.upper()}")
    print(f"   è¯„ä¼°æ¨¡å¼: {mode}")
    if model_path:
        print(f"   é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
    print("=" * 50)

    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = ComprehensiveAgentEvaluator(config_path, model_path)

        # è¿è¡Œæ³›åŒ–æµ‹è¯•
        results = evaluator.run_generalization_test(train_network)

        if results['success']:
            print(f"\nâœ… æ³›åŒ–èƒ½åŠ›æµ‹è¯•å®Œæˆï¼")

            # ç”Ÿæˆå¯è§†åŒ–
            evaluator.create_generalization_visualization(results['results'], train_network)

            # ç”ŸæˆæŠ¥å‘Š
            report = evaluator.generate_evaluation_report(generalization_results=results)

            # æ‰“å°ç®€è¦æ€»ç»“
            summary = results['summary']
            overall = summary['overall_generalization']
            avg_degradation = overall['average_degradation']
            status = overall['status']

            print(f"\nğŸ† æµ‹è¯•æ€»ç»“:")
            print(f"   è®­ç»ƒç½‘ç»œæ€§èƒ½: {summary['train_score']:.3f}")
            print(f"   å¹³å‡æ€§èƒ½ä¸‹é™: {avg_degradation:.1f}%")
            print(f"   æ³›åŒ–è¯„çº§: {status}")

            for network, result in summary['test_results'].items():
                print(f"   {network.upper()}: {result['score']:.3f} ({result['degradation_pct']:.1f}% ä¸‹é™) - {result['status']}")

            return True
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return False

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def run_comprehensive_evaluation(config_path: Optional[str] = None, mode: str = 'standard', model_path: Optional[str] = None):
    """è¿è¡Œå®Œæ•´è¯„ä¼°ï¼ˆåŒ…å«baselineå¯¹æ¯”å’Œæ³›åŒ–æµ‹è¯•ï¼‰"""
    print(f"ğŸ¯ å¯åŠ¨å®Œæ•´è¯„ä¼°")
    print(f"   è¯„ä¼°æ¨¡å¼: {mode}")
    if model_path:
        print(f"   é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
    else:
        print("âŒ é”™è¯¯ï¼šå¿…é¡»æä¾›é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
        print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•: python test.py --comprehensive --model <æ¨¡å‹è·¯å¾„>")
        return False
    print("=" * 50)

    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = ComprehensiveAgentEvaluator(config_path, model_path)

        # è·å–è¯„ä¼°é…ç½®
        eval_config = evaluator.get_evaluation_config(mode)

        comparison_results = None
        generalization_results = None

        # 1. Baselineå¯¹æ¯”æµ‹è¯•
        if eval_config['baseline_comparison']['enabled']:
            print(f"\nğŸ“Š ç¬¬ä¸€é˜¶æ®µ: Baselineå¯¹æ¯”æµ‹è¯•")
            test_networks = eval_config['baseline_comparison']['test_networks']

            for network in test_networks[:1]:  # åªæµ‹è¯•ç¬¬ä¸€ä¸ªç½‘ç»œä»¥èŠ‚çœæ—¶é—´
                print(f"\n   æµ‹è¯•ç½‘ç»œ: {network.upper()}")
                results = evaluator.run_baseline_comparison(network)
                if results['success']:
                    comparison_results = results
                    print(f"   âœ… {network.upper()} å¯¹æ¯”æµ‹è¯•å®Œæˆ")
                    break

        # 2. æ³›åŒ–èƒ½åŠ›æµ‹è¯•
        if eval_config['generalization_test']['enabled']:
            print(f"\nğŸŒ ç¬¬äºŒé˜¶æ®µ: æ³›åŒ–èƒ½åŠ›æµ‹è¯•")
            train_network = eval_config['generalization_test']['train_network']

            results = evaluator.run_generalization_test(train_network)
            if results['success']:
                generalization_results = results
                print(f"   âœ… æ³›åŒ–æµ‹è¯•å®Œæˆ")

        # 3. ç”Ÿæˆç»¼åˆæŠ¥å‘Šå’Œå¯è§†åŒ–
        if comparison_results or generalization_results:
            print(f"\nğŸ“ ç¬¬ä¸‰é˜¶æ®µ: ç”Ÿæˆç»¼åˆæŠ¥å‘Š")

            # ç”Ÿæˆå¯è§†åŒ–
            if comparison_results:
                evaluator.create_comparison_visualization(comparison_results['results'])

            if generalization_results:
                evaluator.create_generalization_visualization(
                    generalization_results['results'],
                    generalization_results['train_network']
                )

            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            report = evaluator.generate_evaluation_report(
                comparison_results=comparison_results,
                generalization_results=generalization_results
            )

            print(f"\nğŸ‰ å®Œæ•´è¯„ä¼°å®Œæˆï¼")
            print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: evaluation_results/ ç›®å½•")

            return True
        else:
            print(f"âŒ æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥äº†")
            return False

    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def print_available_modes():
    """æ‰“å°å¯ç”¨çš„è¯„ä¼°æ¨¡å¼"""
    print("ğŸ“‹ å¯ç”¨çš„è¯„ä¼°æ¨¡å¼:")
    
    modes = {
        'quick': {
            'description': 'å¿«é€Ÿè¯„ä¼°æ¨¡å¼',
            'baseline_comparison': {
                'enabled': True,
                'test_networks': ['ieee14', 'ieee30'],
                'scenarios': ['normal', 'high_load'],
                'runs_per_test': 5
            },
            'generalization_test': {
                'enabled': True,
                'test_networks': ['ieee30'],
                'runs_per_network': 10
            }
        },
        'standard': {
            'description': 'æ ‡å‡†è¯„ä¼°æ¨¡å¼',
            'baseline_comparison': {
                'enabled': True,
                'test_networks': ['ieee14', 'ieee30', 'ieee57'],
                'scenarios': ['normal', 'high_load', 'unbalanced'],
                'runs_per_test': 10
            },
            'generalization_test': {
                'enabled': True,
                'test_networks': ['ieee30', 'ieee57'],
                'runs_per_network': 20
            }
        },
        'comprehensive': {
            'description': 'å…¨é¢è¯„ä¼°æ¨¡å¼',
            'baseline_comparison': {
                'enabled': True,
                'test_networks': ['ieee14', 'ieee30', 'ieee57'],
                'scenarios': ['normal', 'high_load', 'unbalanced', 'fault'],
                'runs_per_test': 15
            },
            'generalization_test': {
                'enabled': True,
                'test_networks': ['ieee30', 'ieee57', 'ieee118'],
                'runs_per_network': 30
            }
        }
    }
    
    for mode_name, mode_config in modes.items():
        print(f"   {mode_name}: {mode_config['description']}")

        # Baselineå¯¹æ¯”é…ç½®
        baseline_config = mode_config.get('baseline_comparison', {})
        if baseline_config.get('enabled', False):
            networks = baseline_config.get('test_networks', [])
            scenarios = baseline_config.get('scenarios', [])
            runs = baseline_config.get('runs_per_test', 0)
            print(f"     - Baselineå¯¹æ¯”: {len(networks)}ä¸ªç½‘ç»œ, {len(scenarios)}ç§åœºæ™¯, æ¯ä¸ª{runs}æ¬¡è¿è¡Œ")

        # æ³›åŒ–æµ‹è¯•é…ç½®
        gen_config = mode_config.get('generalization_test', {})
        if gen_config.get('enabled', False):
            test_networks = gen_config.get('test_networks', [])
            runs = gen_config.get('runs_per_network', 0)
            print(f"     - æ³›åŒ–æµ‹è¯•: {len(test_networks)}ä¸ªæµ‹è¯•ç½‘ç»œ, æ¯ä¸ª{runs}æ¬¡è¿è¡Œ")

        print()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ç”µåŠ›ç½‘ç»œåˆ†åŒºæ™ºèƒ½ä½“ç»¼åˆè¯„ä¼°ç³»ç»Ÿ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python test.py --baseline --network ieee30 --model data/checkpoints/models/agent_ieee57_adaptive_best.pth
  python test.py --generalization --train-network ieee14 --model data/checkpoints/models/agent_ieee57_adaptive_best.pth
  python test.py --comprehensive --mode standard --model data/checkpoints/models/agent_ieee57_adaptive_best.pth
  python test.py --list-modes                                  # æŸ¥çœ‹å¯ç”¨æ¨¡å¼
        """
    )

    # æµ‹è¯•ç±»å‹é€‰æ‹©
    test_group = parser.add_mutually_exclusive_group(required=True)
    test_group.add_argument('--baseline', action='store_true', help='è¿è¡ŒBaselineå¯¹æ¯”æµ‹è¯•')
    test_group.add_argument('--generalization', action='store_true', help='è¿è¡Œæ³›åŒ–èƒ½åŠ›æµ‹è¯•')
    test_group.add_argument('--comprehensive', action='store_true', help='è¿è¡Œå®Œæ•´è¯„ä¼°')
    test_group.add_argument('--list-modes', action='store_true', help='åˆ—å‡ºå¯ç”¨çš„è¯„ä¼°æ¨¡å¼')

    # é…ç½®å‚æ•°
    parser.add_argument('--config', type=str, default=None, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--mode', type=str, default='standard',
                       choices=['quick', 'standard', 'comprehensive'],
                       help='è¯„ä¼°æ¨¡å¼ (é»˜è®¤: standard)')
    parser.add_argument('--model', type=str, help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ (é™¤--list-modeså¤–å¿…éœ€)')

    # Baselineæµ‹è¯•å‚æ•°
    parser.add_argument('--network', type=str, default='ieee30',
                       choices=['ieee14', 'ieee30', 'ieee57', 'ieee118'],
                       help='Baselineæµ‹è¯•ç½‘ç»œ (é»˜è®¤: ieee30)')

    # æ³›åŒ–æµ‹è¯•å‚æ•°
    parser.add_argument('--train-network', type=str, default='ieee14',
                       choices=['ieee14', 'ieee30', 'ieee57'],
                       help='æ³›åŒ–æµ‹è¯•çš„è®­ç»ƒç½‘ç»œ (é»˜è®¤: ieee14)')

    args = parser.parse_args()

    try:
        print("ğŸ¯ ç”µåŠ›ç½‘ç»œåˆ†åŒºæ™ºèƒ½ä½“ç»¼åˆè¯„ä¼°ç³»ç»Ÿ")
        print("=" * 60)
        print(f"å¯åŠ¨æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print()

        if args.list_modes:
            print_available_modes()
            return 0

        # æ£€æŸ¥æ¨¡å‹å‚æ•°
        if not args.model:
            print("âŒ é”™è¯¯ï¼šå¿…é¡»æä¾›é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
            print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•: python test.py <æµ‹è¯•ç±»å‹> --model <æ¨¡å‹è·¯å¾„>")
            print("ğŸ“‹ æŸ¥çœ‹å¸®åŠ©: python test.py --help")
            return 1

        success = False

        if args.baseline:
            success = run_baseline_comparison(args.config, args.network, args.mode, args.model)

        elif args.generalization:
            success = run_generalization_test(args.config, args.train_network, args.mode, args.model)

        elif args.comprehensive:
            success = run_comprehensive_evaluation(args.config, args.mode, args.model)

        if success:
            print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼")
            print(f"ğŸ“ è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹: evaluation_results/ ç›®å½•")
            print(f"ğŸ’¡ æç¤º: å¯ä»¥ä½¿ç”¨ä¸åŒçš„ --mode å‚æ•°æ¥è°ƒæ•´è¯„ä¼°è¯¦ç»†ç¨‹åº¦")
            return 0
        else:
            print(f"\nâŒ è¯„ä¼°å¤±è´¥")
            return 1

    except KeyboardInterrupt:
        print(f"\nâš ï¸ ç”¨æˆ·ä¸­æ–­è¯„ä¼°")
        return 1
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°ç³»ç»Ÿå‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
