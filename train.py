#!/usr/bin/env python3
"""
ç”µåŠ›ç½‘ç»œåˆ†åŒºå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å—

ä¸“æ³¨äºæ¨¡å‹è®­ç»ƒåŠŸèƒ½ï¼š
- é…ç½®åŠ è½½å’ŒéªŒè¯
- æ•°æ®å¤„ç†å’Œç¯å¢ƒåˆ›å»º
- æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–
- åŸºç¡€å¯è§†åŒ–å’Œç»“æœä¿å­˜
- è®­ç»ƒç›‘æ§å’Œæ—¥å¿—è®°å½•
"""

import torch
import numpy as np
import argparse
import yaml
import os
import sys
import time
import json
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from collections import deque
from tqdm import tqdm
import queue
import threading
from rich.panel import Panel

# æ·»åŠ code/srcåˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / 'code' / 'src'))
# æ·»åŠ codeåˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥baseline
sys.path.append(str(Path(__file__).parent / 'code'))

# --- æ ¸å¿ƒæ¨¡å—å¯¼å…¥ ---
try:
    import gymnasium as gym
    from stable_baselines3 import PPO
    from torch_geometric.data import HeteroData
    from data_processing import PowerGridDataProcessor
    from gat import create_production_encoder
    from rl.environment import PowerGridPartitioningEnv
    from rl.agent import PPOAgent
    from rl.adaptive import AdaptiveDirector
    CRITICAL_DEPS_AVAILABLE = True
except ImportError as e:
    CRITICAL_DEPS_AVAILABLE = False
    MISSING_DEP_ERROR = e

# --- å…¨å±€ NaN/Inf å¼‚å¸¸æ£€æµ‹å¼€å…³ ---
torch.autograd.set_detect_anomaly(False)
warnings.filterwarnings("error", message=".*NaN.*", category=RuntimeWarning)
warnings.filterwarnings('ignore', category=UserWarning)

def safe_print(message: str):
    """å®‰å…¨çš„Unicodeæ‰“å°å‡½æ•°ï¼Œé¿å…ç¼–ç é”™è¯¯"""
    try:
        print(message)
    except UnicodeEncodeError:
        # ç§»é™¤Unicodeå­—ç¬¦ï¼Œä½¿ç”¨ASCIIæ›¿ä»£
        ascii_message = message.encode('ascii', 'replace').decode('ascii')
        print(ascii_message)

# å¯¼å…¥ç”µåŠ›ç³»ç»Ÿæ•°æ®åŠ è½½å‡½æ•°
try:
    from scipy.io import loadmat
except ImportError:
    print("âš ï¸ è­¦å‘Š: scipyæœªå®‰è£…ï¼Œæ— æ³•åŠ è½½MATPOWERæ–‡ä»¶")

try:
    import pandapower as pp
    import pandapower.networks as pn
    _pandapower_available = True
except ImportError:
    _pandapower_available = False
    print("âš ï¸ è­¦å‘Š: pandapoweræœªå®‰è£…ï¼Œæ— æ³•åŠ è½½IEEEæ ‡å‡†æµ‹è¯•ç³»ç»Ÿ")


def check_dependencies():
    """æ£€æŸ¥å¯é€‰ä¾èµ–"""
    deps = {
        'stable_baselines3': False,
        'use_tensorboard': False,
        'plotly': False,
        'networkx': False
    }

    try:
        import stable_baselines3
        deps['stable_baselines3'] = True
    except ImportError:
        pass

    try:
        from torch.utils.tensorboard import SummaryWriter
        deps['use_tensorboard'] = True
    except ImportError:
        pass

    try:
        import plotly.graph_objects as go
        deps['plotly'] = True
    except ImportError:
        pass

    try:
        import networkx as nx
        deps['networkx'] = True
    except ImportError:
        pass

    return deps


def load_power_grid_data(case_name: str) -> Dict:
    """
    åŠ è½½ç”µåŠ›ç½‘ç»œæ•°æ®ã€‚
    è¯¥å‡½æ•°ç°åœ¨åªä»pandapoweråŠ è½½æ•°æ®ï¼Œä»¥ç¡®ä¿æ•°æ®æºçš„ä¸€è‡´æ€§å’Œé«˜è´¨é‡ã€‚
    å¦‚æœåŠ è½½å¤±è´¥ï¼Œå°†ç›´æ¥æŠ›å‡ºå¼‚å¸¸ã€‚

    Args:
        case_name: æ¡ˆä¾‹åç§° (ieee14, ieee30, ieee57, ieee118)

    Returns:
        MATPOWERæ ¼å¼çš„ç”µç½‘æ•°æ®å­—å…¸
    """
    if not _pandapower_available:
        raise ImportError("Pandapoweråº“æœªå®‰è£…æˆ–ä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½ç”µç½‘æ•°æ®ã€‚")

    try:
        # è¿™æ˜¯å”¯ä¸€åº”è¯¥è¢«æ‰§è¡Œçš„æ•°æ®åŠ è½½è·¯å¾„
        return load_from_pandapower(case_name)
    except Exception as e:
        print(f"âŒ ä»PandaPoweråŠ è½½æ¡ˆä¾‹ '{case_name}' æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯ã€‚")
        print("è¿™å¯èƒ½æ˜¯ç”±äºæ‹¼å†™é”™è¯¯ã€æ¡ˆä¾‹ä¸å­˜åœ¨æˆ–PandaPoweråº“æœ¬èº«çš„é—®é¢˜ã€‚")
        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä»¥ä¾¿ç¨‹åºåœæ­¢
        raise e


def load_from_pandapower(case_name: str) -> Dict:
    """
    ä»PandaPoweråŠ è½½IEEEæ ‡å‡†æµ‹è¯•ç³»ç»Ÿ

    Args:
        case_name: æ¡ˆä¾‹åç§° (ieee14, ieee30, ieee57, ieee118)

    Returns:
        MATPOWERæ ¼å¼çš„ç”µç½‘æ•°æ®å­—å…¸
    """
    print(f"ä»PandaPoweråŠ è½½ {case_name.upper()} æ•°æ®...")

    # æ˜ å°„æ¡ˆä¾‹åç§°åˆ°PandaPowerå‡½æ•°
    case_mapping = {
        'ieee14': pn.case14,
        'ieee30': pn.case30,
        'ieee57': pn.case57,
        'ieee118': pn.case118
    }

    if case_name not in case_mapping:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¡ˆä¾‹: {case_name}")

    # åŠ è½½PandaPowerç½‘ç»œ
    net = case_mapping[case_name]()
    from code.src.rich_output import rich_success
    rich_success(f"æˆåŠŸåŠ è½½ {case_name.upper()}: {len(net.bus)} èŠ‚ç‚¹, {len(net.line)} çº¿è·¯")

    # è½¬æ¢ä¸ºMATPOWERæ ¼å¼
    mpc = convert_pandapower_to_matpower(net)

    return mpc


def convert_pandapower_to_matpower(net) -> Dict:
    """
    å°†PandaPowerç½‘ç»œè½¬æ¢ä¸ºMATPOWERæ ¼å¼ï¼ˆä½¿ç”¨å®˜æ–¹converterï¼‰

    Args:
        net: PandaPowerç½‘ç»œå¯¹è±¡

    Returns:
        MATPOWERæ ¼å¼çš„ç”µç½‘æ•°æ®å­—å…¸
    """
    # ä½¿ç”¨å®˜æ–¹ converter è¿›è¡Œå¯é è½¬æ¢ï¼Œé¿å…æ‰‹å†™è½¬æ¢ä¸­çš„ç²¾åº¦å’Œç»´æŠ¤é—®é¢˜
    import pandapower as pp
    from pandapower.converter import to_mpc

    # ç¡®ä¿ç½‘ç»œå·²æœ‰æ½®æµç»“æœï¼›å¦‚æœæ²¡æœ‰ï¼Œåˆ™å…ˆè¿è¡Œä¸€æ¬¡å¹³å¦å¯åŠ¨æ½®æµ
    try:
        if getattr(net, "res_bus", None) is None or net.res_bus.empty:
            pp.runpp(net, init="flat", calculate_voltage_angles=False, numba=False)
    except Exception:
        # å¦‚æœæ½®æµå¤±è´¥ï¼Œä»ç»§ç»­è½¬æ¢ï¼ˆto_mpc å…è®¸æ— ç»“æœè½¬æ¢ï¼Œä½†ä¼šç»™å‡ºè­¦å‘Šï¼‰
        pass

    mpc_wrap = to_mpc(net)

    # pandapower >=2.0 è¿”å› {'mpc': dict}ï¼›å‘åå…¼å®¹æ—§ç‰ˆæœ¬
    if isinstance(mpc_wrap, dict) and "mpc" in mpc_wrap:
        mpc = mpc_wrap["mpc"]
    else:
        mpc = mpc_wrap

    return mpc


class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—è®°å½•å™¨ - æ”¯æŒTensorBoardç›‘æ§"""

    def __init__(self, config: Dict[str, Any], total_episodes: int):
        """åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨"""
        self.config = config
        self.start_time = time.time()
        self.episode_rewards = []
        self.episode_lengths = []
        self.actor_losses = []
        self.critic_losses = []
        self.entropies = []
        self.success_rates = []
        self.load_cvs = []
        self.coupling_edges = []
        self.best_reward = -float('inf')
        self.total_episodes = total_episodes

        # æ™ºèƒ½è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ ç›¸å…³æŒ‡æ ‡
        self.curriculum_stages = []
        self.stage_transitions = []
        self.director_decisions = []

        log_config = config.get('logging', {})
        self.metrics_save_interval = log_config.get('metrics_save_interval', 100)

        # ç§»é™¤RichçŠ¶æ€é¢æ¿ï¼Œç®€åŒ–è¾“å‡º
        
        # ä½¿ç”¨ç®€å•è¿›åº¦æ¡
        try:
            from tqdm import tqdm
            self.progress_bar = tqdm(total=total_episodes, desc="ğŸš€ è®­ç»ƒè¿›åº¦",
                                   bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')
            self.use_rich = False
        except ImportError:
            # å¦‚æœtqdmä¹Ÿä¸å¯ç”¨ï¼Œä½¿ç”¨æœ€åŸºæœ¬çš„è¿›åº¦æ˜¾ç¤º
            self.progress_bar = None
            self.use_rich = False

        # è®¾ç½®TensorBoard
        self.use_tensorboard = log_config.get('use_tensorboard', False)
        self.tensorboard_writer = self._setup_tensorboard() if self.use_tensorboard else None

        # HTMLä»ªè¡¨æ¿å·²ç§»åŠ¨åˆ°test.pyä¸­ç”¨äºæ€§èƒ½åˆ†æ

        # ç§»é™¤TUIé›†æˆï¼Œç®€åŒ–ä¸ºåŸºæœ¬è¾“å‡º
        self.use_tui = False



    def _create_status_panel(self, episode: int, reward: float, info: Dict = None):
        """åˆ›å»ºç®€æ´çš„çŠ¶æ€é¢æ¿"""
        try:
            from rich.panel import Panel
            from rich.table import Table
            
            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            avg_reward = sum(self.episode_rewards[-10:]) / min(len(self.episode_rewards), 10) if self.episode_rewards else 0
            progress_pct = (episode + 1) / self.total_episodes * 100
            
            # ã€æ”¹è¿›ã€‘è®¡ç®—å¤šå±‚æ¬¡æˆåŠŸç‡
            success_stats = self._compute_multi_level_success_rate()
            
            # æå–è´¨é‡æŒ‡æ ‡
            quality_score = None
            if info and 'reward_components' in info:
                components = info['reward_components']
                quality_score = components.get('quality_score')
            
            # åˆ›å»ºä¸»è¡¨æ ¼
            table = Table.grid(padding=1)
            table.add_column("æŒ‡æ ‡", style="bold cyan", min_width=12)
            table.add_column("æ•°å€¼", justify="right", min_width=15)
            table.add_column("çŠ¶æ€", justify="center", min_width=8)
            
            # è¿›åº¦ä¿¡æ¯
            table.add_row("è®­ç»ƒè¿›åº¦", f"{episode+1:,}/{self.total_episodes:,} ({progress_pct:.1f}%)", "ğŸ“Š")
            
            # å¥–åŠ±ä¿¡æ¯
            reward_color = "bold green" if reward > 0 else "yellow" if reward > -1 else "red"
            reward_icon = "ğŸ‰" if reward > 0 else "âš¡" if reward > -1 else "âŒ"
            table.add_row("å½“å‰å¥–åŠ±", f"[{reward_color}]{reward:.3f}[/{reward_color}]", reward_icon)
            
            # æœ€ä½³å¥–åŠ±
            best_color = "bold green" if self.best_reward > 0 else "cyan"
            best_icon = "ğŸ†" if self.best_reward > 0 else "ğŸ¯"
            table.add_row("æœ€ä½³å¥–åŠ±", f"[{best_color}]{self.best_reward:.3f}[/{best_color}]", best_icon)
            
            # å¹³å‡å¥–åŠ±ï¼ˆæœ€è¿‘10è½®ï¼‰
            avg_color = "green" if avg_reward > -1 else "yellow" if avg_reward > -2 else "red"
            table.add_row("å¹³å‡å¥–åŠ±", f"[{avg_color}]{avg_reward:.3f}[/{avg_color}]", "ğŸ“ˆ")
            
            # ã€æ”¹è¿›ã€‘æ™ºèƒ½æˆåŠŸç‡æ˜¾ç¤º
            success_display, success_icon = self._format_success_rate_display(success_stats)
            table.add_row("å­¦ä¹ è¿›å±•", success_display, success_icon)
            
            # è´¨é‡åˆ†æ•°
            if quality_score is not None:
                quality_color = "green" if quality_score > 0.4 else "yellow" if quality_score > 0.3 else "red"
                table.add_row("è´¨é‡åˆ†æ•°", f"[{quality_color}]{quality_score:.3f}[/{quality_color}]", "â­")
            
            # ã€æ”¹è¿›ã€‘æ™ºèƒ½ç³»ç»ŸçŠ¶æ€è¯„ä¼°
            system_status = self._evaluate_system_status(success_stats, avg_reward)
            table.add_row("ç³»ç»ŸçŠ¶æ€", system_status, "ğŸ”§")
            
            # è¿è¡Œæ—¶é—´
            elapsed = time.time() - self.start_time
            if elapsed < 60:
                time_str = f"{elapsed:.0f}ç§’"
            elif elapsed < 3600:
                time_str = f"{elapsed/60:.1f}åˆ†é’Ÿ"
            else:
                time_str = f"{elapsed/3600:.1f}å°æ—¶"
            table.add_row("è¿è¡Œæ—¶é—´", time_str, "â±ï¸")
            
            title = "[bold blue]ğŸš€ ç”µåŠ›ç½‘ç»œåˆ†åŒºè®­ç»ƒç›‘æ§[/bold blue]"
            return Panel(table, title=title, border_style="blue", padding=(1, 2))
            
        except Exception as e:
            # å‡ºé”™æ—¶è¿”å›ç®€å•æ–‡æœ¬
            return f"Episode {episode+1}/{self.total_episodes} | å¥–åŠ±: {reward:.3f} | æœ€ä½³: {self.best_reward:.3f}"

    def _compute_multi_level_success_rate(self) -> Dict[str, Any]:
        """
        è®¡ç®—å¤šå±‚æ¬¡æˆåŠŸç‡æŒ‡æ ‡
        
        Returns:
            åŒ…å«ä¸åŒå±‚æ¬¡æˆåŠŸç‡çš„å­—å…¸
        """
        if not self.episode_rewards:
            return {
                'positive_count': 0,
                'positive_rate': 0.0,
                'improvement_count': 0,
                'improvement_rate': 0.0,
                'learning_count': 0,
                'learning_rate': 0.0,
                'total_episodes': 0
            }
        
        total = len(self.episode_rewards)
        
        # 1. ä¼ ç»Ÿæ­£å¥–åŠ±æˆåŠŸç‡
        positive_count = sum(1 for r in self.episode_rewards if r > 0)
        positive_rate = positive_count / total
        
        # 2. ç›¸å¯¹æ”¹è¿›æˆåŠŸç‡ï¼ˆç›¸æ¯”äºåˆå§‹å‡ è½®çš„è¡¨ç°ï¼‰
        baseline_reward = np.mean(self.episode_rewards[:min(3, total)]) if total >= 3 else min(self.episode_rewards)
        improvement_count = sum(1 for r in self.episode_rewards if r > baseline_reward)
        improvement_rate = improvement_count / total
        
        # 3. å­¦ä¹ è¿›å±•æˆåŠŸç‡ï¼ˆè¶…è¿‡åˆç†é˜ˆå€¼ï¼‰
        # å¯¹äºç”µåŠ›ç½‘ç»œåˆ†åŒºä»»åŠ¡ï¼Œ-1.0æ˜¯ä¸€ä¸ªåˆç†çš„å­¦ä¹ é˜ˆå€¼
        learning_threshold = -1.0
        learning_count = sum(1 for r in self.episode_rewards if r > learning_threshold)
        learning_rate = learning_count / total
        
        return {
            'positive_count': positive_count,
            'positive_rate': positive_rate,
            'improvement_count': improvement_count,
            'improvement_rate': improvement_rate,
            'learning_count': learning_count,
            'learning_rate': learning_rate,
            'total_episodes': total,
            'baseline_reward': baseline_reward,
            'learning_threshold': learning_threshold
        }
    
    def _format_success_rate_display(self, success_stats: Dict[str, Any]) -> Tuple[str, str]:
        """
        æ ¼å¼åŒ–æˆåŠŸç‡æ˜¾ç¤º
        
        Args:
            success_stats: å¤šå±‚æ¬¡æˆåŠŸç‡ç»Ÿè®¡
            
        Returns:
            (æ˜¾ç¤ºæ–‡æœ¬, å›¾æ ‡)
        """
        total = success_stats['total_episodes']
        
        # é€‰æ‹©æœ€æœ‰æ„ä¹‰çš„æˆåŠŸç‡æŒ‡æ ‡è¿›è¡Œæ˜¾ç¤º
        if success_stats['positive_rate'] > 0:
            # æœ‰æ­£å¥–åŠ±æ—¶æ˜¾ç¤ºæ­£å¥–åŠ±ç‡
            count = success_stats['positive_count']
            rate = success_stats['positive_rate']
            color = "bold green"
            label = "æ­£å¥–åŠ±"
            icon = "ğŸ‰"
        elif success_stats['learning_rate'] > 0.3:
            # å­¦ä¹ è¿›å±•è‰¯å¥½æ—¶æ˜¾ç¤ºå­¦ä¹ ç‡
            count = success_stats['learning_count']
            rate = success_stats['learning_rate']
            color = "green"
            label = "å­¦ä¹ è¿›å±•"
            icon = "ğŸ“š"
        elif success_stats['improvement_rate'] > 0.2:
            # æœ‰ç›¸å¯¹æ”¹è¿›æ—¶æ˜¾ç¤ºæ”¹è¿›ç‡
            count = success_stats['improvement_count']
            rate = success_stats['improvement_rate']
            color = "yellow"
            label = "ç›¸å¯¹æ”¹è¿›"
            icon = "ğŸ“ˆ"
        else:
            # æ—©æœŸè®­ç»ƒé˜¶æ®µ
            count = success_stats['learning_count']
            rate = success_stats['learning_rate']
            color = "dim"
            label = "æ—©æœŸå­¦ä¹ "
            icon = "ğŸŒ±"
        
        display = f"[{color}]{count}/{total} ({rate*100:.1f}%) {label}[/{color}]"
        return display, icon
    
    def _evaluate_system_status(self, success_stats: Dict[str, Any], avg_reward: float) -> str:
        """
        æ™ºèƒ½è¯„ä¼°ç³»ç»ŸçŠ¶æ€
        
        Args:
            success_stats: æˆåŠŸç‡ç»Ÿè®¡
            avg_reward: å¹³å‡å¥–åŠ±
            
        Returns:
            çŠ¶æ€æ˜¾ç¤ºæ–‡æœ¬
        """
        positive_rate = success_stats['positive_rate']
        learning_rate = success_stats['learning_rate']
        improvement_rate = success_stats['improvement_rate']
        
        if positive_rate > 0.3:
            return "[bold green]ğŸŸ¢ è®­ç»ƒæˆåŠŸ[/bold green]"
        elif positive_rate > 0.1 or learning_rate > 0.5:
            return "[green]ğŸŸ¡ å­¦ä¹ è‰¯å¥½[/green]"
        elif learning_rate > 0.3 or improvement_rate > 0.4:
            return "[yellow]ğŸŸ  ç¨³æ­¥æ”¹è¿›[/yellow]"
        elif avg_reward > -3.0 and improvement_rate > 0.2:
            return "[yellow]ğŸŸ¡ æ­£åœ¨å­¦ä¹ [/yellow]"
        else:
            return "[red]ğŸ”´ éœ€è¦æ›´å¤šè®­ç»ƒ[/red]"

    def _setup_tensorboard(self):
        """è®¾ç½®TensorBoard"""
        try:
            from torch.utils.tensorboard import SummaryWriter
            log_dir = self.config['logging']['log_dir']
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            tensorboard_writer = SummaryWriter(f"{log_dir}/training_{timestamp}")
            from code.src.rich_output import rich_info
            rich_info(f"TensorBoardæ—¥å¿—ç›®å½•: {log_dir}/training_{timestamp}", show_always=True)
            return tensorboard_writer
        except ImportError:
            from code.src.rich_output import rich_warning
            rich_warning("TensorBoardä¸å¯ç”¨ï¼Œè·³è¿‡TensorBoardæ—¥å¿—")
            self.use_tensorboard = False
            return None
        except Exception as e:
            from code.src.rich_output import rich_warning
            rich_warning(f"TensorBoardåˆå§‹åŒ–å¤±è´¥: {e}")
            self.use_tensorboard = False
            return None

    def log_episode(self, episode: int, reward: float, length: int, info: Dict = None):
        """è®°å½•å•ä¸ªå›åˆ"""
        self.episode_rewards.append(reward)
        self.episode_lengths.append(length)

        if reward > self.best_reward:
            self.best_reward = reward

        # å¦‚æœä½¿ç”¨TUIï¼Œå°†æ›´æ–°æ¨é€åˆ°é˜Ÿåˆ—
        if self.use_tui and self.tui_update_queue:
            from code.src.tui_monitor import TrainingUpdate
            
            avg_reward = np.mean(self.episode_rewards) if self.episode_rewards else 0
            quality_score = info.get('quality_score', 0.5) if info else 0.5
            success_rate = np.mean(self.success_rates) * 100 if self.success_rates else 0.0

            update = TrainingUpdate(
                episode=episode,
                total_episodes=self.total_episodes,
                reward=reward,
                best_reward=self.best_reward,
                avg_reward=avg_reward,
                quality_score=quality_score,
                success_rate=success_rate,
                log_message=f"[{episode+1}/{self.total_episodes}] Reward: {reward:.3f}"
            )
            self.tui_update_queue.put(update)
            # æ³¨é‡Šæ‰æå‰è¿”å›ï¼Œè®©ä»£ç ç»§ç»­æ‰§è¡ŒTensorBoardè®°å½•
            # return # TUIæ¥ç®¡æ˜¾ç¤ºï¼Œç›´æ¥è¿”å›

        # ä½¿ç”¨ç®€å•è¿›åº¦æ¡
        if self.progress_bar:
            # è®¡ç®—ä¸€äº›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            avg_reward = sum(self.episode_rewards[-10:]) / min(len(self.episode_rewards), 10) if self.episode_rewards else reward

            # æ›´æ–°è¿›åº¦æ¡æè¿°
            desc = f"ğŸš€ è®­ç»ƒè¿›åº¦ | å½“å‰: {reward:.3f} | æœ€ä½³: {self.best_reward:.3f} | å¹³å‡: {avg_reward:.3f}"
            self.progress_bar.set_description(desc)
            self.progress_bar.update(1)
        else:
            # å¦‚æœæ²¡æœ‰è¿›åº¦æ¡ï¼Œä½¿ç”¨ç®€å•çš„æ–‡æœ¬è¾“å‡º
            if episode % 100 == 0 or episode == self.total_episodes - 1:
                avg_reward = sum(self.episode_rewards[-10:]) / min(len(self.episode_rewards), 10) if self.episode_rewards else reward
                print(f"Episode {episode+1}/{self.total_episodes} | å¥–åŠ±: {reward:.3f} | æœ€ä½³: {self.best_reward:.3f} | å¹³å‡: {avg_reward:.3f}")

        # è®°å½•é¢å¤–ä¿¡æ¯ - ã€ä¿®å¤ã€‘é€‚é…æ–°ç³»ç»ŸæŒ‡æ ‡åç§°
        if info:
            if 'success' in info:
                self.success_rates.append(1.0 if info['success'] else 0.0)
            # ä¼˜å…ˆä½¿ç”¨æ–°æŒ‡æ ‡åç§°ï¼Œå‘åå…¼å®¹æ—§åç§°
            if 'cv' in info or 'load_cv' in info:
                cv_value = info.get('cv', info.get('load_cv', 0.0))
                self.load_cvs.append(cv_value)
            if 'coupling_ratio' in info or 'coupling_edges' in info:
                coupling_value = info.get('coupling_ratio', info.get('coupling_edges', 0.0))
                self.coupling_edges.append(coupling_value)

        # TensorBoardæ—¥å¿— - é‡æ–°è®¾è®¡çš„åˆ†ç»„è®°å½•
        if self.use_tensorboard and self.tensorboard_writer:
            self._log_tensorboard_metrics(episode, reward, length, info)

    def _log_tensorboard_metrics(self, episode: int, reward: float, length: int, info: dict):
        """
        åˆ†ç»„è®°å½•TensorBoardæŒ‡æ ‡ï¼Œæä¾›æ¸…æ™°çš„å¯è§†åŒ–ç»“æ„

        æŒ‡æ ‡åˆ†ç»„ï¼š
        - Core: æ ¸å¿ƒè®­ç»ƒæŒ‡æ ‡
        - Reward: å¥–åŠ±ç»„ä»¶è¯¦æƒ…
        - Quality: è´¨é‡è¯„ä¼°æŒ‡æ ‡
        - Debug: è°ƒè¯•å’Œè¯Šæ–­ä¿¡æ¯
        """
        if not (self.use_tensorboard and self.tensorboard_writer):
            return

        # === 1. æ ¸å¿ƒè®­ç»ƒæŒ‡æ ‡ (Core Training Metrics) ===
        self.tensorboard_writer.add_scalar('Core/Episode_Reward', reward, episode)
        self.tensorboard_writer.add_scalar('Core/Episode_Length', length, episode)

        if info:
            # æˆåŠŸç‡å’ŒåŸºç¡€çŠ¶æ€
            if 'success' in info:
                self.tensorboard_writer.add_scalar('Core/Success_Rate', float(info['success']), episode)
            if 'step' in info:
                self.tensorboard_writer.add_scalar('Core/Current_Step', info['step'], episode)

        # === 2. å¥–åŠ±ç»„ä»¶è¯¦æƒ… (Reward Components) ===
        if info:
            # ç»ˆå±€å¥–åŠ±ç»„ä»¶
            reward_components = {
                'balance_reward': 'Balance_Component',
                'decoupling_reward': 'Decoupling_Component',
                'power_reward': 'Power_Component',
                'quality_reward': 'Quality_Total',
                'final_reward': 'Final_Total'
            }

            for key, display_name in reward_components.items():
                if key in info and isinstance(info[key], (int, float)):
                    self.tensorboard_writer.add_scalar(f'Reward/{display_name}', info[key], episode)

            # å¥–åŠ±è°ƒèŠ‚å› å­
            adjustment_factors = {
                'threshold_bonus': 'Threshold_Bonus',
                'termination_discount': 'Termination_Discount'
            }

            for key, display_name in adjustment_factors.items():
                if key in info and isinstance(info[key], (int, float)):
                    self.tensorboard_writer.add_scalar(f'Reward/{display_name}', info[key], episode)

        # === 3. è´¨é‡è¯„ä¼°æŒ‡æ ‡ (Quality Assessment) ===
        if info:
            quality_metrics = {
                'quality_score': 'Unified_Quality_Score',
                'plateau_confidence': 'Plateau_Confidence'
            }

            for key, display_name in quality_metrics.items():
                if key in info and isinstance(info[key], (int, float)):
                    self.tensorboard_writer.add_scalar(f'Quality/{display_name}', info[key], episode)

        # === 4. è°ƒè¯•å’Œè¯Šæ–­ä¿¡æ¯ (Debug & Diagnostics) ===
        if info:
            debug_metrics = {
                'reward_mode': 'Current_Reward_Mode'
            }

            for key, display_name in debug_metrics.items():
                if key in info:
                    # å¯¹äºå­—ç¬¦ä¸²ç±»å‹ï¼Œè½¬æ¢ä¸ºæ•°å€¼ç¼–ç 
                    if isinstance(info[key], str):
                        # ç®€å•çš„å­—ç¬¦ä¸²å“ˆå¸Œç¼–ç ç”¨äºå¯è§†åŒ–
                        value = hash(info[key]) % 1000
                        self.tensorboard_writer.add_scalar(f'Debug/{display_name}', value, episode)
                    elif isinstance(info[key], (int, float)):
                        self.tensorboard_writer.add_scalar(f'Debug/{display_name}', info[key], episode)

        # å®šæœŸåˆ·æ–°ä»¥ç¡®ä¿æ•°æ®å†™å…¥
        if episode % 10 == 0:
            self.tensorboard_writer.flush()

    def log_training_step(self, episode: int, actor_loss: float = None,
                         critic_loss: float = None, entropy: float = None):
        """è®°å½•è®­ç»ƒæ­¥éª¤ - ä½¿ç”¨åˆ†ç»„è®°å½•"""
        if actor_loss is not None:
            self.actor_losses.append(actor_loss)
            if self.use_tensorboard and self.tensorboard_writer:
                self.tensorboard_writer.add_scalar('Training/Actor_Loss', actor_loss, episode)

        if critic_loss is not None:
            self.critic_losses.append(critic_loss)
            if self.use_tensorboard and self.tensorboard_writer:
                self.tensorboard_writer.add_scalar('Training/Critic_Loss', critic_loss, episode)

        if entropy is not None:
            self.entropies.append(entropy)
            if self.use_tensorboard and self.tensorboard_writer:
                self.tensorboard_writer.add_scalar('Training/Policy_Entropy', entropy, episode)

        # TensorBoardæ•°æ®ä¼šåœ¨_log_tensorboard_metricsä¸­å®šæœŸåˆ·æ–°



    def get_statistics(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        if not self.episode_rewards:
            return {}

        stats = {
            'total_episodes': len(self.episode_rewards),
            'best_reward': self.best_reward,
            'mean_reward': np.mean(self.episode_rewards),
            'std_reward': np.std(self.episode_rewards),
            'mean_length': np.mean(self.episode_lengths),
            'training_time': time.time() - self.start_time
        }

        if self.success_rates:
            stats['success_rate'] = np.mean(self.success_rates)
        if self.load_cvs:
            stats['mean_load_cv'] = np.mean(self.load_cvs)
        if self.actor_losses:
            stats['mean_actor_loss'] = np.mean(self.actor_losses)
        if self.critic_losses:
            stats['mean_critic_loss'] = np.mean(self.critic_losses)

        return stats

    def set_training_mode(self, training: bool = True):
        """
        è®¾ç½®è®­ç»ƒ/è¯„ä¼°æ¨¡å¼ï¼Œæ§åˆ¶TensorBoardæŒ‡æ ‡è®°å½•

        Args:
            training: Trueä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆåªè®°å½•è®­ç»ƒç›¸å…³æŒ‡æ ‡ï¼‰ï¼ŒFalseä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆè®°å½•æ‰€æœ‰æŒ‡æ ‡ï¼‰
        """
        self.training_mode = training

    # HTMLä»ªè¡¨æ¿ç”ŸæˆåŠŸèƒ½å·²ç§»åŠ¨åˆ°test.pyä¸­ç”¨äºæ€§èƒ½åˆ†æ

    def close(self):
        """å…³é—­æ—¥å¿—è®°å½•å™¨"""
        # å…³é—­è¿›åº¦æ¡
        if hasattr(self, 'progress_bar') and self.progress_bar:
            self.progress_bar.close()

        if self.tensorboard_writer:
            self.tensorboard_writer.close()


class UnifiedTrainer:
    """ç»Ÿä¸€è®­ç»ƒå™¨"""

    def __init__(self, agent, env, config, gym_env=None):
        self.agent = agent
        self.env = env
        self.config = config
        self.gym_env = gym_env  # åœºæ™¯ç”Ÿæˆç¯å¢ƒåŒ…è£…å™¨ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
        # å¤„ç†è®¾å¤‡é…ç½®
        device_config = config['system']['device']
        if device_config == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device_config)
        self.logger = None # å°†åœ¨trainæ–¹æ³•ä¸­åˆå§‹åŒ–
        self.tui_thread = None

    def train(self, num_episodes: int, max_steps_per_episode: int, update_interval: int = 10):
        """è®­ç»ƒæ™ºèƒ½ä½“"""
        self.logger = TrainingLogger(self.config, num_episodes)
        
        # å¦‚æœå¯ç”¨TUIï¼Œåœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­è¿è¡Œå®ƒ
        if self.logger.use_tui:
            self.tui_thread = threading.Thread(target=self.logger.tui_app.run, daemon=True)
            self.tui_thread.start()
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ä»¥ç¡®ä¿TUIå¯åŠ¨
            time.sleep(1)

        from code.src.rich_output import rich_info
        if not self.config.get('debug', {}).get('training_output', {}).get('only_show_errors', True):
            rich_info(f"TensorBoard: {'å·²å¯ç”¨' if self.logger.use_tensorboard else 'å·²ç¦ç”¨'}")
            rich_info(f"æŒ‡æ ‡ä¿å­˜é—´éš”: {self.logger.metrics_save_interval} å›åˆ")

        for episode in range(num_episodes):
            # å¦‚æœä½¿ç”¨åœºæ™¯ç”Ÿæˆç¯å¢ƒï¼Œéœ€è¦é‡ç½®Gymç¯å¢ƒä»¥ç”Ÿæˆæ–°åœºæ™¯
            if self.gym_env is not None:
                obs_array, info = self.gym_env.reset()
                # æ›´æ–°å†…éƒ¨ç¯å¢ƒå¼•ç”¨ï¼ˆå› ä¸ºæ¯æ¬¡é‡ç½®éƒ½ä¼šåˆ›å»ºæ–°çš„å†…éƒ¨ç¯å¢ƒï¼‰
                self.env = self.gym_env.internal_env
                state, _ = self.env.reset()  # é‡ç½®å†…éƒ¨ç¯å¢ƒçŠ¶æ€
            else:
                state, _ = self.env.reset()  # æ ‡å‡†ç¯å¢ƒé‡ç½®

            episode_reward = 0
            episode_length = 0
            episode_info = {}

            for step in range(max_steps_per_episode):
                # é€‰æ‹©åŠ¨ä½œ
                action, log_prob, value = self.agent.select_action(state)

                if action is None:  # æ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œ
                    break

                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, terminated, truncated, info = self.env.step(action)
                done = terminated or truncated

                # å­˜å‚¨ç»éªŒ
                self.agent.store_experience(state, action, reward, log_prob, value, done)

                episode_reward += reward
                episode_length += 1
                state = next_state

                # æ”¶é›†å›åˆä¿¡æ¯
                if info:
                    episode_info.update(info)

                if done:
                    break

            # è®°å½•åˆ°loggerï¼ˆåŒ…å«è¯¦ç»†ä¿¡æ¯ï¼‰
            self.logger.log_episode(episode, episode_reward, episode_length, episode_info)

            # ã€æ–°å¢ã€‘éªŒè¯æŒ‡æ ‡å®Œæ•´æ€§ï¼Œç¡®ä¿ä¿®å¤ç”Ÿæ•ˆ - ç®€åŒ–ç‰ˆ
            if episode < 3 and not self.config.get('debug', {}).get('training_output', {}).get('only_show_errors', True):
                self._validate_metrics_integrity(episode_info, episode)

            # æ›´æ–°æ™ºèƒ½ä½“å¹¶è®°å½•è®­ç»ƒæŒ‡æ ‡
            if episode % update_interval == 0 and episode > 0:
                try:
                    # --- ä½¿ç”¨ try-except åŒ…è£¹ agent.update() ---
                    training_stats = self.agent.update()
                    if training_stats:
                        self.logger.log_training_step(
                            episode,
                            training_stats.get('actor_loss'),
                            training_stats.get('critic_loss'),
                            training_stats.get('entropy')
                        )

                except RuntimeError as e:
                    if "[NaNGuard]" in str(e) or "NaN" in str(e):
                        print(f"\nâŒ [NaN Dumper] æ•è·åˆ°è‡´å‘½é”™è¯¯: {e}")

                        dump_dir = "diagnostics"
                        os.makedirs(dump_dir, exist_ok=True)
                        dump_path = os.path.join(dump_dir, f"nan_dump_episode_{episode}.pt")

                        print(f"  ... æ­£åœ¨ä¿å­˜è¯Šæ–­ä¿¡æ¯åˆ°: {dump_path}")

                        # æ”¶é›†è§¦å‘å¼‚å¸¸çš„æ‰¹é‡æ•°æ®
                        batch_data = {
                            'states': self.agent.memory.states,
                            'actions': self.agent.memory.actions,
                            'logprobs': self.agent.memory.logprobs,
                            'rewards': self.agent.memory.rewards,
                            'is_terminals': self.agent.memory.is_terminals,
                        }

                        torch.save({
                            "error_message": str(e),
                            "episode": episode,
                            "actor_state_dict": self.agent.actor.state_dict(),
                            "critic_state_dict": self.agent.critic.state_dict(),
                            "actor_optimizer_state_dict": self.agent.actor_optimizer.state_dict(),
                            "critic_optimizer_state_dict": self.agent.critic_optimizer.state_dict(),
                            "triggering_batch": batch_data,
                        }, dump_path)

                        print("  ... ä¿å­˜å®Œæˆã€‚æ­£åœ¨ä¸­æ–­è®­ç»ƒã€‚")
                        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä»¥æ­£å¸¸é€€å‡ºè®­ç»ƒå¾ªç¯
                        raise e
                    else:
                        # å¦‚æœæ˜¯å…¶ä»–è¿è¡Œæ—¶é”™è¯¯ï¼Œåˆ™ç›´æ¥æŠ›å‡º
                        raise

            # ä¿å­˜ä¸­é—´ç»“æœ
            if episode % self.logger.metrics_save_interval == 0 and episode > 0:
                self._save_intermediate_results(episode)

        # ç®€æ´çš„è®­ç»ƒå®Œæˆç»Ÿè®¡
        final_stats = self.logger.get_statistics()
        positive_rewards = sum(1 for r in self.logger.episode_rewards if r > 0)
        total_episodes = final_stats.get('total_episodes', 0)
        best_reward = final_stats.get('best_reward', 0)
        mean_reward = final_stats.get('mean_reward', 0)

        # ä½¿ç”¨Richåˆ›å»ºç®€æ´çš„å®Œæˆè¡¨æ ¼
        try:
            from rich.console import Console
            from rich.table import Table
            from rich.panel import Panel
            
            console = Console()
            table = Table.grid(padding=1)
            table.add_column("æŒ‡æ ‡", style="bold cyan")
            table.add_column("ç»“æœ", style="green", justify="right")
            
            table.add_row("å›åˆæ•°", f"{total_episodes:,}")
            table.add_row("æœ€ä½³å¥–åŠ±", f"{best_reward:.4f}")
            table.add_row("å¹³å‡å¥–åŠ±", f"{mean_reward:.4f}")
            
            # ã€æ”¹è¿›ã€‘ä½¿ç”¨æ™ºèƒ½æˆåŠŸç‡ç»Ÿè®¡
            if hasattr(self.logger, '_compute_multi_level_success_rate'):
                success_stats = self.logger._compute_multi_level_success_rate()
                if success_stats['positive_rate'] > 0:
                    success_text = f"æ­£å¥–åŠ±: {success_stats['positive_count']}/{total_episodes} ({success_stats['positive_rate']*100:.1f}%)"
                elif success_stats['learning_rate'] > 0.3:
                    success_text = f"å­¦ä¹ è¿›å±•: {success_stats['learning_count']}/{total_episodes} ({success_stats['learning_rate']*100:.1f}%)"
                else:
                    success_text = f"ç›¸å¯¹æ”¹è¿›: {success_stats['improvement_count']}/{total_episodes} ({success_stats['improvement_rate']*100:.1f}%)"
            else:
                # å¤‡ç”¨ä¼ ç»Ÿè®¡ç®—
                success_text = f"{positive_rewards}/{total_episodes} ({positive_rewards/total_episodes*100:.1f}%)"
            
            table.add_row("å­¦ä¹ æ•ˆæœ", success_text)
            table.add_row("è®­ç»ƒæ—¶é—´", f"{final_stats.get('training_time', 0)/60:.1f} åˆ†é’Ÿ")
            
            # ã€æ”¹è¿›ã€‘æ™ºèƒ½ç³»ç»ŸçŠ¶æ€è¯„ä¼°
            if hasattr(self.logger, '_compute_multi_level_success_rate'):
                success_stats = self.logger._compute_multi_level_success_rate()
                if success_stats['positive_rate'] > 0.3:
                    status = "[bold green]ğŸ‰ è®­ç»ƒæˆåŠŸ[/bold green]"
                elif success_stats['positive_rate'] > 0.1 or success_stats['learning_rate'] > 0.5:
                    status = "[green]âœ… å­¦ä¹ è‰¯å¥½[/green]"
                elif success_stats['learning_rate'] > 0.3 or success_stats['improvement_rate'] > 0.4:
                    status = "[yellow]ğŸŸ  ç¨³æ­¥æ”¹è¿›[/yellow]"
                elif mean_reward > -3.0 and success_stats['improvement_rate'] > 0.2:
                    status = "[yellow]ğŸŸ¡ æ­£åœ¨å­¦ä¹ [/yellow]"
                else:
                    status = "[red]âš ï¸ éœ€è¦æ›´å¤šè®­ç»ƒ[/red]"
            else:
                # å¤‡ç”¨ä¼ ç»Ÿè¯„ä¼°
                if best_reward > 0:
                    status = "[bold green]ğŸ‰ é‡æ„æˆåŠŸ[/bold green]"
                elif positive_rewards > 0:
                    status = "[yellow]âœ… é‡æ„æœ‰æ•ˆ[/yellow]"
                else:
                    status = "[red]âš ï¸ éœ€è¦æ›´å¤šè®­ç»ƒ[/red]"
            table.add_row("ç³»ç»ŸçŠ¶æ€", status)
            
            panel = Panel(table, title="[bold blue]ğŸ¯ è®­ç»ƒå®Œæˆç»Ÿè®¡[/bold blue]", border_style="blue")
            console.print(panel)
            
        except ImportError:
            # å¤‡ç”¨ç®€å•è¾“å‡º
            print(f"\nğŸ¯ è®­ç»ƒå®Œæˆ: {total_episodes}å›åˆ, æœ€ä½³å¥–åŠ±: {best_reward:.4f}, æˆåŠŸç‡: {positive_rewards/total_episodes*100:.1f}%")

        return {
            'episode_rewards': self.logger.episode_rewards,
            'episode_lengths': self.logger.episode_lengths,
            'training_stats': final_stats
        }

    def _validate_metrics_integrity(self, episode_info: Dict[str, Any], episode: int):
        """
        éªŒè¯æŒ‡æ ‡å®Œæ•´æ€§ï¼Œç¡®ä¿ä¿®å¤ç”Ÿæ•ˆ

        Args:
            episode_info: episodeä¿¡æ¯å­—å…¸
            episode: episodeç¼–å·
        """
        try:
            # æ£€æŸ¥å…³é”®æŒ‡æ ‡æ˜¯å¦ä¸å†æ˜¯å›ºå®šå€¼
            metrics = episode_info.get('metrics', {})

            # æ£€æŸ¥CVæŒ‡æ ‡
            cv_value = metrics.get('cv', metrics.get('load_cv', 1.0))
            if cv_value == 1.0 and episode > 1:
                from code.src.rich_output import rich_warning
                rich_warning(f"Episode {episode}: CVæŒ‡æ ‡ä»ä¸ºå›ºå®šå€¼1.0ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")

            # è¾“å‡ºæŒ‡æ ‡æ‘˜è¦ï¼ˆä»…å‰3ä¸ªepisodeï¼Œæ›´ç®€æ´ï¼‰
            if episode <= 2:
                coupling_ratio = metrics.get('coupling_ratio', 1.0)
                connectivity = metrics.get('connectivity', 0.0)
                from code.src.rich_output import rich_debug
                rich_debug(f"Episode {episode} æŒ‡æ ‡: CV={cv_value:.3f}, Coupling={coupling_ratio:.3f}, Conn={connectivity:.3f}")

        except Exception as e:
            from code.src.rich_output import rich_warning
            rich_warning(f"Episode {episode} æŒ‡æ ‡éªŒè¯å¤±è´¥: {e}")

    def _save_intermediate_results(self, episode: int):
        """ä¿å­˜ä¸­é—´è®­ç»ƒç»“æœ"""
        try:
            stats = self.logger.get_statistics()
            checkpoint_dir = Path(self.config['logging']['checkpoint_dir'])
            checkpoint_dir.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats_file = checkpoint_dir / f"training_stats_episode_{episode}.json"
            with open(stats_file, 'w') as f:
                json.dump(stats, f, indent=2)

            print(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: episode {episode}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ä¸­é—´ç»“æœå¤±è´¥: {e}")

    def evaluate(self, num_episodes: int = 10):
        """è¯„ä¼°æ™ºèƒ½ä½“ - ä½¿ç”¨åˆç†çš„æˆåŠŸæ ‡å‡†"""
        print(f"ğŸ” è¯„ä¼°æ™ºèƒ½ä½“ {num_episodes} è½®...")

        eval_rewards = []
        success_count = 0

        for episode in range(num_episodes):
            state, _ = self.env.reset()  # è§£åŒ…å…ƒç»„
            episode_reward = 0
            episode_info = {}

            for step in range(200):  # æœ€å¤§æ­¥æ•°
                action, _, _ = self.agent.select_action(state, training=False)

                if action is None:  # æ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œ
                    break

                state, reward, terminated, truncated, info = self.env.step(action)
                done = terminated or truncated
                episode_reward += reward
                
                # æ”¶é›†æœ€åçš„ç¯å¢ƒä¿¡æ¯
                if info:
                    episode_info.update(info)

                if done:
                    break

            eval_rewards.append(episode_reward)
            
            # ã€æ”¹è¿›ã€‘ä½¿ç”¨åˆç†çš„æˆåŠŸæ ‡å‡†è¯„ä¼°ï¼Œè€Œä¸æ˜¯ä¾èµ–ç¯å¢ƒsuccessæ ‡å¿—
            is_success = self._evaluate_episode_success(episode_reward, episode_info)
            if is_success:
                success_count += 1

        return {
            'avg_reward': np.mean(eval_rewards),
            'success_rate': success_count / num_episodes,
            'rewards': eval_rewards
        }



    def _evaluate_episode_success(self, episode_reward: float, episode_info: Dict[str, Any]) -> bool:
        """
        è¯„ä¼°å•ä¸ªepisodeæ˜¯å¦æˆåŠŸ
        
        Args:
            episode_reward: episodeæ€»å¥–åŠ±
            episode_info: episodeä¿¡æ¯ï¼ˆå¯èƒ½åŒ…å«æŒ‡æ ‡ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # æˆåŠŸæ ‡å‡†1: å¥–åŠ±è¶…è¿‡åˆç†é˜ˆå€¼
            if episode_reward > -1.0:  # å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œ-1.0æ˜¯ä¸€ä¸ªåˆç†çš„æˆåŠŸé˜ˆå€¼
                return True
            
            # æˆåŠŸæ ‡å‡†2: åŸºäºè´¨é‡æŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if episode_info and 'metrics' in episode_info:
                metrics = episode_info['metrics']
                
                # æ£€æŸ¥è´Ÿè½½å¹³è¡¡ï¼ˆCV < 0.5ï¼‰
                cv = metrics.get('cv', metrics.get('load_cv', 1.0))
                if cv < 0.5:
                    return True
                
                # æ£€æŸ¥è¿é€šæ€§ï¼ˆconnectivity > 0.9ï¼‰
                connectivity = metrics.get('connectivity', 0.0)
                if connectivity > 0.9:
                    return True
                
                # æ£€æŸ¥è€¦åˆæ¯”ç‡ï¼ˆcoupling_ratio < 0.3ï¼‰
                coupling_ratio = metrics.get('coupling_ratio', 1.0)
                if coupling_ratio < 0.3:
                    return True
            
            # æˆåŠŸæ ‡å‡†3: åŸºäºè´¨é‡åˆ†æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if episode_info and 'reward_components' in episode_info:
                components = episode_info['reward_components']
                quality_score = components.get('quality_score', 0.0)
                if quality_score > 0.4:  # è´¨é‡åˆ†æ•°è¶…è¿‡0.4è®¤ä¸ºæˆåŠŸ
                    return True
            
            # å¦‚æœéƒ½ä¸æ»¡è¶³ï¼Œä½†å¥–åŠ±æœ‰æ˜¾è‘—æ”¹è¿›ï¼ˆç›¸æ¯”-5.0åŸºçº¿ï¼‰
            if episode_reward > -2.5:  # ç›¸æ¯”å¾ˆå·®çš„åŸºçº¿æœ‰æ˜¾è‘—æ”¹è¿›
                return True
                
            return False
            
        except Exception as e:
            # å¦‚æœè¯„ä¼°å‡ºé”™ï¼Œä¿å®ˆåœ°æ ¹æ®å¥–åŠ±åˆ¤æ–­
            return episode_reward > -1.5



    def close(self):
        """æ¸…ç†èµ„æº"""
        if self.logger:
            self.logger.close()


def create_environment_from_config(config: Dict, hetero_data: HeteroData, node_embeddings: Dict, attention_weights: Dict, mpc_data: Dict, device: torch.device, is_normalized: bool = True) -> Tuple[PowerGridPartitioningEnv, Optional[gym.Env]]:
    """æ ¹æ®é…ç½®åˆ›å»ºç¯å¢ƒå®ä¾‹ï¼Œå¦‚æœéœ€è¦åˆ™ä½¿ç”¨GymåŒ…è£…å™¨ã€‚"""
    should_use_gym_wrapper = config.get('scenario_generation', {}).get('enabled', True) or \
                             config.get('parallel_training', {}).get('enabled', False)

    if should_use_gym_wrapper:
        from code.src.rl.gym_wrapper import PowerGridPartitionGymEnv
        config_copy = config.copy()
        config_copy['system']['device'] = str(device)
        
        gym_env = PowerGridPartitionGymEnv(
            base_case_data=mpc_data,
            config=config_copy,
            use_scenario_generator=config.get('scenario_generation', {}).get('enabled', True),
            scenario_seed=config['system']['seed']
        )
        _, _ = gym_env.reset()
        env = gym_env.internal_env
        return env, gym_env
    
    # åˆ›å»ºæ ‡å‡†ç¯å¢ƒ
    env_config = config['environment']
    env = PowerGridPartitioningEnv(
        hetero_data,
        node_embeddings=node_embeddings,
        num_partitions=env_config['num_partitions'],
        reward_weights=env_config.get('reward_weights', {}),
        max_steps=env_config['max_steps'],
        device=device,
        attention_weights=attention_weights,
        config=config,
        is_normalized=is_normalized
    )
    return env, None


if __name__ == "__main__":
    import argparse, yaml, copy, pprint, sys
    
    parser = argparse.ArgumentParser(description="RL Trainer for Power Grid Partitioning")
    parser.add_argument("--mode", default="fast", help="é…ç½®é¢„è®¾åç§° (fast / full / ieee118 ç­‰)")
    parser.add_argument("--run", action="store_true", help="æ‰§è¡Œå®Œæ•´è®­ç»ƒæµç¨‹ã€‚è‹¥çœç•¥åˆ™ä»…æ‰“å°åˆå¹¶åçš„é…ç½®")
    args = parser.parse_args()

    # 1. è¯»å–é…ç½®æ–‡ä»¶
    cfg_file = Path("config.yaml")
    if not cfg_file.exists():
        safe_print("âŒ æ‰¾ä¸åˆ° config.yaml ï¼Œè¯·æ£€æŸ¥é¡¹ç›®ç›®å½•")
        sys.exit(1)
    with open(cfg_file, "r", encoding="utf-8") as f:
        base_cfg = yaml.safe_load(f)

    # 2. é€’å½’åˆå¹¶é¢„è®¾
    def deep_update(dest: Dict[str, Any], src: Dict[str, Any]):
        for k, v in src.items():
            if isinstance(v, dict) and k in dest:
                deep_update(dest[k], v)
            else:
                dest[k] = copy.deepcopy(v)

    if args.mode in base_cfg:
        deep_update(base_cfg, base_cfg[args.mode])
    else:
        safe_print(f"âš ï¸ æœªæ‰¾åˆ°åä¸º '{args.mode}' çš„é¢„è®¾ï¼Œä½¿ç”¨åŸºç¡€é…ç½®")

    if not args.run:
        safe_print("ğŸš€ é…ç½®å·²åŠ è½½ï¼ˆæœªå¯åŠ¨è®­ç»ƒï¼Œæ·»åŠ  --run å¯å¼€å§‹è®­ç»ƒï¼‰\nâ”€" * 40)
        pprint.pp(base_cfg)
        sys.exit(0)

    # 3. === æ­£å¼è®­ç»ƒç®¡é“ ===
    safe_print("ğŸš€ å¼€å§‹è®­ç»ƒæµç¨‹ ...")

    # 3.1 è®¾å¤‡
    device_str = base_cfg['system'].get('device', 'auto')
    if device_str == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(device_str)

    # 3.2 æ•°æ®åŠ è½½
    mpc_data = load_power_grid_data(base_cfg['data']['case_name'])

    # 3.3 å›¾å¤„ç†
    from code.src.data_processing import PowerGridDataProcessor
    processor = PowerGridDataProcessor(
        normalize=base_cfg['data'].get('normalize', True),
        cache_dir=base_cfg['data'].get('cache_dir', 'data/cache')
    )
    hetero_data = processor.graph_from_mpc(mpc_data, config=base_cfg)

    # 3.4 ç¼–ç å™¨ (GAT)
    encoder_cfg = base_cfg.get('gat', {})
    encoder = create_production_encoder(hetero_data, encoder_cfg).to(device)
    hetero_data = hetero_data.to(device)

    # é¢„è®¡ç®—èŠ‚ç‚¹åµŒå…¥ & æ³¨æ„åŠ›æƒé‡
    with torch.no_grad():
        node_embeddings_dict, attention_weights = encoder.encode_nodes_with_attention(hetero_data, base_cfg)

    # å°† node_embeddings_dict è½¬ä¸ºåŒä¸€ç»´åº¦å¼ é‡å­—å…¸ï¼ˆç¯å¢ƒå†…éƒ¨æŒ‰ node_type é”®è®¿é—®ï¼‰

    # 3.5 åˆ›å»ºç¯å¢ƒ
    env, gym_env = create_environment_from_config(
        base_cfg, hetero_data, node_embeddings_dict, attention_weights, mpc_data, device,
        is_normalized=base_cfg['data'].get('normalize', True)
    )

    # 3.6 åˆ›å»ºæ™ºèƒ½ä½“ï¼ˆéœ€å…ˆè·å–å®é™…stateç»´åº¦ï¼‰
    # å…ˆresetä¸€æ¬¡ç¯å¢ƒä»¥æ‹¿åˆ°ç¤ºä¾‹state
    init_state, _ = env.reset()
    node_emb_dim = init_state['node_embeddings'].shape[1]
    region_emb_dim = init_state['region_embeddings'].shape[1]

    agent = PPOAgent(
        node_embedding_dim=node_emb_dim,
        region_embedding_dim=region_emb_dim,
        num_partitions=base_cfg['environment']['num_partitions'],
        agent_config=base_cfg.get('agent', {}),
        device=device
    )

     # 3.7 è®­ç»ƒå™¨
    trainer = UnifiedTrainer(agent, env, base_cfg, gym_env=gym_env)

    train_cfg = base_cfg['training']
    trainer.train(
        num_episodes=train_cfg['num_episodes'],
        max_steps_per_episode=train_cfg['max_steps_per_episode'],
        update_interval=train_cfg.get('update_interval', 10)
    )

    safe_print("ğŸ‰ è®­ç»ƒæµç¨‹ç»“æŸã€‚")
    
