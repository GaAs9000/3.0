#!/usr/bin/env python3
"""
ç”µåŠ›ç½‘ç»œåˆ†åŒºå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å—

ä¸“æ³¨äºæ¨¡å‹è®­ç»ƒåŠŸèƒ½ï¼š
- é…ç½®åŠ è½½å’ŒéªŒè¯
- æ•°æ®å¤„ç†å’Œç¯å¢ƒåˆ›å»º
- æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–
- åŸºç¡€å¯è§†åŒ–å’Œç»“æœä¿å­˜
- è®­ç»ƒç›‘æ§å’Œæ—¥å¿—è®°å½•
"""

import torch
import numpy as np
import argparse
import yaml
import os
import sys
import time
import json
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from collections import deque
from tqdm import tqdm
import queue
import threading

# æ·»åŠ code/srcåˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / 'code' / 'src'))
# æ·»åŠ codeåˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥baseline
sys.path.append(str(Path(__file__).parent / 'code'))

# --- å…¨å±€ NaN/Inf å¼‚å¸¸æ£€æµ‹å¼€å…³ ---
torch.autograd.set_detect_anomaly(False)
warnings.filterwarnings("error", message=".*NaN.*", category=RuntimeWarning)
warnings.filterwarnings('ignore', category=UserWarning)

def safe_print(message: str):
    """å®‰å…¨çš„Unicodeæ‰“å°å‡½æ•°ï¼Œé¿å…ç¼–ç é”™è¯¯"""
    try:
        print(message)
    except UnicodeEncodeError:
        # ç§»é™¤Unicodeå­—ç¬¦ï¼Œä½¿ç”¨ASCIIæ›¿ä»£
        ascii_message = message.encode('ascii', 'replace').decode('ascii')
        print(ascii_message)

# å¯¼å…¥ç”µåŠ›ç³»ç»Ÿæ•°æ®åŠ è½½å‡½æ•°
try:
    from scipy.io import loadmat
except ImportError:
    print("âš ï¸ è­¦å‘Š: scipyæœªå®‰è£…ï¼Œæ— æ³•åŠ è½½MATPOWERæ–‡ä»¶")

try:
    import pandapower as pp
    import pandapower.networks as pn
    _pandapower_available = True
except ImportError:
    _pandapower_available = False
    print("âš ï¸ è­¦å‘Š: pandapoweræœªå®‰è£…ï¼Œæ— æ³•åŠ è½½IEEEæ ‡å‡†æµ‹è¯•ç³»ç»Ÿ")


def check_dependencies():
    """æ£€æŸ¥å¯é€‰ä¾èµ–"""
    deps = {
        'stable_baselines3': False,
        'use_tensorboard': False,
        'plotly': False,
        'networkx': False
    }

    try:
        import stable_baselines3
        deps['stable_baselines3'] = True
    except ImportError:
        pass

    try:
        from torch.utils.tensorboard import SummaryWriter
        deps['use_tensorboard'] = True
    except ImportError:
        pass

    try:
        import plotly.graph_objects as go
        deps['plotly'] = True
    except ImportError:
        pass

    try:
        import networkx as nx
        deps['networkx'] = True
    except ImportError:
        pass

    return deps


def load_power_grid_data(case_name: str) -> Dict:
    """
    åŠ è½½ç”µåŠ›ç½‘ç»œæ•°æ®

    Args:
        case_name: æ¡ˆä¾‹åç§° (ieee14, ieee30, ieee57, ieee118)

    Returns:
        MATPOWERæ ¼å¼çš„ç”µç½‘æ•°æ®å­—å…¸
    """
    # é¦–å…ˆå°è¯•ä»PandaPoweråŠ è½½
    if _pandapower_available:
        try:
            return load_from_pandapower(case_name)
        except Exception as e:
            print(f"âš ï¸ PandaPoweråŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°å†…ç½®æ•°æ®...")

    # å›é€€åˆ°å†…ç½®çš„IEEEæ ‡å‡†æµ‹è¯•ç³»ç»Ÿæ•°æ®
    ieee_cases = {
        'ieee14': {
            'baseMVA': 100.0,
            'bus': np.array([
                [1, 3, 0, 0, 0, 0, 1, 1.06, 0, 345, 1, 1.06, 0.94],
                [2, 2, 21.7, 12.7, 0, 0, 1, 1.045, -4.98, 345, 1, 1.06, 0.94],
                [3, 2, 94.2, 19, 0, 0, 1, 1.01, -12.72, 345, 1, 1.06, 0.94],
                [4, 1, 47.8, -3.9, 0, 0, 1, 1.019, -10.33, 345, 1, 1.06, 0.94],
                [5, 1, 7.6, 1.6, 0, 0, 1, 1.02, -8.78, 345, 1, 1.06, 0.94],
                [6, 2, 11.2, 7.5, 0, 12.2, 1, 1.07, -14.22, 345, 1, 1.06, 0.94],
                [7, 1, 0, 0, 0, 0, 1, 1.062, -13.37, 345, 1, 1.06, 0.94],
                [8, 2, 0, 0, 0, 17.4, 1, 1.09, -13.36, 345, 1, 1.06, 0.94],
                [9, 1, 29.5, 16.6, 0, 0, 1, 1.056, -14.94, 345, 1, 1.06, 0.94],
                [10, 1, 9, 5.8, 0, 0, 1, 1.051, -15.1, 345, 1, 1.06, 0.94],
                [11, 1, 3.5, 1.8, 0, 0, 1, 1.057, -14.79, 345, 1, 1.06, 0.94],
                [12, 1, 6.1, 1.6, 0, 0, 1, 1.055, -15.07, 345, 1, 1.06, 0.94],
                [13, 1, 13.5, 5.8, 0, 0, 1, 1.05, -15.16, 345, 1, 1.06, 0.94],
                [14, 1, 14.9, 5, 0, 0, 1, 1.036, -16.04, 345, 1, 1.06, 0.94]
            ]),
            'branch': np.array([
                [1, 2, 0.01938, 0.05917, 0.0528, 0, 0, 0, 0, 0, 1, -360, 360],
                [1, 5, 0.05403, 0.22304, 0.0492, 0, 0, 0, 0, 0, 1, -360, 360],
                [2, 3, 0.04699, 0.19797, 0.0438, 0, 0, 0, 0, 0, 1, -360, 360],
                [2, 4, 0.05811, 0.17632, 0.034, 0, 0, 0, 0, 0, 1, -360, 360],
                [2, 5, 0.05695, 0.17388, 0.0346, 0, 0, 0, 0, 0, 1, -360, 360],
                [3, 4, 0.06701, 0.17103, 0.0128, 0, 0, 0, 0, 0, 1, -360, 360],
                [4, 5, 0.01335, 0.04211, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [4, 7, 0, 0.20912, 0, 0.978, 0, 0, 0, 0, 1, -360, 360],
                [4, 9, 0, 0.55618, 0, 0.969, 0, 0, 0, 0, 1, -360, 360],
                [5, 6, 0, 0.25202, 0, 0.932, 0, 0, 0, 0, 1, -360, 360],
                [6, 11, 0.09498, 0.1989, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [6, 12, 0.12291, 0.25581, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [6, 13, 0.06615, 0.13027, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [7, 8, 0, 0.17615, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [7, 9, 0, 0.11001, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [9, 10, 0.03181, 0.0845, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [9, 14, 0.12711, 0.27038, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [10, 11, 0.08205, 0.19207, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [12, 13, 0.22092, 0.19988, 0, 0, 0, 0, 0, 0, 1, -360, 360],
                [13, 14, 0.17093, 0.34802, 0, 0, 0, 0, 0, 0, 1, -360, 360]
            ])
        }
    }

    if case_name in ieee_cases:
        return ieee_cases[case_name]
    else:
        # å¯¹äºå…¶ä»–æ¡ˆä¾‹ï¼Œè¿”å›é»˜è®¤çš„IEEE14æ•°æ®
        print(f"âš ï¸ æ¡ˆä¾‹ {case_name} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨IEEE14é»˜è®¤æ•°æ®")
        return ieee_cases['ieee14']


def load_from_pandapower(case_name: str) -> Dict:
    """
    ä»PandaPoweråŠ è½½IEEEæ ‡å‡†æµ‹è¯•ç³»ç»Ÿ

    Args:
        case_name: æ¡ˆä¾‹åç§° (ieee14, ieee30, ieee57, ieee118)

    Returns:
        MATPOWERæ ¼å¼çš„ç”µç½‘æ•°æ®å­—å…¸
    """
    print(f"ğŸ”Œ ä»PandaPoweråŠ è½½ {case_name.upper()} æ•°æ®...")

    # æ˜ å°„æ¡ˆä¾‹åç§°åˆ°PandaPowerå‡½æ•°
    case_mapping = {
        'ieee14': pn.case14,
        'ieee30': pn.case30,
        'ieee57': pn.case57,
        'ieee118': pn.case118
    }

    if case_name not in case_mapping:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¡ˆä¾‹: {case_name}")

    # åŠ è½½PandaPowerç½‘ç»œ
    net = case_mapping[case_name]()
    from code.src.rich_output import rich_success
    rich_success(f"æˆåŠŸåŠ è½½ {case_name.upper()}: {len(net.bus)} èŠ‚ç‚¹, {len(net.line)} çº¿è·¯")

    # è½¬æ¢ä¸ºMATPOWERæ ¼å¼
    mpc = convert_pandapower_to_matpower(net)

    return mpc


def convert_pandapower_to_matpower(net) -> Dict:
    """
    å°†PandaPowerç½‘ç»œè½¬æ¢ä¸ºMATPOWERæ ¼å¼

    Args:
        net: PandaPowerç½‘ç»œå¯¹è±¡

    Returns:
        MATPOWERæ ¼å¼çš„ç”µç½‘æ•°æ®å­—å…¸
    """
    # åŸºç¡€MVA
    baseMVA = net.sn_mva

    # èŠ‚ç‚¹æ•°æ®è½¬æ¢
    bus_data = []
    for idx, bus in net.bus.iterrows():
        # MATPOWER busæ ¼å¼: [bus_i, type, Pd, Qd, Gs, Bs, area, Vm, Va, baseKV, zone, Vmax, Vmin]
        bus_type = 1  # PQèŠ‚ç‚¹

        # æ£€æŸ¥æ˜¯å¦ä¸ºå‘ç”µæœºèŠ‚ç‚¹
        if idx in net.gen.bus.values:
            gen_at_bus = net.gen[net.gen.bus == idx]
            if not gen_at_bus.empty:
                # æ£€æŸ¥æ˜¯å¦ä¸ºslackèŠ‚ç‚¹
                if any(gen_at_bus.slack):
                    bus_type = 3  # slackèŠ‚ç‚¹
                else:
                    bus_type = 2  # PVèŠ‚ç‚¹

        # è·å–è´Ÿè·æ•°æ®
        pd = qd = 0
        if idx in net.load.bus.values:
            load_at_bus = net.load[net.load.bus == idx]
            pd = load_at_bus.p_mw.sum()
            qd = load_at_bus.q_mvar.sum()

        # è·å–ç”µå‹ç­‰çº§
        vn_kv = bus.vn_kv if hasattr(bus, 'vn_kv') else 345.0  # é»˜è®¤345kV

        bus_row = [
            idx + 1,  # bus number (1-indexed)
            bus_type,  # bus type
            pd,  # Pd (MW)
            qd,  # Qd (MVAr)
            0,  # Gs (MW)
            0,  # Bs (MVAr)
            1,  # area
            1.0,  # Vm (p.u.)
            0,  # Va (degrees)
            vn_kv,  # baseKV
            1,  # zone
            1.1,  # Vmax
            0.9   # Vmin
        ]
        bus_data.append(bus_row)

    # çº¿è·¯æ•°æ®è½¬æ¢
    branch_data = []
    for idx, line in net.line.iterrows():
        # è·å–çº¿è·¯ç”µå‹ç­‰çº§ï¼ˆä»è¿æ¥çš„èŠ‚ç‚¹è·å–ï¼‰
        from_bus_vn = net.bus.loc[line.from_bus, 'vn_kv'] if line.from_bus in net.bus.index else 345.0

        # MATPOWER branchæ ¼å¼: [fbus, tbus, r, x, b, rateA, rateB, rateC, ratio, angle, status, angmin, angmax]
        # ç®€åŒ–çš„é˜»æŠ—è®¡ç®—
        r_pu = line.r_ohm_per_km * line.length_km / (from_bus_vn**2 / baseMVA) if hasattr(line, 'r_ohm_per_km') else 0.01
        x_pu = line.x_ohm_per_km * line.length_km / (from_bus_vn**2 / baseMVA) if hasattr(line, 'x_ohm_per_km') else 0.05
        b_pu = 0.0  # ç®€åŒ–å¤„ç†
        rate_a = line.max_i_ka * from_bus_vn * np.sqrt(3) / 1000 if hasattr(line, 'max_i_ka') else 100.0

        branch_row = [
            line.from_bus + 1,  # from bus (1-indexed)
            line.to_bus + 1,    # to bus (1-indexed)
            r_pu,  # r (p.u.)
            x_pu,  # x (p.u.)
            b_pu,  # b (p.u.)
            rate_a,  # rateA (MVA)
            0,  # rateB
            0,  # rateC
            0,  # ratio
            0,  # angle
            1,  # status
            -360,  # angmin
            360    # angmax
        ]
        branch_data.append(branch_row)

    # å˜å‹å™¨æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
    for idx, trafo in net.trafo.iterrows():
        # ç®€åŒ–çš„å˜å‹å™¨æ¨¡å‹
        branch_row = [
            trafo.hv_bus + 1,  # from bus (1-indexed)
            trafo.lv_bus + 1,  # to bus (1-indexed)
            trafo.vk_percent / 100 * (trafo.vn_hv_kv**2 / baseMVA),  # r (p.u.)
            trafo.vkr_percent / 100 * (trafo.vn_hv_kv**2 / baseMVA),  # x (p.u.)
            0,  # b (p.u.)
            trafo.sn_mva,  # rateA (MVA)
            0,  # rateB
            0,  # rateC
            trafo.vn_hv_kv / trafo.vn_lv_kv,  # ratio
            0,  # angle
            1,  # status
            -360,  # angmin
            360    # angmax
        ]
        branch_data.append(branch_row)

    return {
        'baseMVA': baseMVA,
        'bus': np.array(bus_data),
        'branch': np.array(branch_data)
    }


class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—è®°å½•å™¨ - æ”¯æŒTensorBoardç›‘æ§"""

    def __init__(self, config: Dict[str, Any], total_episodes: int):
        """åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨"""
        self.config = config
        self.start_time = time.time()
        self.episode_rewards = []
        self.episode_lengths = []
        self.actor_losses = []
        self.critic_losses = []
        self.entropies = []
        self.success_rates = []
        self.load_cvs = []
        self.coupling_edges = []
        self.best_reward = -float('inf')
        self.total_episodes = total_episodes

        # æ™ºèƒ½è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ ç›¸å…³æŒ‡æ ‡
        self.curriculum_stages = []
        self.stage_transitions = []
        self.director_decisions = []

        log_config = config.get('logging', {})
        self.metrics_save_interval = log_config.get('metrics_save_interval', 100)

        # ç§»é™¤RichçŠ¶æ€é¢æ¿ï¼Œç®€åŒ–è¾“å‡º
        
        # ä½¿ç”¨ç®€å•è¿›åº¦æ¡
        try:
            from tqdm import tqdm
            self.progress_bar = tqdm(total=total_episodes, desc="ğŸš€ è®­ç»ƒè¿›åº¦",
                                   bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')
            self.use_rich = False
        except ImportError:
            # å¦‚æœtqdmä¹Ÿä¸å¯ç”¨ï¼Œä½¿ç”¨æœ€åŸºæœ¬çš„è¿›åº¦æ˜¾ç¤º
            self.progress_bar = None
            self.use_rich = False

        # è®¾ç½®TensorBoard
        self.use_tensorboard = log_config.get('use_tensorboard', False)
        self.tensorboard_writer = self._setup_tensorboard() if self.use_tensorboard else None

        # HTMLä»ªè¡¨æ¿å·²ç§»åŠ¨åˆ°test.pyä¸­ç”¨äºæ€§èƒ½åˆ†æ

        # ç§»é™¤TUIé›†æˆï¼Œç®€åŒ–ä¸ºåŸºæœ¬è¾“å‡º
        self.use_tui = False



    def _create_status_panel(self, episode: int, reward: float, info: Dict = None):
        """åˆ›å»ºç®€æ´çš„çŠ¶æ€é¢æ¿"""
        try:
            from rich.panel import Panel
            from rich.table import Table
            
            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            avg_reward = sum(self.episode_rewards[-10:]) / min(len(self.episode_rewards), 10) if self.episode_rewards else 0
            progress_pct = (episode + 1) / self.total_episodes * 100
            
            # ã€æ”¹è¿›ã€‘è®¡ç®—å¤šå±‚æ¬¡æˆåŠŸç‡
            success_stats = self._compute_multi_level_success_rate()
            
            # æå–è´¨é‡æŒ‡æ ‡
            quality_score = None
            if info and 'reward_components' in info:
                components = info['reward_components']
                quality_score = components.get('quality_score')
            
            # åˆ›å»ºä¸»è¡¨æ ¼
            table = Table.grid(padding=1)
            table.add_column("æŒ‡æ ‡", style="bold cyan", min_width=12)
            table.add_column("æ•°å€¼", justify="right", min_width=15)
            table.add_column("çŠ¶æ€", justify="center", min_width=8)
            
            # è¿›åº¦ä¿¡æ¯
            table.add_row("è®­ç»ƒè¿›åº¦", f"{episode+1:,}/{self.total_episodes:,} ({progress_pct:.1f}%)", "ğŸ“Š")
            
            # å¥–åŠ±ä¿¡æ¯
            reward_color = "bold green" if reward > 0 else "yellow" if reward > -1 else "red"
            reward_icon = "ğŸ‰" if reward > 0 else "âš¡" if reward > -1 else "âŒ"
            table.add_row("å½“å‰å¥–åŠ±", f"[{reward_color}]{reward:.3f}[/{reward_color}]", reward_icon)
            
            # æœ€ä½³å¥–åŠ±
            best_color = "bold green" if self.best_reward > 0 else "cyan"
            best_icon = "ğŸ†" if self.best_reward > 0 else "ğŸ¯"
            table.add_row("æœ€ä½³å¥–åŠ±", f"[{best_color}]{self.best_reward:.3f}[/{best_color}]", best_icon)
            
            # å¹³å‡å¥–åŠ±ï¼ˆæœ€è¿‘10è½®ï¼‰
            avg_color = "green" if avg_reward > -1 else "yellow" if avg_reward > -2 else "red"
            table.add_row("å¹³å‡å¥–åŠ±", f"[{avg_color}]{avg_reward:.3f}[/{avg_color}]", "ğŸ“ˆ")
            
            # ã€æ”¹è¿›ã€‘æ™ºèƒ½æˆåŠŸç‡æ˜¾ç¤º
            success_display, success_icon = self._format_success_rate_display(success_stats)
            table.add_row("å­¦ä¹ è¿›å±•", success_display, success_icon)
            
            # è´¨é‡åˆ†æ•°
            if quality_score is not None:
                quality_color = "green" if quality_score > 0.4 else "yellow" if quality_score > 0.3 else "red"
                table.add_row("è´¨é‡åˆ†æ•°", f"[{quality_color}]{quality_score:.3f}[/{quality_color}]", "â­")
            
            # ã€æ”¹è¿›ã€‘æ™ºèƒ½ç³»ç»ŸçŠ¶æ€è¯„ä¼°
            system_status = self._evaluate_system_status(success_stats, avg_reward)
            table.add_row("ç³»ç»ŸçŠ¶æ€", system_status, "ğŸ”§")
            
            # è¿è¡Œæ—¶é—´
            elapsed = time.time() - self.start_time
            if elapsed < 60:
                time_str = f"{elapsed:.0f}ç§’"
            elif elapsed < 3600:
                time_str = f"{elapsed/60:.1f}åˆ†é’Ÿ"
            else:
                time_str = f"{elapsed/3600:.1f}å°æ—¶"
            table.add_row("è¿è¡Œæ—¶é—´", time_str, "â±ï¸")
            
            title = "[bold blue]ğŸš€ ç”µåŠ›ç½‘ç»œåˆ†åŒºè®­ç»ƒç›‘æ§[/bold blue]"
            return Panel(table, title=title, border_style="blue", padding=(1, 2))
            
        except Exception as e:
            # å‡ºé”™æ—¶è¿”å›ç®€å•æ–‡æœ¬
            return f"Episode {episode+1}/{self.total_episodes} | å¥–åŠ±: {reward:.3f} | æœ€ä½³: {self.best_reward:.3f}"

    def _compute_multi_level_success_rate(self) -> Dict[str, Any]:
        """
        è®¡ç®—å¤šå±‚æ¬¡æˆåŠŸç‡æŒ‡æ ‡
        
        Returns:
            åŒ…å«ä¸åŒå±‚æ¬¡æˆåŠŸç‡çš„å­—å…¸
        """
        if not self.episode_rewards:
            return {
                'positive_count': 0,
                'positive_rate': 0.0,
                'improvement_count': 0,
                'improvement_rate': 0.0,
                'learning_count': 0,
                'learning_rate': 0.0,
                'total_episodes': 0
            }
        
        total = len(self.episode_rewards)
        
        # 1. ä¼ ç»Ÿæ­£å¥–åŠ±æˆåŠŸç‡
        positive_count = sum(1 for r in self.episode_rewards if r > 0)
        positive_rate = positive_count / total
        
        # 2. ç›¸å¯¹æ”¹è¿›æˆåŠŸç‡ï¼ˆç›¸æ¯”äºåˆå§‹å‡ è½®çš„è¡¨ç°ï¼‰
        baseline_reward = np.mean(self.episode_rewards[:min(3, total)]) if total >= 3 else min(self.episode_rewards)
        improvement_count = sum(1 for r in self.episode_rewards if r > baseline_reward)
        improvement_rate = improvement_count / total
        
        # 3. å­¦ä¹ è¿›å±•æˆåŠŸç‡ï¼ˆè¶…è¿‡åˆç†é˜ˆå€¼ï¼‰
        # å¯¹äºç”µåŠ›ç½‘ç»œåˆ†åŒºä»»åŠ¡ï¼Œ-1.0æ˜¯ä¸€ä¸ªåˆç†çš„å­¦ä¹ é˜ˆå€¼
        learning_threshold = -1.0
        learning_count = sum(1 for r in self.episode_rewards if r > learning_threshold)
        learning_rate = learning_count / total
        
        return {
            'positive_count': positive_count,
            'positive_rate': positive_rate,
            'improvement_count': improvement_count,
            'improvement_rate': improvement_rate,
            'learning_count': learning_count,
            'learning_rate': learning_rate,
            'total_episodes': total,
            'baseline_reward': baseline_reward,
            'learning_threshold': learning_threshold
        }
    
    def _format_success_rate_display(self, success_stats: Dict[str, Any]) -> Tuple[str, str]:
        """
        æ ¼å¼åŒ–æˆåŠŸç‡æ˜¾ç¤º
        
        Args:
            success_stats: å¤šå±‚æ¬¡æˆåŠŸç‡ç»Ÿè®¡
            
        Returns:
            (æ˜¾ç¤ºæ–‡æœ¬, å›¾æ ‡)
        """
        total = success_stats['total_episodes']
        
        # é€‰æ‹©æœ€æœ‰æ„ä¹‰çš„æˆåŠŸç‡æŒ‡æ ‡è¿›è¡Œæ˜¾ç¤º
        if success_stats['positive_rate'] > 0:
            # æœ‰æ­£å¥–åŠ±æ—¶æ˜¾ç¤ºæ­£å¥–åŠ±ç‡
            count = success_stats['positive_count']
            rate = success_stats['positive_rate']
            color = "bold green"
            label = "æ­£å¥–åŠ±"
            icon = "ğŸ‰"
        elif success_stats['learning_rate'] > 0.3:
            # å­¦ä¹ è¿›å±•è‰¯å¥½æ—¶æ˜¾ç¤ºå­¦ä¹ ç‡
            count = success_stats['learning_count']
            rate = success_stats['learning_rate']
            color = "green"
            label = "å­¦ä¹ è¿›å±•"
            icon = "ğŸ“š"
        elif success_stats['improvement_rate'] > 0.2:
            # æœ‰ç›¸å¯¹æ”¹è¿›æ—¶æ˜¾ç¤ºæ”¹è¿›ç‡
            count = success_stats['improvement_count']
            rate = success_stats['improvement_rate']
            color = "yellow"
            label = "ç›¸å¯¹æ”¹è¿›"
            icon = "ğŸ“ˆ"
        else:
            # æ—©æœŸè®­ç»ƒé˜¶æ®µ
            count = success_stats['learning_count']
            rate = success_stats['learning_rate']
            color = "dim"
            label = "æ—©æœŸå­¦ä¹ "
            icon = "ğŸŒ±"
        
        display = f"[{color}]{count}/{total} ({rate*100:.1f}%) {label}[/{color}]"
        return display, icon
    
    def _evaluate_system_status(self, success_stats: Dict[str, Any], avg_reward: float) -> str:
        """
        æ™ºèƒ½è¯„ä¼°ç³»ç»ŸçŠ¶æ€
        
        Args:
            success_stats: æˆåŠŸç‡ç»Ÿè®¡
            avg_reward: å¹³å‡å¥–åŠ±
            
        Returns:
            çŠ¶æ€æ˜¾ç¤ºæ–‡æœ¬
        """
        positive_rate = success_stats['positive_rate']
        learning_rate = success_stats['learning_rate']
        improvement_rate = success_stats['improvement_rate']
        
        if positive_rate > 0.3:
            return "[bold green]ğŸŸ¢ è®­ç»ƒæˆåŠŸ[/bold green]"
        elif positive_rate > 0.1 or learning_rate > 0.5:
            return "[green]ğŸŸ¡ å­¦ä¹ è‰¯å¥½[/green]"
        elif learning_rate > 0.3 or improvement_rate > 0.4:
            return "[yellow]ğŸŸ  ç¨³æ­¥æ”¹è¿›[/yellow]"
        elif avg_reward > -3.0 and improvement_rate > 0.2:
            return "[yellow]ğŸŸ¡ æ­£åœ¨å­¦ä¹ [/yellow]"
        else:
            return "[red]ğŸ”´ éœ€è¦æ›´å¤šè®­ç»ƒ[/red]"

    def _setup_tensorboard(self):
        """è®¾ç½®TensorBoard"""
        try:
            from torch.utils.tensorboard import SummaryWriter
            log_dir = self.config['logging']['log_dir']
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            tensorboard_writer = SummaryWriter(f"{log_dir}/training_{timestamp}")
            from code.src.rich_output import rich_info
            rich_info(f"TensorBoardæ—¥å¿—ç›®å½•: {log_dir}/training_{timestamp}", show_always=True)
            return tensorboard_writer
        except ImportError:
            from code.src.rich_output import rich_warning
            rich_warning("TensorBoardä¸å¯ç”¨ï¼Œè·³è¿‡TensorBoardæ—¥å¿—")
            self.use_tensorboard = False
            return None
        except Exception as e:
            from code.src.rich_output import rich_warning
            rich_warning(f"TensorBoardåˆå§‹åŒ–å¤±è´¥: {e}")
            self.use_tensorboard = False
            return None

    def log_episode(self, episode: int, reward: float, length: int, info: Dict = None):
        """è®°å½•å•ä¸ªå›åˆ"""
        self.episode_rewards.append(reward)
        self.episode_lengths.append(length)

        if reward > self.best_reward:
            self.best_reward = reward

        # å¦‚æœä½¿ç”¨TUIï¼Œå°†æ›´æ–°æ¨é€åˆ°é˜Ÿåˆ—
        if self.use_tui and self.tui_update_queue:
            from code.src.tui_monitor import TrainingUpdate
            
            avg_reward = np.mean(self.episode_rewards) if self.episode_rewards else 0
            quality_score = info.get('quality_score', 0.5) if info else 0.5
            success_rate = np.mean(self.success_rates) * 100 if self.success_rates else 0.0

            update = TrainingUpdate(
                episode=episode,
                total_episodes=self.total_episodes,
                reward=reward,
                best_reward=self.best_reward,
                avg_reward=avg_reward,
                quality_score=quality_score,
                success_rate=success_rate,
                log_message=f"[{episode+1}/{self.total_episodes}] Reward: {reward:.3f}"
            )
            self.tui_update_queue.put(update)
            # æ³¨é‡Šæ‰æå‰è¿”å›ï¼Œè®©ä»£ç ç»§ç»­æ‰§è¡ŒTensorBoardè®°å½•
            # return # TUIæ¥ç®¡æ˜¾ç¤ºï¼Œç›´æ¥è¿”å›

        # ä½¿ç”¨ç®€å•è¿›åº¦æ¡
        if self.progress_bar:
            # è®¡ç®—ä¸€äº›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            avg_reward = sum(self.episode_rewards[-10:]) / min(len(self.episode_rewards), 10) if self.episode_rewards else reward

            # æ›´æ–°è¿›åº¦æ¡æè¿°
            desc = f"ğŸš€ è®­ç»ƒè¿›åº¦ | å½“å‰: {reward:.3f} | æœ€ä½³: {self.best_reward:.3f} | å¹³å‡: {avg_reward:.3f}"
            self.progress_bar.set_description(desc)
            self.progress_bar.update(1)
        else:
            # å¦‚æœæ²¡æœ‰è¿›åº¦æ¡ï¼Œä½¿ç”¨ç®€å•çš„æ–‡æœ¬è¾“å‡º
            if episode % 100 == 0 or episode == self.total_episodes - 1:
                avg_reward = sum(self.episode_rewards[-10:]) / min(len(self.episode_rewards), 10) if self.episode_rewards else reward
                print(f"Episode {episode+1}/{self.total_episodes} | å¥–åŠ±: {reward:.3f} | æœ€ä½³: {self.best_reward:.3f} | å¹³å‡: {avg_reward:.3f}")

        # è®°å½•é¢å¤–ä¿¡æ¯ - ã€ä¿®å¤ã€‘é€‚é…æ–°ç³»ç»ŸæŒ‡æ ‡åç§°
        if info:
            if 'success' in info:
                self.success_rates.append(1.0 if info['success'] else 0.0)
            # ä¼˜å…ˆä½¿ç”¨æ–°æŒ‡æ ‡åç§°ï¼Œå‘åå…¼å®¹æ—§åç§°
            if 'cv' in info or 'load_cv' in info:
                cv_value = info.get('cv', info.get('load_cv', 0.0))
                self.load_cvs.append(cv_value)
            if 'coupling_ratio' in info or 'coupling_edges' in info:
                coupling_value = info.get('coupling_ratio', info.get('coupling_edges', 0.0))
                self.coupling_edges.append(coupling_value)

        # TensorBoardæ—¥å¿— - é‡æ–°è®¾è®¡çš„åˆ†ç»„è®°å½•
        if self.use_tensorboard and self.tensorboard_writer:
            self._log_tensorboard_metrics(episode, reward, length, info)

    def _log_tensorboard_metrics(self, episode: int, reward: float, length: int, info: dict):
        """
        åˆ†ç»„è®°å½•TensorBoardæŒ‡æ ‡ï¼Œæä¾›æ¸…æ™°çš„å¯è§†åŒ–ç»“æ„

        æŒ‡æ ‡åˆ†ç»„ï¼š
        - Core: æ ¸å¿ƒè®­ç»ƒæŒ‡æ ‡
        - Reward: å¥–åŠ±ç»„ä»¶è¯¦æƒ…
        - Quality: è´¨é‡è¯„ä¼°æŒ‡æ ‡
        - Debug: è°ƒè¯•å’Œè¯Šæ–­ä¿¡æ¯
        """
        if not (self.use_tensorboard and self.tensorboard_writer):
            return

        # === 1. æ ¸å¿ƒè®­ç»ƒæŒ‡æ ‡ (Core Training Metrics) ===
        self.tensorboard_writer.add_scalar('Core/Episode_Reward', reward, episode)
        self.tensorboard_writer.add_scalar('Core/Episode_Length', length, episode)

        if info:
            # æˆåŠŸç‡å’ŒåŸºç¡€çŠ¶æ€
            if 'success' in info:
                self.tensorboard_writer.add_scalar('Core/Success_Rate', float(info['success']), episode)
            if 'step' in info:
                self.tensorboard_writer.add_scalar('Core/Current_Step', info['step'], episode)

        # === 2. å¥–åŠ±ç»„ä»¶è¯¦æƒ… (Reward Components) ===
        if info:
            # ç»ˆå±€å¥–åŠ±ç»„ä»¶
            reward_components = {
                'balance_reward': 'Balance_Component',
                'decoupling_reward': 'Decoupling_Component',
                'power_reward': 'Power_Component',
                'quality_reward': 'Quality_Total',
                'final_reward': 'Final_Total'
            }

            for key, display_name in reward_components.items():
                if key in info and isinstance(info[key], (int, float)):
                    self.tensorboard_writer.add_scalar(f'Reward/{display_name}', info[key], episode)

            # å¥–åŠ±è°ƒèŠ‚å› å­
            adjustment_factors = {
                'threshold_bonus': 'Threshold_Bonus',
                'termination_discount': 'Termination_Discount'
            }

            for key, display_name in adjustment_factors.items():
                if key in info and isinstance(info[key], (int, float)):
                    self.tensorboard_writer.add_scalar(f'Reward/{display_name}', info[key], episode)

        # === 3. è´¨é‡è¯„ä¼°æŒ‡æ ‡ (Quality Assessment) ===
        if info:
            quality_metrics = {
                'quality_score': 'Unified_Quality_Score',
                'plateau_confidence': 'Plateau_Confidence'
            }

            for key, display_name in quality_metrics.items():
                if key in info and isinstance(info[key], (int, float)):
                    self.tensorboard_writer.add_scalar(f'Quality/{display_name}', info[key], episode)

        # === 4. è°ƒè¯•å’Œè¯Šæ–­ä¿¡æ¯ (Debug & Diagnostics) ===
        if info:
            debug_metrics = {
                'reward_mode': 'Current_Reward_Mode'
            }

            for key, display_name in debug_metrics.items():
                if key in info:
                    # å¯¹äºå­—ç¬¦ä¸²ç±»å‹ï¼Œè½¬æ¢ä¸ºæ•°å€¼ç¼–ç 
                    if isinstance(info[key], str):
                        # ç®€å•çš„å­—ç¬¦ä¸²å“ˆå¸Œç¼–ç ç”¨äºå¯è§†åŒ–
                        value = hash(info[key]) % 1000
                        self.tensorboard_writer.add_scalar(f'Debug/{display_name}', value, episode)
                    elif isinstance(info[key], (int, float)):
                        self.tensorboard_writer.add_scalar(f'Debug/{display_name}', info[key], episode)

        # å®šæœŸåˆ·æ–°ä»¥ç¡®ä¿æ•°æ®å†™å…¥
        if episode % 10 == 0:
            self.tensorboard_writer.flush()

    def log_training_step(self, episode: int, actor_loss: float = None,
                         critic_loss: float = None, entropy: float = None):
        """è®°å½•è®­ç»ƒæ­¥éª¤ - ä½¿ç”¨åˆ†ç»„è®°å½•"""
        if actor_loss is not None:
            self.actor_losses.append(actor_loss)
            if self.use_tensorboard and self.tensorboard_writer:
                self.tensorboard_writer.add_scalar('Training/Actor_Loss', actor_loss, episode)

        if critic_loss is not None:
            self.critic_losses.append(critic_loss)
            if self.use_tensorboard and self.tensorboard_writer:
                self.tensorboard_writer.add_scalar('Training/Critic_Loss', critic_loss, episode)

        if entropy is not None:
            self.entropies.append(entropy)
            if self.use_tensorboard and self.tensorboard_writer:
                self.tensorboard_writer.add_scalar('Training/Policy_Entropy', entropy, episode)

        # TensorBoardæ•°æ®ä¼šåœ¨_log_tensorboard_metricsä¸­å®šæœŸåˆ·æ–°



    def get_statistics(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        if not self.episode_rewards:
            return {}

        stats = {
            'total_episodes': len(self.episode_rewards),
            'best_reward': self.best_reward,
            'mean_reward': np.mean(self.episode_rewards),
            'std_reward': np.std(self.episode_rewards),
            'mean_length': np.mean(self.episode_lengths),
            'training_time': time.time() - self.start_time
        }

        if self.success_rates:
            stats['success_rate'] = np.mean(self.success_rates)
        if self.load_cvs:
            stats['mean_load_cv'] = np.mean(self.load_cvs)
        if self.actor_losses:
            stats['mean_actor_loss'] = np.mean(self.actor_losses)
        if self.critic_losses:
            stats['mean_critic_loss'] = np.mean(self.critic_losses)

        return stats

    def set_training_mode(self, training: bool = True):
        """
        è®¾ç½®è®­ç»ƒ/è¯„ä¼°æ¨¡å¼ï¼Œæ§åˆ¶TensorBoardæŒ‡æ ‡è®°å½•

        Args:
            training: Trueä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆåªè®°å½•è®­ç»ƒç›¸å…³æŒ‡æ ‡ï¼‰ï¼ŒFalseä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆè®°å½•æ‰€æœ‰æŒ‡æ ‡ï¼‰
        """
        self.training_mode = training

    # HTMLä»ªè¡¨æ¿ç”ŸæˆåŠŸèƒ½å·²ç§»åŠ¨åˆ°test.pyä¸­ç”¨äºæ€§èƒ½åˆ†æ

    def close(self):
        """å…³é—­æ—¥å¿—è®°å½•å™¨"""
        # å…³é—­è¿›åº¦æ¡
        if hasattr(self, 'progress_bar') and self.progress_bar:
            self.progress_bar.close()

        if self.tensorboard_writer:
            self.tensorboard_writer.close()


class UnifiedTrainer:
    """ç»Ÿä¸€è®­ç»ƒå™¨"""

    def __init__(self, agent, env, config, gym_env=None):
        self.agent = agent
        self.env = env
        self.config = config
        self.gym_env = gym_env  # åœºæ™¯ç”Ÿæˆç¯å¢ƒåŒ…è£…å™¨ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
        # å¤„ç†è®¾å¤‡é…ç½®
        device_config = config['system']['device']
        if device_config == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device_config)
        self.logger = None # å°†åœ¨trainæ–¹æ³•ä¸­åˆå§‹åŒ–
        self.tui_thread = None

    def train(self, num_episodes: int, max_steps_per_episode: int, update_interval: int = 10):
        """è®­ç»ƒæ™ºèƒ½ä½“"""
        self.logger = TrainingLogger(self.config, num_episodes)
        
        # å¦‚æœå¯ç”¨TUIï¼Œåœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­è¿è¡Œå®ƒ
        if self.logger.use_tui:
            self.tui_thread = threading.Thread(target=self.logger.tui_app.run, daemon=True)
            self.tui_thread.start()
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ä»¥ç¡®ä¿TUIå¯åŠ¨
            time.sleep(1)

        from code.src.rich_output import rich_info
        if not self.config.get('debug', {}).get('training_output', {}).get('only_show_errors', True):
            rich_info(f"TensorBoard: {'å·²å¯ç”¨' if self.logger.use_tensorboard else 'å·²ç¦ç”¨'}")
            rich_info(f"æŒ‡æ ‡ä¿å­˜é—´éš”: {self.logger.metrics_save_interval} å›åˆ")

        for episode in range(num_episodes):
            # å¦‚æœä½¿ç”¨åœºæ™¯ç”Ÿæˆç¯å¢ƒï¼Œéœ€è¦é‡ç½®Gymç¯å¢ƒä»¥ç”Ÿæˆæ–°åœºæ™¯
            if self.gym_env is not None:
                obs_array, info = self.gym_env.reset()
                # æ›´æ–°å†…éƒ¨ç¯å¢ƒå¼•ç”¨ï¼ˆå› ä¸ºæ¯æ¬¡é‡ç½®éƒ½ä¼šåˆ›å»ºæ–°çš„å†…éƒ¨ç¯å¢ƒï¼‰
                self.env = self.gym_env.internal_env
                state, _ = self.env.reset()  # é‡ç½®å†…éƒ¨ç¯å¢ƒçŠ¶æ€
            else:
                state, _ = self.env.reset()  # æ ‡å‡†ç¯å¢ƒé‡ç½®

            episode_reward = 0
            episode_length = 0
            episode_info = {}

            for step in range(max_steps_per_episode):
                # é€‰æ‹©åŠ¨ä½œ
                action, log_prob, value = self.agent.select_action(state)

                if action is None:  # æ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œ
                    break

                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, terminated, truncated, info = self.env.step(action)
                done = terminated or truncated

                # å­˜å‚¨ç»éªŒ
                self.agent.store_experience(state, action, reward, log_prob, value, done)

                episode_reward += reward
                episode_length += 1
                state = next_state

                # æ”¶é›†å›åˆä¿¡æ¯
                if info:
                    episode_info.update(info)

                if done:
                    break

            # è®°å½•åˆ°loggerï¼ˆåŒ…å«è¯¦ç»†ä¿¡æ¯ï¼‰
            self.logger.log_episode(episode, episode_reward, episode_length, episode_info)

            # ã€æ–°å¢ã€‘éªŒè¯æŒ‡æ ‡å®Œæ•´æ€§ï¼Œç¡®ä¿ä¿®å¤ç”Ÿæ•ˆ - ç®€åŒ–ç‰ˆ
            if episode < 3 and not self.config.get('debug', {}).get('training_output', {}).get('only_show_errors', True):
                self._validate_metrics_integrity(episode_info, episode)

            # æ›´æ–°æ™ºèƒ½ä½“å¹¶è®°å½•è®­ç»ƒæŒ‡æ ‡
            if episode % update_interval == 0 and episode > 0:
                try:
                    # --- ä½¿ç”¨ try-except åŒ…è£¹ agent.update() ---
                    training_stats = self.agent.update()
                    if training_stats:
                        self.logger.log_training_step(
                            episode,
                            training_stats.get('actor_loss'),
                            training_stats.get('critic_loss'),
                            training_stats.get('entropy')
                        )

                except RuntimeError as e:
                    if "[NaNGuard]" in str(e) or "NaN" in str(e):
                        print(f"\nâŒ [NaN Dumper] æ•è·åˆ°è‡´å‘½é”™è¯¯: {e}")

                        dump_dir = "diagnostics"
                        os.makedirs(dump_dir, exist_ok=True)
                        dump_path = os.path.join(dump_dir, f"nan_dump_episode_{episode}.pt")

                        print(f"  ... æ­£åœ¨ä¿å­˜è¯Šæ–­ä¿¡æ¯åˆ°: {dump_path}")

                        # æ”¶é›†è§¦å‘å¼‚å¸¸çš„æ‰¹é‡æ•°æ®
                        batch_data = {
                            'states': self.agent.memory.states,
                            'actions': self.agent.memory.actions,
                            'logprobs': self.agent.memory.logprobs,
                            'rewards': self.agent.memory.rewards,
                            'is_terminals': self.agent.memory.is_terminals,
                        }

                        torch.save({
                            "error_message": str(e),
                            "episode": episode,
                            "actor_state_dict": self.agent.actor.state_dict(),
                            "critic_state_dict": self.agent.critic.state_dict(),
                            "actor_optimizer_state_dict": self.agent.actor_optimizer.state_dict(),
                            "critic_optimizer_state_dict": self.agent.critic_optimizer.state_dict(),
                            "triggering_batch": batch_data,
                        }, dump_path)

                        print("  ... ä¿å­˜å®Œæˆã€‚æ­£åœ¨ä¸­æ–­è®­ç»ƒã€‚")
                        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä»¥æ­£å¸¸é€€å‡ºè®­ç»ƒå¾ªç¯
                        raise e
                    else:
                        # å¦‚æœæ˜¯å…¶ä»–è¿è¡Œæ—¶é”™è¯¯ï¼Œåˆ™ç›´æ¥æŠ›å‡º
                        raise

            # ä¿å­˜ä¸­é—´ç»“æœ
            if episode % self.logger.metrics_save_interval == 0 and episode > 0:
                self._save_intermediate_results(episode)

        # ç®€æ´çš„è®­ç»ƒå®Œæˆç»Ÿè®¡
        final_stats = self.logger.get_statistics()
        positive_rewards = sum(1 for r in self.logger.episode_rewards if r > 0)
        total_episodes = final_stats.get('total_episodes', 0)
        best_reward = final_stats.get('best_reward', 0)
        mean_reward = final_stats.get('mean_reward', 0)

        # ä½¿ç”¨Richåˆ›å»ºç®€æ´çš„å®Œæˆè¡¨æ ¼
        try:
            from rich.console import Console
            from rich.table import Table
            from rich.panel import Panel
            
            console = Console()
            table = Table.grid(padding=1)
            table.add_column("æŒ‡æ ‡", style="bold cyan")
            table.add_column("ç»“æœ", style="green", justify="right")
            
            table.add_row("å›åˆæ•°", f"{total_episodes:,}")
            table.add_row("æœ€ä½³å¥–åŠ±", f"{best_reward:.4f}")
            table.add_row("å¹³å‡å¥–åŠ±", f"{mean_reward:.4f}")
            
            # ã€æ”¹è¿›ã€‘ä½¿ç”¨æ™ºèƒ½æˆåŠŸç‡ç»Ÿè®¡
            if hasattr(self.logger, '_compute_multi_level_success_rate'):
                success_stats = self.logger._compute_multi_level_success_rate()
                if success_stats['positive_rate'] > 0:
                    success_text = f"æ­£å¥–åŠ±: {success_stats['positive_count']}/{total_episodes} ({success_stats['positive_rate']*100:.1f}%)"
                elif success_stats['learning_rate'] > 0.3:
                    success_text = f"å­¦ä¹ è¿›å±•: {success_stats['learning_count']}/{total_episodes} ({success_stats['learning_rate']*100:.1f}%)"
                else:
                    success_text = f"ç›¸å¯¹æ”¹è¿›: {success_stats['improvement_count']}/{total_episodes} ({success_stats['improvement_rate']*100:.1f}%)"
            else:
                # å¤‡ç”¨ä¼ ç»Ÿè®¡ç®—
                success_text = f"{positive_rewards}/{total_episodes} ({positive_rewards/total_episodes*100:.1f}%)"
            
            table.add_row("å­¦ä¹ æ•ˆæœ", success_text)
            table.add_row("è®­ç»ƒæ—¶é—´", f"{final_stats.get('training_time', 0)/60:.1f} åˆ†é’Ÿ")
            
            # ã€æ”¹è¿›ã€‘æ™ºèƒ½ç³»ç»ŸçŠ¶æ€è¯„ä¼°
            if hasattr(self.logger, '_compute_multi_level_success_rate'):
                success_stats = self.logger._compute_multi_level_success_rate()
                if success_stats['positive_rate'] > 0.3:
                    status = "[bold green]ğŸ‰ è®­ç»ƒæˆåŠŸ[/bold green]"
                elif success_stats['positive_rate'] > 0.1 or success_stats['learning_rate'] > 0.5:
                    status = "[green]âœ… å­¦ä¹ è‰¯å¥½[/green]"
                elif success_stats['learning_rate'] > 0.3 or success_stats['improvement_rate'] > 0.4:
                    status = "[yellow]ğŸŸ  ç¨³æ­¥æ”¹è¿›[/yellow]"
                elif mean_reward > -3.0 and success_stats['improvement_rate'] > 0.2:
                    status = "[yellow]ğŸŸ¡ æ­£åœ¨å­¦ä¹ [/yellow]"
                else:
                    status = "[red]âš ï¸ éœ€è¦æ›´å¤šè®­ç»ƒ[/red]"
            else:
                # å¤‡ç”¨ä¼ ç»Ÿè¯„ä¼°
                if best_reward > 0:
                    status = "[bold green]ğŸ‰ é‡æ„æˆåŠŸ[/bold green]"
                elif positive_rewards > 0:
                    status = "[yellow]âœ… é‡æ„æœ‰æ•ˆ[/yellow]"
                else:
                    status = "[red]âš ï¸ éœ€è¦æ›´å¤šè®­ç»ƒ[/red]"
            table.add_row("ç³»ç»ŸçŠ¶æ€", status)
            
            panel = Panel(table, title="[bold blue]ğŸ¯ è®­ç»ƒå®Œæˆç»Ÿè®¡[/bold blue]", border_style="blue")
            console.print(panel)
            
        except ImportError:
            # å¤‡ç”¨ç®€å•è¾“å‡º
            print(f"\nğŸ¯ è®­ç»ƒå®Œæˆ: {total_episodes}å›åˆ, æœ€ä½³å¥–åŠ±: {best_reward:.4f}, æˆåŠŸç‡: {positive_rewards/total_episodes*100:.1f}%")

        return {
            'episode_rewards': self.logger.episode_rewards,
            'episode_lengths': self.logger.episode_lengths,
            'training_stats': final_stats
        }

    def _validate_metrics_integrity(self, episode_info: Dict[str, Any], episode: int):
        """
        éªŒè¯æŒ‡æ ‡å®Œæ•´æ€§ï¼Œç¡®ä¿ä¿®å¤ç”Ÿæ•ˆ

        Args:
            episode_info: episodeä¿¡æ¯å­—å…¸
            episode: episodeç¼–å·
        """
        try:
            # æ£€æŸ¥å…³é”®æŒ‡æ ‡æ˜¯å¦ä¸å†æ˜¯å›ºå®šå€¼
            metrics = episode_info.get('metrics', {})

            # æ£€æŸ¥CVæŒ‡æ ‡
            cv_value = metrics.get('cv', metrics.get('load_cv', 1.0))
            if cv_value == 1.0 and episode > 1:
                from code.src.rich_output import rich_warning
                rich_warning(f"Episode {episode}: CVæŒ‡æ ‡ä»ä¸ºå›ºå®šå€¼1.0ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")

            # è¾“å‡ºæŒ‡æ ‡æ‘˜è¦ï¼ˆä»…å‰3ä¸ªepisodeï¼Œæ›´ç®€æ´ï¼‰
            if episode <= 2:
                coupling_ratio = metrics.get('coupling_ratio', 1.0)
                connectivity = metrics.get('connectivity', 0.0)
                from code.src.rich_output import rich_debug
                rich_debug(f"Episode {episode} æŒ‡æ ‡: CV={cv_value:.3f}, Coupling={coupling_ratio:.3f}, Conn={connectivity:.3f}")

        except Exception as e:
            from code.src.rich_output import rich_warning
            rich_warning(f"Episode {episode} æŒ‡æ ‡éªŒè¯å¤±è´¥: {e}")

    def _save_intermediate_results(self, episode: int):
        """ä¿å­˜ä¸­é—´è®­ç»ƒç»“æœ"""
        try:
            stats = self.logger.get_statistics()
            checkpoint_dir = Path(self.config['logging']['checkpoint_dir'])
            checkpoint_dir.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats_file = checkpoint_dir / f"training_stats_episode_{episode}.json"
            with open(stats_file, 'w') as f:
                json.dump(stats, f, indent=2)

            print(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: episode {episode}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ä¸­é—´ç»“æœå¤±è´¥: {e}")

    def evaluate(self, num_episodes: int = 10):
        """è¯„ä¼°æ™ºèƒ½ä½“ - ä½¿ç”¨åˆç†çš„æˆåŠŸæ ‡å‡†"""
        print(f"ğŸ” è¯„ä¼°æ™ºèƒ½ä½“ {num_episodes} è½®...")

        eval_rewards = []
        success_count = 0

        for episode in range(num_episodes):
            state, _ = self.env.reset()  # è§£åŒ…å…ƒç»„
            episode_reward = 0
            episode_info = {}

            for step in range(200):  # æœ€å¤§æ­¥æ•°
                action, _, _ = self.agent.select_action(state, training=False)

                if action is None:  # æ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œ
                    break

                state, reward, terminated, truncated, info = self.env.step(action)
                done = terminated or truncated
                episode_reward += reward
                
                # æ”¶é›†æœ€åçš„ç¯å¢ƒä¿¡æ¯
                if info:
                    episode_info.update(info)

                if done:
                    break

            eval_rewards.append(episode_reward)
            
            # ã€æ”¹è¿›ã€‘ä½¿ç”¨åˆç†çš„æˆåŠŸæ ‡å‡†è¯„ä¼°ï¼Œè€Œä¸æ˜¯ä¾èµ–ç¯å¢ƒsuccessæ ‡å¿—
            is_success = self._evaluate_episode_success(episode_reward, episode_info)
            if is_success:
                success_count += 1

        return {
            'avg_reward': np.mean(eval_rewards),
            'success_rate': success_count / num_episodes,
            'rewards': eval_rewards
        }



    def _evaluate_episode_success(self, episode_reward: float, episode_info: Dict[str, Any]) -> bool:
        """
        è¯„ä¼°å•ä¸ªepisodeæ˜¯å¦æˆåŠŸ
        
        Args:
            episode_reward: episodeæ€»å¥–åŠ±
            episode_info: episodeä¿¡æ¯ï¼ˆå¯èƒ½åŒ…å«æŒ‡æ ‡ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # æˆåŠŸæ ‡å‡†1: å¥–åŠ±è¶…è¿‡åˆç†é˜ˆå€¼
            if episode_reward > -1.0:  # å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œ-1.0æ˜¯ä¸€ä¸ªåˆç†çš„æˆåŠŸé˜ˆå€¼
                return True
            
            # æˆåŠŸæ ‡å‡†2: åŸºäºè´¨é‡æŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if episode_info and 'metrics' in episode_info:
                metrics = episode_info['metrics']
                
                # æ£€æŸ¥è´Ÿè½½å¹³è¡¡ï¼ˆCV < 0.5ï¼‰
                cv = metrics.get('cv', metrics.get('load_cv', 1.0))
                if cv < 0.5:
                    return True
                
                # æ£€æŸ¥è¿é€šæ€§ï¼ˆconnectivity > 0.9ï¼‰
                connectivity = metrics.get('connectivity', 0.0)
                if connectivity > 0.9:
                    return True
                
                # æ£€æŸ¥è€¦åˆæ¯”ç‡ï¼ˆcoupling_ratio < 0.3ï¼‰
                coupling_ratio = metrics.get('coupling_ratio', 1.0)
                if coupling_ratio < 0.3:
                    return True
            
            # æˆåŠŸæ ‡å‡†3: åŸºäºè´¨é‡åˆ†æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if episode_info and 'reward_components' in episode_info:
                components = episode_info['reward_components']
                quality_score = components.get('quality_score', 0.0)
                if quality_score > 0.4:  # è´¨é‡åˆ†æ•°è¶…è¿‡0.4è®¤ä¸ºæˆåŠŸ
                    return True
            
            # å¦‚æœéƒ½ä¸æ»¡è¶³ï¼Œä½†å¥–åŠ±æœ‰æ˜¾è‘—æ”¹è¿›ï¼ˆç›¸æ¯”-5.0åŸºçº¿ï¼‰
            if episode_reward > -2.5:  # ç›¸æ¯”å¾ˆå·®çš„åŸºçº¿æœ‰æ˜¾è‘—æ”¹è¿›
                return True
                
            return False
            
        except Exception as e:
            # å¦‚æœè¯„ä¼°å‡ºé”™ï¼Œä¿å®ˆåœ°æ ¹æ®å¥–åŠ±åˆ¤æ–­
            return episode_reward > -1.5



    def close(self):
        """æ¸…ç†èµ„æº"""
        if self.logger:
            self.logger.close()


class UnifiedTrainingSystem:
    """ç»Ÿä¸€è®­ç»ƒç³»ç»Ÿ - ä¸“æ³¨äºè®­ç»ƒåŠŸèƒ½"""

    def __init__(self, config_path: Optional[str] = None, **kwargs):
        """
        åˆå§‹åŒ–ç»Ÿä¸€è®­ç»ƒç³»ç»Ÿ

        Args:
            config_path: è‡ªå®šä¹‰é…ç½®æ–‡ä»¶è·¯å¾„
            **kwargs: ç”¨äºè¦†ç›–é…ç½®çš„é”®å€¼å¯¹
        """
        # 1. åŠ è½½å’Œåˆå¹¶é…ç½®
        self.config = self._load_config(config_path, kwargs)

        # 2. è®¾ç½®è®¾å¤‡
        self.device = self._setup_device()

        self.setup_directories()

    def _load_config(self, config_path: Optional[str], overrides: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        åŠ è½½é…ç½®ï¼Œä¼˜å…ˆçº§å¦‚ä¸‹: å‘½ä»¤è¡Œè¦†ç›– > æ–‡ä»¶é…ç½® > é»˜è®¤é…ç½®
        """
        # 1. è·å–é»˜è®¤é…ç½®
        final_config = self._create_default_config()

        # 2. ç¡®å®šè¦åŠ è½½çš„é…ç½®æ–‡ä»¶è·¯å¾„
        # å¦‚æœå‘½ä»¤è¡Œæ²¡æœ‰æŒ‡å®š --config, åˆ™ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„ config.yaml
        if config_path is None:
            config_path = 'config.yaml'
        
        # 3. å¦‚æœé…ç½®æ–‡ä»¶å­˜åœ¨ï¼Œåˆ™åŠ è½½å¹¶åˆå¹¶
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    file_config = yaml.safe_load(f)
                
                if file_config: # ç¡®ä¿æ–‡ä»¶ä¸æ˜¯ç©ºçš„
                    final_config = self._deep_merge_config(final_config, file_config)
                    from code.src.rich_output import rich_success
                    rich_success(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
            except Exception as e:
                from code.src.rich_output import rich_warning
                rich_warning(f"âš ï¸ åŠ è½½é…ç½®æ–‡ä»¶ {config_path} å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")
        else:
            # åªåœ¨ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†--configä½†æ–‡ä»¶ä¸å­˜åœ¨æ—¶å‘å‡ºè­¦å‘Š
            if config_path != 'config.yaml':
                 from code.src.rich_output import rich_warning
                 rich_warning(f"âš ï¸ æŒ‡å®šçš„é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: '{config_path}'ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")

        # 4. åº”ç”¨æ¥è‡ªæ„é€ å‡½æ•°çš„å‘½ä»¤è¡Œè¦†ç›–é¡¹
        if overrides:
            final_config = self._deep_merge_config(final_config, overrides)
            from code.src.rich_output import rich_info
            rich_info("âœ… å·²åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®ã€‚")

        return final_config

    def _deep_merge_config(self, base_config: Dict[str, Any], preset_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ·±åº¦åˆå¹¶é…ç½®å­—å…¸"""
        result = base_config.copy()

        for key, value in preset_config.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge_config(result[key], value)
            else:
                result[key] = value

        return result

    def _create_default_config(self) -> Dict[str, Any]:
        """åˆ›å»ºé»˜è®¤é…ç½®"""
        return {
            'system': {
                'name': 'unified_power_grid_partitioning',
                'version': '2.0',
                'device': 'auto',
                'seed': 42,
                'num_threads': 1
            },
            'data': {
                'case_name': 'ieee14',
                'normalize': True,
                'cache_dir': 'data/cache'
            },
            'training': {
                'mode': 'standard',  # standard, parallel, curriculum, large_scale
                'num_episodes': 1000,
                'max_steps_per_episode': 200,
                'update_interval': 10,
                'save_interval': 100,
                'eval_interval': 50
            },
            'environment': {
                'num_partitions': 3,
                'max_steps': 200,
                'reward_weights': {
                    'load_b': 0.4,
                    'decoupling': 0.4,
                    'power_b': 0.2
                }
            },
            'gat': {
                'hidden_channels': 64,
                'gnn_layers': 3,
                'heads': 4,
                'output_dim': 128,
                'dropout': 0.1
            },
            'agent': {
                'type': 'ppo',  # ppo, sb3_ppo
                'lr_actor': 3e-5,  # é™ä½10å€ç”¨äºæ•°å€¼ç¨³å®š
                'lr_critic': 1e-4,  # é™ä½10å€ç”¨äºæ•°å€¼ç¨³å®š
                'gamma': 0.99,
                'eps_clip': 0.2,
                'k_epochs': 4,
                'entropy_coef': 0.01,
                'value_coef': 0.5,
                'hidden_dim': 256
            },
            'parallel_training': {
                'enabled': False,
                'num_cpus': 12,
                'total_timesteps': 5_000_000,
                'scenario_generation': True
            },
            'scenario_generation': {
                'enabled': True,  # é»˜è®¤å¯ç”¨åœºæ™¯ç”Ÿæˆ
                'perturb_prob': 0.8,
                'perturb_types': ['n-1', 'load_gen_fluctuation', 'both', 'none'],
                'scale_range': [0.8, 1.2]
            },
            'curriculum': {
                'enabled': False,
                'start_partitions': 2,
                'end_partitions': 5,
                'episodes_per_stage': 200
            },
            'evaluation': {
                'num_episodes': 20,
                'include_baselines': True,
                'baseline_methods': ['spectral', 'kmeans', 'random']
            },
            'visualization': {
                'enabled': True,
                'save_figures': True,
                'figures_dir': 'data/figures',
                'interactive': True
            },
            'logging': {
                'use_tensorboard': True,
                'log_dir': 'data/logs',
                'checkpoint_dir': 'data/checkpoints',
                'console_log_interval': 10,
                'metrics_save_interval': 50
            },
            # ã€æ–°å¢ã€‘ç®€æ´è¾“å‡ºé…ç½®
            'debug': {
                'training_output': {
                    'only_show_errors': True,  # é»˜è®¤åªæ˜¾ç¤ºå…³é”®ä¿¡æ¯
                    'show_cache_loading': False,
                    'show_attention_collection': False,
                    'show_state_manager_details': False,
                    'show_metis_details': False,
                    'show_scenario_generation': False,
                    'use_rich_status_panel': True  # ä½¿ç”¨RichçŠ¶æ€é¢æ¿
                },
                'verbose_logging': False
            }
        }

    def _setup_device(self) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        device_config = self.config['system'].get('device', 'auto')
        if device_config == 'auto':
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            device = torch.device(device_config)

        safe_print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
        return device

    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        dirs = [
            self.config['data']['cache_dir'],
            self.config['logging']['log_dir'],
            self.config['logging']['checkpoint_dir'],
            self.config['visualization']['figures_dir'],
            'data/models', 'data/output', 'data/experiments'
        ]

        for dir_path in dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)

    def get_training_configs(self) -> Dict[str, Dict[str, Any]]:
        """è·å–ä¸åŒè®­ç»ƒæ¨¡å¼çš„é…ç½®"""
        base_config = self.config.copy()
        
        # æ™ºèƒ½è‡ªé€‚åº”é»˜è®¤å¯ç”¨
        base_config['adaptive_curriculum'] = {'enabled': True}

        configs = {
            'fast': {
                **base_config,
                **base_config.get('fast', {}),
                'training': {
                    **base_config['training'],
                    **base_config.get('fast', {}).get('training', {}),
                    'mode': 'fast'
                }
            },
            'full': {
                **base_config,
                **base_config.get('full', {}),
                'training': {
                    **base_config['training'],
                    **base_config.get('full', {}).get('training', {}),
                    'mode': 'full'
                },
                'parallel_training': {
                    **base_config['parallel_training'],
                    **base_config.get('full', {}).get('parallel_training', {})
                }
            },
            'ieee118': {
                **base_config,
                **base_config.get('ieee118', {}),
                'training': {
                    **base_config['training'],
                    **base_config.get('ieee118', {}).get('training', {}),
                    'mode': 'ieee118'
                },
                'parallel_training': {
                    **base_config['parallel_training'],
                    **base_config.get('ieee118', {}).get('parallel_training', {})
                }
            }
        }

        return configs

    def run_training(self, mode: str = 'fast', **kwargs) -> Dict[str, Any]:
        """è¿è¡Œè®­ç»ƒ"""
        print(f"\nğŸš€ å¼€å§‹{mode.upper()}æ¨¡å¼è®­ç»ƒ")

        # è·å–æ¨¡å¼é…ç½®
        configs = self.get_training_configs()
        if mode not in configs:
            print(f"âš ï¸ æœªçŸ¥è®­ç»ƒæ¨¡å¼: {mode}ï¼Œä½¿ç”¨å¿«é€Ÿæ¨¡å¼")
            mode = 'fast'

        config = configs[mode]

        # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
        for key, value in kwargs.items():
            if '.' in key:
                keys = key.split('.')
                current = config
                for k in keys[:-1]:
                    if k not in current:
                        current[k] = {}
                    current = current[k]
                current[keys[-1]] = value
            else:
                config[key] = value

        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(config['system']['seed'])
        np.random.seed(config['system']['seed'])

        try:
            # æ ¹æ®é…ç½®é€‰æ‹©è®­ç»ƒæµç¨‹
            if config['parallel_training']['enabled']:
                return self._run_parallel_training(config)
            elif config.get('adaptive_curriculum', {}).get('enabled', False):
                return self._run_curriculum_training(config)
            else:
                return self._run_standard_training(config)

        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _run_standard_training(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œæ ‡å‡†è®­ç»ƒ"""
        # åªåœ¨ç®€æ´æ¨¡å¼ä¸‹æ˜¾ç¤ºå…³é”®ä¿¡æ¯
        from code.src.rich_output import rich_info
        only_show_errors = config.get('debug', {}).get('training_output', {}).get('only_show_errors', True)
        
        if not only_show_errors:
            print("ğŸ“Š æ ‡å‡†è®­ç»ƒæ¨¡å¼")
            print("ğŸ”§ ç³»ç»ŸçŠ¶æ€: ç°ä»£åŒ–é‡æ„å®Œæˆ - ç»Ÿä¸€å¥–åŠ±ç³»ç»Ÿ")
        else:
            rich_info("å¯åŠ¨æ ‡å‡†è®­ç»ƒæ¨¡å¼", show_always=True)

        # å¯¼å…¥å¿…è¦æ¨¡å—
        from code.src.data_processing import PowerGridDataProcessor
        from code.src.gat import create_hetero_graph_encoder
        from code.src.rl.environment import PowerGridPartitioningEnv
        from code.src.rl.agent import PPOAgent

        # 1. æ•°æ®å¤„ç†
        if not only_show_errors:
            print("\n1ï¸âƒ£ æ•°æ®å¤„ç†...")
        
        processor = PowerGridDataProcessor(
            normalize=config['data']['normalize'],
            cache_dir=config['data']['cache_dir']
        )

        # åŠ è½½æ•°æ®
        mpc = load_power_grid_data(config['data']['case_name'])
        hetero_data = processor.graph_from_mpc(mpc, config).to(self.device)
        
        if not only_show_errors:
            print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {hetero_data}")

        # 2. GATç¼–ç å™¨
        if not only_show_errors:
            print("\n2ï¸âƒ£ GATç¼–ç å™¨...")
            
        gat_config = config['gat']
        encoder = create_hetero_graph_encoder(
            hetero_data,
            hidden_channels=gat_config['hidden_channels'],
            gnn_layers=gat_config['gnn_layers'],
            heads=gat_config['heads'],
            output_dim=gat_config['output_dim']
        ).to(self.device)

        with torch.no_grad():
            node_embeddings, attention_weights = encoder.encode_nodes_with_attention(hetero_data, config)

        if not only_show_errors:
            print(f"âœ… ç¼–ç å™¨åˆå§‹åŒ–å®Œæˆ")

        # 3. ç¯å¢ƒï¼ˆæ”¯æŒåœºæ™¯ç”Ÿæˆï¼‰
        if not only_show_errors:
            print("\n3ï¸âƒ£ å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ...")
        env_config = config['environment']
        scenario_config = config.get('scenario_generation', {})
        use_scenario_generation = scenario_config.get('enabled', True)  # é»˜è®¤å¯ç”¨åœºæ™¯ç”Ÿæˆ

        if use_scenario_generation:
            if not only_show_errors:
                print("ğŸ­ å¯ç”¨åœºæ™¯ç”ŸæˆåŠŸèƒ½...")
                
            # ä½¿ç”¨æ”¯æŒåœºæ™¯ç”Ÿæˆçš„Gymç¯å¢ƒåŒ…è£…å™¨
            try:
                from code.src.rl.gym_wrapper import PowerGridPartitionGymEnv

                # ç¡®ä¿è®¾å¤‡é…ç½®æ­£ç¡®ä¼ é€’
                config_copy = config.copy()
                config_copy['system']['device'] = str(self.device)  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²

                gym_env = PowerGridPartitionGymEnv(
                    base_case_data=mpc,
                    config=config_copy,
                    use_scenario_generator=True,
                    scenario_seed=config['system']['seed']
                )

                # é‡ç½®ç¯å¢ƒä»¥è·å–åˆå§‹çŠ¶æ€
                obs_array, info = gym_env.reset()
                env = gym_env.internal_env  # è·å–å†…éƒ¨çš„PowerGridPartitioningEnv

                rich_info(f"åœºæ™¯ç”Ÿæˆç¯å¢ƒ: {env.total_nodes}èŠ‚ç‚¹, {env.num_partitions}åˆ†åŒº", show_always=True)

            except ImportError as e:
                from code.src.rich_output import rich_warning
                rich_warning(f"åœºæ™¯ç”Ÿæˆæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
                use_scenario_generation = False

        if not use_scenario_generation:
            if not only_show_errors:
                print("ğŸ“Š ä½¿ç”¨æ ‡å‡†ç¯å¢ƒï¼ˆæ— åœºæ™¯ç”Ÿæˆï¼‰...")
                
            env = PowerGridPartitioningEnv(
                hetero_data=hetero_data,
                node_embeddings=node_embeddings,
                num_partitions=env_config['num_partitions'],
                reward_weights=env_config['reward_weights'],
                max_steps=env_config['max_steps'],
                device=self.device,
                attention_weights=attention_weights,
                config=config
            )

            rich_info(f"æ ‡å‡†ç¯å¢ƒ: {env.total_nodes}èŠ‚ç‚¹, {env.num_partitions}åˆ†åŒº", show_always=True)

        # 4. æ™ºèƒ½ä½“
        if not only_show_errors:
            print("\n4ï¸âƒ£ PPOæ™ºèƒ½ä½“...")
        agent_config = config['agent']
        node_embedding_dim = env.state_manager.embedding_dim
        region_embedding_dim = node_embedding_dim * 2

        agent = PPOAgent(
            node_embedding_dim=node_embedding_dim,
            region_embedding_dim=region_embedding_dim,
            num_partitions=env.num_partitions,
            lr_actor=agent_config['lr_actor'],
            lr_critic=agent_config['lr_critic'],
            gamma=agent_config['gamma'],
            eps_clip=agent_config['eps_clip'],
            k_epochs=agent_config['k_epochs'],
            entropy_coef=agent_config['entropy_coef'],
            value_coef=agent_config['value_coef'],
            device=self.device,
            max_grad_norm=agent_config.get('max_grad_norm', None),
            # ã€ä¿®æ”¹ã€‘ä¼ é€’ç‹¬ç«‹çš„è°ƒåº¦å™¨é…ç½®
            actor_scheduler_config=agent_config.get('actor_scheduler', {}),
            critic_scheduler_config=agent_config.get('critic_scheduler', {})
        )

        if not only_show_errors:
            print(f"âœ… æ™ºèƒ½ä½“åˆ›å»ºå®Œæˆ")

        # 5. è®­ç»ƒ
        rich_info("å¼€å§‹è®­ç»ƒ...", show_always=True)
        # å¦‚æœä½¿ç”¨äº†åœºæ™¯ç”Ÿæˆï¼Œä¼ é€’gym_envç»™è®­ç»ƒå™¨
        gym_env_ref = gym_env if use_scenario_generation and 'gym_env' in locals() else None
        trainer = UnifiedTrainer(agent=agent, env=env, config=config, gym_env=gym_env_ref)

        training_config = config['training']
        history = trainer.train(
            num_episodes=training_config['num_episodes'],
            max_steps_per_episode=training_config['max_steps_per_episode'],
            update_interval=training_config['update_interval']
        )

        # 6. ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
        model_dir = Path(config['logging']['checkpoint_dir']) / 'models'
        model_dir.mkdir(parents=True, exist_ok=True)
        model_path = model_dir / f"agent_{config['data']['case_name']}_best.pth"

        agent.save(str(model_path))
        rich_info(f"æ¨¡å‹å·²ä¿å­˜: {model_path}", show_always=True)

        # 7. åŸºç¡€è¯„ä¼°
        rich_info("å¼€å§‹è¯„ä¼°...", show_always=True)
        eval_stats = trainer.evaluate()

        trainer.close()

        return {
            'success': True,
            'mode': 'standard',
            'config': config,
            'history': history,
            'eval_stats': eval_stats,
            'best_reward': trainer.logger.best_reward,
            'model_path': str(model_path)
        }

    def _run_parallel_training(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå¹¶è¡Œè®­ç»ƒ"""
        print("ğŸŒ å¹¶è¡Œè®­ç»ƒæ¨¡å¼")

        if not self.deps['stable_baselines3']:
            print("âŒ å¹¶è¡Œè®­ç»ƒéœ€è¦stable-baselines3ï¼Œè¯·å®‰è£…ï¼špip install stable-baselines3")
            return {'success': False, 'error': 'Missing stable-baselines3'}

        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰gymå’Œstable_baselines3
            try:
                import gym
                from stable_baselines3 import PPO
                has_sb3 = True
            except ImportError:
                has_sb3 = False

            if has_sb3:
                # ä½¿ç”¨Stable-Baselines3çš„å¹¶è¡Œè®­ç»ƒ
                return self._run_sb3_parallel_training(config)
            else:
                print("âš ï¸ ç¼ºå°‘stable-baselines3ï¼Œè·³è¿‡å¹¶è¡Œè®­ç»ƒ")
                return {'success': False, 'error': 'Missing stable-baselines3'}

        except Exception as e:
            print(f"âŒ å¹¶è¡Œè®­ç»ƒå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _run_sb3_parallel_training(self, config: Dict[str, Any]) -> Dict[str, Any]:
        from code.src.data_processing import PowerGridDataProcessor
        from code.src.rl.gym_wrapper import make_parallel_env
        from stable_baselines3 import PPO

        # ã€ä¿®å¤ã€‘åœ¨åˆ›å»ºå¹¶è¡Œç¯å¢ƒå‰ï¼Œå°†ä¸»è¿›ç¨‹ä¸­å·²è§£æå¥½çš„è®¾å¤‡åç§°æ›´æ–°åˆ°é…ç½®å­—å…¸ä¸­
        # é¿å…å­è¿›ç¨‹æ”¶åˆ° "auto" å­—ç¬¦ä¸²å¯¼è‡´ torch.device() æŠ¥é”™
        config['system']['device'] = str(self.device)
        print(f"ğŸ”§ å¹¶è¡Œè®­ç»ƒè®¾å¤‡é…ç½®å·²æ›´æ–°: {config['system']['device']}")

        # 1. æ•°æ®å¤„ç†
        print("\n1ï¸âƒ£ æ•°æ®å¤„ç†...")
        processor = PowerGridDataProcessor(
            normalize=config['data']['normalize'],
            cache_dir=config['data']['cache_dir']
        )

        # åŠ è½½æ•°æ®
        mpc = load_power_grid_data(config['data']['case_name'])

        # 2. åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
        print("\n2ï¸âƒ£ åˆ›å»ºå¹¶è¡Œç¯å¢ƒ...")
        parallel_config = config['parallel_training']

        parallel_env = make_parallel_env(
            base_case_data=mpc,
            config=config,
            num_envs=parallel_config['num_cpus'],
            use_scenario_generator=parallel_config['scenario_generation']
        )

        # 3. åˆ›å»ºå¹¶è®­ç»ƒPPOæ™ºèƒ½ä½“
        print("\n3ï¸âƒ£ åˆ›å»ºPPOæ™ºèƒ½ä½“...")
        model = PPO(
            "MlpPolicy",
            parallel_env,
            verbose=1,
            tensorboard_log=config['logging']['log_dir']
        )

        print("\n4ï¸âƒ£ å¼€å§‹å¹¶è¡Œè®­ç»ƒ...")
        model.learn(total_timesteps=parallel_config['total_timesteps'])

        # 4. ä¿å­˜æ¨¡å‹
        model_path = Path(config['logging']['checkpoint_dir']) / "parallel_model"
        model_path.parent.mkdir(exist_ok=True, parents=True)
        model.save(str(model_path))

        return {
            'success': True,
            'mode': 'parallel',
            'model_path': str(model_path),
            'total_timesteps': parallel_config['total_timesteps'],
            'config': config
        }

    def _run_curriculum_training(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œè¯¾ç¨‹å­¦ä¹ è®­ç»ƒ"""
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ™ºèƒ½è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ 
        if config.get('adaptive_curriculum', {}).get('enabled', False):
            return self._run_adaptive_curriculum_training(config)
        else:
            return self._run_traditional_curriculum_training(config)

    def _run_traditional_curriculum_training(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œä¼ ç»Ÿè¯¾ç¨‹å­¦ä¹ è®­ç»ƒ"""
        print("ğŸ“š ä¼ ç»Ÿè¯¾ç¨‹å­¦ä¹ è®­ç»ƒæ¨¡å¼")

        curriculum_config = config['curriculum']
        start_partitions = curriculum_config['start_partitions']
        end_partitions = curriculum_config['end_partitions']
        episodes_per_stage = curriculum_config['episodes_per_stage']

        results = []

        for num_partitions in range(start_partitions, end_partitions + 1):
            print(f"\nğŸ“– è¯¾ç¨‹é˜¶æ®µ: {num_partitions}ä¸ªåˆ†åŒº")

            # æ›´æ–°é…ç½®
            stage_config = config.copy()
            stage_config['environment']['num_partitions'] = num_partitions
            stage_config['training']['num_episodes'] = episodes_per_stage

            # è¿è¡Œè¯¥é˜¶æ®µçš„è®­ç»ƒ
            stage_result = self._run_standard_training(stage_config)
            results.append(stage_result)

            if not stage_result['success']:
                break

        return {
            'success': all(r['success'] for r in results),
            'mode': 'traditional_curriculum',
            'config': config,
            'stage_results': results
        }

    def _run_adaptive_curriculum_training(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œæ™ºèƒ½è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ"""
        print("ğŸ§  æ™ºèƒ½è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæ¨¡å¼")
        print("=" * 60)

        try:
            # å¯¼å…¥æ™ºèƒ½å¯¼æ¼”ç³»ç»Ÿ
            from code.src.rl.adaptive import AdaptiveDirector

            # è·å–åŸºç¡€è®­ç»ƒæ¨¡å¼
            base_mode = self._detect_base_mode(config)

            # åˆå§‹åŒ–æ™ºèƒ½å¯¼æ¼”
            director = AdaptiveDirector(config, base_mode)
            print(f"âœ… æ™ºèƒ½å¯¼æ¼”ç³»ç»Ÿå·²åˆå§‹åŒ– (åŸºç¡€æ¨¡å¼: {base_mode})")

            # è¿è¡Œè‡ªé€‚åº”è®­ç»ƒ
            result = self._run_adaptive_training_with_director(config, director)

            return {
                'success': result.get('success', False),
                'mode': 'adaptive_curriculum',
                'config': config,
                'director_status': director.get_status_summary(),
                **result
            }

        except ImportError as e:
            print(f"âŒ æ— æ³•å¯¼å…¥æ™ºèƒ½å¯¼æ¼”ç³»ç»Ÿ: {e}")
            print("ğŸ”„ å›é€€åˆ°ä¼ ç»Ÿè¯¾ç¨‹å­¦ä¹ æ¨¡å¼")
            return self._run_traditional_curriculum_training(config)
        except Exception as e:
            print(f"âŒ æ™ºèƒ½è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e), 'mode': 'adaptive_curriculum'}

    def _detect_base_mode(self, config: Dict[str, Any]) -> str:
        """æ£€æµ‹åŸºç¡€è®­ç»ƒæ¨¡å¼"""
        # æ ¹æ®é…ç½®ç‰¹å¾æ£€æµ‹åŸºç¡€æ¨¡å¼
        num_episodes = config.get('training', {}).get('num_episodes', 1500)
        parallel_enabled = config.get('parallel_training', {}).get('enabled', False)

        if num_episodes >= 4000 or parallel_enabled:
            if num_episodes >= 4000:
                return 'ieee118'
            else:
                return 'full'
        else:
            return 'fast'

    def _run_adaptive_training_with_director(self, config: Dict[str, Any], director) -> Dict[str, Any]:
        """ä½¿ç”¨æ™ºèƒ½å¯¼æ¼”è¿è¡Œè‡ªé€‚åº”è®­ç»ƒ"""
        print("\nğŸ¬ å¯åŠ¨æ™ºèƒ½å¯¼æ¼”è®­ç»ƒæµç¨‹...")

        # 1. ç¯å¢ƒå’Œæ™ºèƒ½ä½“åˆå§‹åŒ–
        print("\n1ï¸âƒ£ åˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“...")
        mpc = load_power_grid_data(config['data']['case_name'])

        # åˆ›å»ºç¯å¢ƒï¼ˆä½¿ç”¨åœºæ™¯ç”Ÿæˆï¼‰
        if config['scenario_generation']['enabled']:
            from code.src.rl.gym_wrapper import PowerGridPartitionGymEnv

            # ç¡®ä¿è®¾å¤‡é…ç½®æ­£ç¡®ä¼ é€’
            config_copy = config.copy()
            config_copy['system']['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'

            gym_env = PowerGridPartitionGymEnv(
                base_case_data=mpc,
                config=config_copy,
                use_scenario_generator=True,
                scenario_seed=config['system']['seed']
            )
            # é‡ç½®ç¯å¢ƒä»¥è·å–åˆå§‹çŠ¶æ€
            obs_array, info = gym_env.reset()
            env = gym_env.internal_env
        else:
            from code.src.rl.environment import PowerGridPartitioningEnv
            env = PowerGridPartitioningEnv(mpc, config)
            gym_env = None

        # åˆ›å»ºæ™ºèƒ½ä½“
        from code.src.rl.agent import PPOAgent

        # è·å–æ­£ç¡®çš„åµŒå…¥ç»´åº¦
        node_embedding_dim = env.state_manager.embedding_dim
        region_embedding_dim = node_embedding_dim * 2

        agent = PPOAgent(
            node_embedding_dim=node_embedding_dim,
            region_embedding_dim=region_embedding_dim,
            num_partitions=env.num_partitions,
            lr_actor=config['agent']['lr_actor'],
            lr_critic=config['agent']['lr_critic'],
            gamma=config['agent']['gamma'],
            eps_clip=config['agent']['eps_clip'],
            k_epochs=config['agent']['k_epochs'],
            entropy_coef=config['agent']['entropy_coef'],
            value_coef=config['agent']['value_coef'],
            device=env.device,
            actor_scheduler_config=config['agent'].get('actor_scheduler'),
            critic_scheduler_config=config['agent'].get('critic_scheduler')
        )

        # 2. æ™ºèƒ½è‡ªé€‚åº”è®­ç»ƒå¾ªç¯
        print("\n2ï¸âƒ£ å¼€å§‹æ™ºèƒ½è‡ªé€‚åº”è®­ç»ƒ...")
        num_episodes = config['training']['num_episodes']
        max_steps_per_episode = config['training']['max_steps_per_episode']
        update_interval = config['training']['update_interval']

        # åˆå§‹åŒ–è®­ç»ƒæ—¥å¿—
        logger = TrainingLogger(config, num_episodes)

        # æ™ºèƒ½å¯¼æ¼”çŠ¶æ€è·Ÿè¸ª
        director_decisions = []
        stage_transitions = []

        # ç§»é™¤å¤æ‚çš„Richè¡¨æ ¼ï¼Œä½¿ç”¨ç®€å•è¿›åº¦æ¡
        use_rich_table = False

        # åˆ›å»ºç®€å•çš„è¿›åº¦æ¡
        try:
            from tqdm import tqdm
            progress_bar = tqdm(total=num_episodes, desc="ğŸ§  æ™ºèƒ½è‡ªé€‚åº”è®­ç»ƒ",
                              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')
        except ImportError:
            progress_bar = None

        try:
            for episode in range(num_episodes):
                # é‡ç½®ç¯å¢ƒ
                if gym_env is not None:
                    obs_array, info = gym_env.reset()
                    env = gym_env.internal_env
                    state, _ = env.reset()
                else:
                    state, _ = env.reset()

                episode_reward = 0
                episode_length = 0

                # Episodeæ‰§è¡Œ
                for step in range(max_steps_per_episode):
                    action, log_prob, value = agent.select_action(state, training=True)

                    if action is None:
                        break

                    state, reward, terminated, truncated, info = env.step(action)
                    done = terminated or truncated

                    agent.store_experience(state, action, reward, log_prob, value, done)
                    episode_reward += reward
                    episode_length += 1

                    if done:
                        break

                # æ”¶é›†episodeä¿¡æ¯ - ã€ä¿®å¤ã€‘é€‚é…æ–°å¥–åŠ±ç³»ç»Ÿçš„æŒ‡æ ‡åç§°
                episode_info = {
                    'episode': episode,
                    'reward': episode_reward,
                    'episode_length': episode_length,
                    'success': info.get('success', False),
                    # æ–°ç³»ç»Ÿä½¿ç”¨'cv'è€Œä¸æ˜¯'load_cv'ï¼Œæä¾›å‘åå…¼å®¹
                    'load_cv': info.get('cv', info.get('load_cv', 1.0)),
                    'cv': info.get('cv', info.get('load_cv', 1.0)),  # åŒæ—¶æä¾›æ–°åç§°
                    'coupling_ratio': info.get('coupling_ratio', 1.0),
                    'connectivity': info.get('connectivity', 0.0),
                    **info
                }

                # æ™ºèƒ½å¯¼æ¼”å†³ç­–
                director_decision = director.step(episode, episode_info)
                director_decisions.append(director_decision)

                # åº”ç”¨æ™ºèƒ½å¯¼æ¼”çš„å‚æ•°è°ƒæ•´
                self._apply_director_decision(env, agent, director_decision)

                # è®°å½•é˜¶æ®µè½¬æ¢
                if 'stage_info' in director_decision:
                    stage_info = director_decision['stage_info']
                    if len(stage_transitions) == 0 or stage_transitions[-1]['stage'] != stage_info['current_stage']:
                        stage_transitions.append({
                            'episode': episode,
                            'stage': stage_info['current_stage'],
                            'stage_name': stage_info['stage_name']
                        })

                # æ›´æ–°è¿›åº¦æ¡
                if progress_bar:
                    # è·å–å½“å‰é˜¶æ®µä¿¡æ¯
                    stage_name = "unknown"
                    if director_decision and 'stage_info' in director_decision:
                        stage_name = director_decision['stage_info'].get('stage_name', 'unknown')

                    # æ›´æ–°è¿›åº¦æ¡æè¿°
                    current_best = max([r for r in logger.episode_rewards if r is not None] + [-float('inf')])
                    desc = f"ğŸ§  æ™ºèƒ½è‡ªé€‚åº”è®­ç»ƒ | é˜¶æ®µ: {stage_name} | å½“å‰: {episode_reward:.3f} | æœ€ä½³: {current_best:.3f}"
                    progress_bar.set_description(desc)
                    progress_bar.update(1)

                # è®°å½•è®­ç»ƒæ—¥å¿—
                logger.log_episode(episode, episode_reward, episode_length, episode_info)

                # æ™ºèƒ½ä½“æ›´æ–°
                if episode % update_interval == 0 and episode > 0:
                    try:
                        training_stats = agent.update()
                        if training_stats:
                            logger.log_training_step(
                                episode,
                                training_stats.get('actor_loss'),
                                training_stats.get('critic_loss'),
                                training_stats.get('entropy')
                            )
                    except Exception as e:
                        print(f"âš ï¸ Episode {episode} æ™ºèƒ½ä½“æ›´æ–°å¤±è´¥: {e}")

                # å®šæœŸä¿å­˜ä¸­é—´ç»“æœ
                if episode % config['training']['save_interval'] == 0 and episode > 0:
                    self._save_adaptive_intermediate_results(episode, director, logger)

            # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
            model_dir = Path(config['logging']['checkpoint_dir']) / 'models'
            model_dir.mkdir(parents=True, exist_ok=True)
            model_path = model_dir / f"agent_{config['data']['case_name']}_adaptive_best.pth"

            agent.save(str(model_path))
            print(f"ğŸ’¾ è‡ªé€‚åº”è®­ç»ƒæ¨¡å‹å·²ä¿å­˜: {model_path}")

            # è®­ç»ƒå®Œæˆç»Ÿè®¡
            final_stats = logger.get_statistics()
            director_summary = director.get_status_summary()

            # å…³é—­è¿›åº¦æ¡
            if progress_bar:
                progress_bar.close()

            # ç»Ÿè®¡é˜¶æ®µè½¬æ¢ä¿¡æ¯
            emergency_transitions = sum(1 for t in stage_transitions if t['stage_name'] == 'emergency_recovery')
            normal_transitions = len(stage_transitions) - emergency_transitions

            # ç®€å•çš„æ–‡æœ¬è¾“å‡ºè®­ç»ƒç»“æœ
            print("\nğŸ¯ æ™ºèƒ½è‡ªé€‚åº”è®­ç»ƒå®Œæˆ")
            print("=" * 50)
            print(f"æ€»å›åˆæ•°: {final_stats.get('total_episodes', 0)}")
            print(f"æœ€ä½³å¥–åŠ±: {final_stats.get('best_reward', 0):.4f}")
            print(f"å¹³å‡å¥–åŠ±: {final_stats.get('mean_reward', 0):.4f}")
            print(f"æœ€ç»ˆé˜¶æ®µ: {director_summary['current_stage']}")
            print(f"æ™ºèƒ½è½¬æ¢: {normal_transitions}æ¬¡æ­£å¸¸ + {emergency_transitions}æ¬¡ç´§æ€¥æ¢å¤")

            return {
                'success': True,
                'episode_rewards': logger.episode_rewards,
                'episode_lengths': logger.episode_lengths,
                'training_stats': final_stats,
                'director_decisions': director_decisions,
                'stage_transitions': stage_transitions,
                'director_summary': director_summary,
                'model_path': str(model_path)
            }

        except Exception as e:
            # ç¡®ä¿å…³é—­è¿›åº¦æ¡
            if progress_bar:
                progress_bar.close()
            print(f"âŒ æ™ºèƒ½è‡ªé€‚åº”è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

        finally:
            # æ¸…ç†èµ„æº
            if progress_bar:
                try:
                    progress_bar.close()
                except:
                    pass

    def _apply_director_decision(self, env, agent, decision: Dict[str, Any]):
        """åº”ç”¨æ™ºèƒ½å¯¼æ¼”çš„å†³ç­–åˆ°ç¯å¢ƒå’Œæ™ºèƒ½ä½“ - å¢å¼ºç‰ˆæ”¯æŒåŠ¨æ€çº¦æŸ"""
        import builtins

        try:
            # é™é»˜æ¨¡å¼ï¼šå‡å°‘æ—¥å¿—è¾“å‡º
            verbose = self.config.get('debug', {}).get('adaptive_curriculum_verbose', False)

            # ã€æ–°å¢ã€‘æ›´æ–°åŠ¨æ€çº¦æŸå‚æ•°
            constraint_params = {}

            # çº¦æŸæ¨¡å¼è®¾ç½®
            if 'connectivity_penalty' in decision:
                constraint_params['connectivity_penalty'] = decision['connectivity_penalty']
                # æ ¹æ®æƒ©ç½šå¼ºåº¦è‡ªåŠ¨è®¾ç½®çº¦æŸæ¨¡å¼
                if decision['connectivity_penalty'] > 0:
                    constraint_params['constraint_mode'] = 'soft'
                else:
                    constraint_params['constraint_mode'] = 'hard'

            if 'action_mask_relaxation' in decision:
                constraint_params['action_mask_relaxation'] = decision['action_mask_relaxation']

            # åº”ç”¨åŠ¨æ€çº¦æŸå‚æ•°åˆ°ç¯å¢ƒ
            if constraint_params and hasattr(env, 'update_dynamic_constraints'):
                env.update_dynamic_constraints(constraint_params)
                if verbose:
                    print(f"ğŸ”§ æ›´æ–°åŠ¨æ€çº¦æŸå‚æ•°: {constraint_params}")

            # æ›´æ–°ç¯å¢ƒå¥–åŠ±å‚æ•°
            if hasattr(env, 'reward_function') and 'reward_weights' in decision:
                reward_weights = decision['reward_weights']
                if hasattr(env.reward_function, 'update_weights'):
                    if verbose:
                        env.reward_function.update_weights(reward_weights)
                    else:
                        # ä¸´æ—¶ç¦ç”¨æ‰“å°
                        original_print = builtins.print
                        builtins.print = lambda *args, **kwargs: None
                        env.reward_function.update_weights(reward_weights)
                        builtins.print = original_print

            # ã€ä¿ç•™ã€‘å‘åå…¼å®¹çš„è¿é€šæ€§æƒ©ç½šè®¾ç½®
            if 'connectivity_penalty' in decision:
                if hasattr(env, 'connectivity_penalty'):
                    env.connectivity_penalty = decision['connectivity_penalty']

            # æ›´æ–°æ™ºèƒ½ä½“å­¦ä¹ ç‡
            if 'learning_rate_factor' in decision and decision['learning_rate_factor'] != 1.0:
                if hasattr(agent, 'update_learning_rate'):
                    if verbose:
                        agent.update_learning_rate(decision['learning_rate_factor'])
                    else:
                        # ä¸´æ—¶ç¦ç”¨æ‰“å°
                        original_print = builtins.print
                        builtins.print = lambda *args, **kwargs: None
                        agent.update_learning_rate(decision['learning_rate_factor'])
                        builtins.print = original_print

        except Exception as e:
            # ç¡®ä¿ä½¿ç”¨åŸå§‹çš„printå‡½æ•°ï¼Œé¿å…è¢«ä¸´æ—¶æ›¿æ¢å½±å“
            print(f"âš ï¸ åº”ç”¨å¯¼æ¼”å†³ç­–æ—¶å‡ºé”™: {e}")
            import traceback
            if verbose:
                traceback.print_exc()

    def _save_adaptive_intermediate_results(self, episode: int, director, logger):
        """ä¿å­˜æ™ºèƒ½è‡ªé€‚åº”è®­ç»ƒçš„ä¸­é—´ç»“æœ"""
        try:
            from pathlib import Path
            import json

            checkpoint_dir = Path(self.config['logging']['checkpoint_dir'])
            checkpoint_dir.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜è®­ç»ƒç»Ÿè®¡
            stats = logger.get_statistics()
            stats_file = checkpoint_dir / f"adaptive_training_stats_episode_{episode}.json"
            with open(stats_file, 'w') as f:
                json.dump(stats, f, indent=2)

            # ä¿å­˜æ™ºèƒ½å¯¼æ¼”çŠ¶æ€
            director_status = director.get_status_summary()
            director_file = checkpoint_dir / f"director_status_episode_{episode}.json"
            with open(director_file, 'w') as f:
                json.dump(director_status, f, indent=2)

            print(f"ğŸ’¾ æ™ºèƒ½è‡ªé€‚åº”ä¸­é—´ç»“æœå·²ä¿å­˜: episode {episode}")

        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ™ºèƒ½è‡ªé€‚åº”ä¸­é—´ç»“æœå¤±è´¥: {e}")

    def save_results(self, results: Dict[str, Any], output_dir: str = 'data/experiments'):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        exp_dir = Path(output_dir) / f"training_{timestamp}"
        exp_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ç»“æœ
        results_file = exp_dir / "results.json"
        with open(results_file, 'w') as f:
            # è¿‡æ»¤ä¸èƒ½åºåˆ—åŒ–çš„å¯¹è±¡
            serializable_results = {}
            for key, value in results.items():
                if key not in ['env', 'agent', 'trainer']:
                    try:
                        json.dumps(value)
                        serializable_results[key] = value
                    except (TypeError, ValueError):
                        serializable_results[key] = str(value)

            json.dump(serializable_results, f, indent=2)

        print(f"ğŸ’¾ è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {exp_dir}")
        return exp_dir


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç”µåŠ›ç½‘ç»œåˆ†åŒºå¼ºåŒ–å­¦ä¹ ç»Ÿä¸€è®­ç»ƒç³»ç»Ÿ')
    parser.add_argument('--config', type=str, default=None, help='è‡ªå®šä¹‰é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--mode', type=str, default='standard', help='è®­ç»ƒæ¨¡å¼')
    parser.add_argument('--case', type=str, default=None, help='è¦†ç›–ç”µç½‘æ¡ˆä¾‹')
    parser.add_argument('--episodes', type=int, default=None, help='è¦†ç›–è®­ç»ƒå›åˆæ•°')
    parser.add_argument('--tui', action='store_true', help='å¯ç”¨Textual TUIç›‘æ§å™¨')
    parser.add_argument('--device', type=str, default=None, help='è¦†ç›–è®¡ç®—è®¾å¤‡ (cpu/cuda)')

    args = parser.parse_args()

    # å°†æ‰€æœ‰å‘½ä»¤è¡Œå‚æ•°ç»Ÿä¸€å¤„ç†ä¸ºé…ç½®è¦†ç›–é¡¹
    config_overrides = {
        'training': {'mode': args.mode},
        'tui': {'enabled': args.tui}
    }
    if args.case:
        config_overrides['data'] = {'case_name': args.case}
    if args.episodes:
        config_overrides.setdefault('training', {})['num_episodes'] = args.episodes
    if args.device:
        config_overrides['system'] = {'device': args.device}

    try:
        # å°†é…ç½®è¦†ç›–é¡¹ç»Ÿä¸€ä¼ é€’ç»™ç³»ç»Ÿ
        system = UnifiedTrainingSystem(
            config_path=args.config,
            **config_overrides
        )
        
        # ç›´æ¥ä½¿ç”¨ç³»ç»Ÿå†…çš„æœ€ç»ˆé…ç½®æ¥è¿è¡Œè®­ç»ƒ
        results = system.run_training(mode=system.config['training']['mode'])

        # ä¿å­˜ç»“æœ
        if results.get('success', False):
            system.save_results(results, 'experiments')

        # è¾“å‡ºç»“æœæ‘˜è¦
        if results.get('success', False):
            print(f"\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!")
            if 'best_reward' in results:
                print(f"ğŸ† æœ€ä½³å¥–åŠ±: {results['best_reward']:.4f}")
            if 'eval_stats' in results:
                eval_stats = results['eval_stats']
                print(f"ğŸ“Š è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± {eval_stats.get('avg_reward', 0):.4f}, "
                      f"æˆåŠŸç‡ {eval_stats.get('success_rate', 0):.3f}")
        else:
            print(f"\nâŒ è®­ç»ƒå¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return 1

    except Exception as e:
        try:
            print(f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}")
        except UnicodeEncodeError:
            print(f"[ERROR] ç³»ç»Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
