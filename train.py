#!/usr/bin/env python3
"""
ç”µåŠ›ç½‘ç»œåˆ†åŒºå¼ºåŒ–å­¦ä¹ è®­ç»ƒç³»ç»Ÿ v2.0

é‡æ„ç‰ˆæœ¬ - å››ä¸ªè§£è€¦çš„è®­ç»ƒæ¨¡å¼ï¼š
1. BasicTrainer - çº¯åŸºç¡€è®­ç»ƒï¼ˆæ— è¯¾ç¨‹å­¦ä¹ ã€æ— æ‹“æ‰‘å˜åŒ–ï¼‰
2. TopologyTrainer - åªæ‹“æ‰‘å˜åŒ–è®­ç»ƒï¼ˆå¤šå°ºåº¦ç”Ÿæˆå™¨ï¼‰
3. AdaptiveTrainer - åªå‚æ•°è‡ªé€‚åº”è®­ç»ƒï¼ˆAdaptiveDirectorï¼‰
4. UnifiedTrainer - ç»Ÿä¸€å¯¼æ¼”è®­ç»ƒï¼ˆæ‹“æ‰‘+å‚æ•°åè°ƒï¼‰

ä½œè€…ï¼šClaude & User Collaboration
æ—¥æœŸï¼š2025-07-13
ç‰ˆæœ¬ï¼š2.0 - æ¸…æ™°æ¶æ„é‡æ„ç‰ˆ
"""

import sys
import os
from pathlib import Path

# å°† code/src ç›®å½•æ·»åŠ åˆ° Python æœç´¢è·¯å¾„ï¼Œä»¥è§£å†³æ¨¡å—å¯¼å…¥é—®é¢˜
# è¿™ä½¿å¾—è„šæœ¬æ— è®ºä»å“ªé‡Œè¿è¡Œï¼Œéƒ½èƒ½æ­£ç¡®æ‰¾åˆ° rl ç­‰æ¨¡å—
project_root = Path(__file__).resolve().parent
src_path = project_root / 'code' / 'src'
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

import torch
import numpy as np
import argparse
import yaml
import os
import sys
import time
import json
import warnings
import shutil
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
from collections import deque
from abc import ABC, abstractmethod
import logging
from functools import wraps

# æ·»åŠ ä»£ç è·¯å¾„
# sys.path.append(str(Path(__file__).parent / 'code' / 'src'))  <- å†—ä½™ï¼Œå·²ç”±é¡¶éƒ¨çš„ä»£ç å–ä»£
# sys.path.append(str(Path(__file__).parent / 'code')) <- å†—ä½™
# sys.path.append(str(Path(__file__).parent / 'code' / 'utils')) <- å†—ä½™


def handle_exceptions(func):
    """ç»Ÿä¸€å¼‚å¸¸å¤„ç†è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"å‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            raise
    return wrapper


def ensure_device_consistency(obj, target_device):
    """ç¡®ä¿å¯¹è±¡åœ¨ç›®æ ‡è®¾å¤‡ä¸Šï¼Œé¿å…é‡å¤è½¬ç§»"""
    if hasattr(obj, 'device') and obj.device == target_device:
        return obj  # å·²ç»åœ¨ç›®æ ‡è®¾å¤‡ä¸Šï¼Œé¿å…é‡å¤è½¬ç§»
    elif hasattr(obj, 'to'):
        return obj.to(target_device)
    else:
        return obj

# æ ¸å¿ƒæ¨¡å—å¯¼å…¥
try:
    import gymnasium as gym
    from torch_geometric.data import HeteroData
    from code.src.data_processing import PowerGridDataProcessor
    from code.src.gat import create_production_encoder
    from code.src.rl.environment import PowerGridPartitioningEnv
    from code.src.rl.enhanced_environment import EnhancedPowerGridPartitioningEnv, create_enhanced_environment
    from code.src.rl.enhanced_agent import create_enhanced_agent
    from code.src.pretrain.gnn_pretrainer import GNNPretrainer, PretrainConfig
    from code.src.rl.scale_aware_generator import ScaleAwareSyntheticGenerator, GridGenerationConfig
    from code.src.rl.adaptive import AdaptiveDirector
    from code.src.rl.unified_director import UnifiedDirector
    from code.src.rl.gym_wrapper import PowerGridPartitionGymEnv
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = e

# å…¨å±€é…ç½®
torch.autograd.set_detect_anomaly(False)
warnings.filterwarnings("error", message=".*NaN.*", category=RuntimeWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# è®¾ç½®æ—¥å¿—
def setup_logging(verbose: bool = False, debug: bool = False):
    """è®¾ç½®æ—¥å¿—çº§åˆ«å’Œæ ¼å¼"""
    if debug:
        level = logging.DEBUG
    elif verbose:
        level = logging.INFO
    else:
        level = logging.WARNING

    # è®¾ç½®æ ¹æ—¥å¿—çº§åˆ«
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # è®¾ç½®ç‰¹å®šæ¨¡å—çš„æ—¥å¿—çº§åˆ«
    if not verbose:
        # é™é»˜ç¬¬ä¸‰æ–¹åº“çš„æ—¥å¿—
        logging.getLogger('pandapower').setLevel(logging.WARNING)
        logging.getLogger('matplotlib').setLevel(logging.WARNING)
        # ä¿ç•™GNNé¢„è®­ç»ƒçš„å…³é”®ä¿¡æ¯ï¼Œä½†å‡å°‘è¯¦ç»†è¾“å‡º
        logging.getLogger('pretrain.gnn_pretrainer').setLevel(logging.INFO)
        logging.getLogger('rl.enhanced_environment').setLevel(logging.WARNING)
        logging.getLogger('models.actor_network').setLevel(logging.WARNING)
        logging.getLogger('models.partition_encoder').setLevel(logging.WARNING)
        logging.getLogger('rl.enhanced_agent').setLevel(logging.WARNING)
        logging.getLogger('rl.unified_director').setLevel(logging.WARNING)
        logging.getLogger('rl.adaptive').setLevel(logging.WARNING)

    # ç¡®ä¿ä¸»æ¨¡å—æ—¥å¿—å§‹ç»ˆå¯è§
    logging.getLogger(__name__).setLevel(logging.INFO if verbose else logging.WARNING)

# é»˜è®¤è®¾ç½®
setup_logging()
logger = logging.getLogger(__name__)

# ç®€æ´è¾“å‡ºå‡½æ•°
def print_header(title: str):
    """æ‰“å°æ ‡é¢˜"""
    print(f"\nğŸš€ {title}")

def print_status(message: str, status: str = "info"):
    """æ‰“å°çŠ¶æ€ä¿¡æ¯"""
    icons = {"info": "â„¹ï¸", "success": "âœ…", "warning": "âš ï¸", "error": "âŒ"}
    icon = icons.get(status, "â„¹ï¸")
    print(f"{icon} {message}")

def print_progress(current: int, total: int, message: str = ""):
    """æ‰“å°è¿›åº¦ä¿¡æ¯"""
    percentage = (current / total) * 100 if total > 0 else 0
    print(f"ğŸ“Š è¿›åº¦: {current}/{total} ({percentage:.1f}%) {message}")

def print_result(key: str, value: Any):
    """æ‰“å°ç»“æœä¿¡æ¯"""
    print(f"ğŸ“ˆ {key}: {value}")


# =====================================================
# åŸºç¡€è®¾æ–½å±‚
# =====================================================

class DataManager:
    """æ•°æ®åŠ è½½å’Œå¤„ç†ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.data_config = config.get('data', {})
        self.processor = None
        
    def load_power_grid_data(self, case_name: str) -> Dict:
        """åŠ è½½ç”µåŠ›ç½‘ç»œæ•°æ®
        
        Args:
            case_name: æ¡ˆä¾‹åç§° (ieee14, ieee30, ieee57, ieee118)
            
        Returns:
            MATPOWERæ ¼å¼çš„ç”µç½‘æ•°æ®å­—å…¸
        """
        try:
            import pandapower as pp
            import pandapower.networks as pn
            from pandapower.converter import to_mpc
            
            # æ˜ å°„æ¡ˆä¾‹åç§°åˆ°PandaPowerå‡½æ•°
            case_mapping = {
                'ieee14': pn.case14,
                'ieee30': pn.case30,
                'ieee39': pn.case39,
                'ieee57': pn.case57,
                'ieee118': pn.case118
            }
            
            if case_name not in case_mapping:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¡ˆä¾‹: {case_name}")
            
            # åŠ è½½PandaPowerç½‘ç»œ
            net = case_mapping[case_name]()
            logger.info(f"æˆåŠŸåŠ è½½ {case_name.upper()}: {len(net.bus)} èŠ‚ç‚¹, {len(net.line)} çº¿è·¯")
            
            # ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜
            self._fix_pandapower_data_types(net)

            # è¿è¡Œæ½®æµåˆ†æ
            try:
                if getattr(net, "res_bus", None) is None or net.res_bus.empty:
                    pp.runpp(net, init="flat", calculate_voltage_angles=False, numba=False)
            except Exception as e:
                logger.warning(f"æ½®æµåˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
                pass
            
            # è½¬æ¢ä¸ºMATPOWERæ ¼å¼
            mpc_wrap = to_mpc(net)
            mpc = mpc_wrap["mpc"] if isinstance(mpc_wrap, dict) and "mpc" in mpc_wrap else mpc_wrap
            
            return mpc
            
        except ImportError:
            raise ImportError("Pandapoweråº“æœªå®‰è£…æˆ–ä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½ç”µç½‘æ•°æ®")
        except Exception as e:
            logger.error(f"åŠ è½½æ¡ˆä¾‹ '{case_name}' å¤±è´¥: {e}")
            raise

    def _fix_pandapower_data_types(self, net):
        """ä¿®å¤PandaPoweræ•°æ®ç±»å‹é—®é¢˜"""
        try:
            # ä¿®å¤å˜å‹å™¨æ•°æ®ç±»å‹é—®é¢˜
            if hasattr(net, 'trafo') and not net.trafo.empty:
                # ç¡®ä¿å˜å‹å™¨å‚æ•°ä¸ºæ•°å€¼ç±»å‹
                numeric_cols = ['tap_neutral', 'tap_min', 'tap_max', 'tap_pos']
                for col in numeric_cols:
                    if col in net.trafo.columns:
                        net.trafo[col] = pd.to_numeric(net.trafo[col], errors='coerce')
                        # ä¿®å¤pandas FutureWarning - ä½¿ç”¨æ­£ç¡®çš„fillnaæ–¹å¼
                        if col == 'tap_neutral':
                            net.trafo[col] = net.trafo[col].fillna(0.0)
                        elif col in ['tap_min', 'tap_max']:
                            net.trafo[col] = net.trafo[col].fillna(1.0)
                        elif col == 'tap_pos':
                            net.trafo[col] = net.trafo[col].fillna(0)

            # ä¿®å¤å‘ç”µæœºç”µå‹é™åˆ¶é—®é¢˜
            if hasattr(net, 'gen') and not net.gen.empty and hasattr(net, 'bus') and not net.bus.empty:
                for idx, gen in net.gen.iterrows():
                    bus_idx = gen['bus']
                    if bus_idx in net.bus.index:
                        # ç¡®ä¿å‘ç”µæœºç”µå‹è®¾å®šå€¼åœ¨æ¯çº¿ç”µå‹é™åˆ¶èŒƒå›´å†…
                        vm_pu = gen.get('vm_pu', 1.0)
                        max_vm_pu = net.bus.loc[bus_idx, 'max_vm_pu']
                        min_vm_pu = net.bus.loc[bus_idx, 'min_vm_pu']

                        if vm_pu > max_vm_pu:
                            net.gen.loc[idx, 'vm_pu'] = max_vm_pu
                        elif vm_pu < min_vm_pu:
                            net.gen.loc[idx, 'vm_pu'] = min_vm_pu

        except Exception as e:
            logger.warning(f"ä¿®å¤PandaPoweræ•°æ®ç±»å‹æ—¶å‡ºé”™: {e}")
            pass
    
    def setup_processor(self, device: torch.device, timestamp_dir: Optional[str] = None) -> PowerGridDataProcessor:
        """è®¾ç½®æ•°æ®å¤„ç†å™¨"""
        if self.processor is None:
            # å¦‚æœæä¾›äº†æ—¶é—´æˆ³ç›®å½•ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
            if timestamp_dir:
                cache_dir = str(Path(timestamp_dir) / "cache")
            else:
                cache_dir = self.data_config.get('cache_dir', 'data/latest/cache')

            self.processor = PowerGridDataProcessor(
                normalize=self.data_config.get('normalize', True),
                cache_dir=cache_dir
            )
        return self.processor
    
    def process_data(self, mpc: Dict, config: Dict[str, Any], device: torch.device, timestamp_dir: Optional[str] = None) -> HeteroData:
        """å¤„ç†ç”µç½‘æ•°æ®ä¸ºå›¾æ•°æ®"""
        processor = self.setup_processor(device, timestamp_dir)
        hetero_data = processor.graph_from_mpc(mpc, config).to(str(device))
        logger.info(f"æ•°æ®å¤„ç†å®Œæˆ: {hetero_data}")
        return hetero_data


class EnvironmentFactory:
    """ç¯å¢ƒåˆ›å»ºå·¥å‚"""
    
    @staticmethod
    @handle_exceptions
    def create_basic_environment(hetero_data: HeteroData, config: Dict[str, Any]) -> EnhancedPowerGridPartitioningEnv:
        """åˆ›å»ºåŸºç¡€ç¯å¢ƒï¼ˆæ— åœºæ™¯ç”Ÿæˆï¼‰"""
        env_config = config['environment']

        # ç¡®ä¿è®¾å¤‡é…ç½®æ­£ç¡®
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        config_copy = config.copy()
        config_copy['device'] = str(device) # ç¡®ä¿ä¼ é€’çš„æ˜¯å­—ç¬¦ä¸²
        config_copy['reward_weights'] = env_config.get('reward_weights', {})
        config_copy['max_steps'] = env_config.get('max_steps', 200)
        config_copy['enable_features_cache'] = env_config.get('enable_features_cache', True)

        # ä½¿ç”¨é¢„è®­ç»ƒçš„GATç¼–ç å™¨ç”ŸæˆèŠ‚ç‚¹åµŒå…¥
        encoder = AgentFactory.setup_gnn_pretraining(hetero_data, config)
        encoder = encoder.to(device)

        with torch.no_grad():
            node_embeddings = encoder.encode_nodes(hetero_data)

        # è°ƒè¯•ï¼šæ‰“å°èŠ‚ç‚¹åµŒå…¥çš„ç»´åº¦ä¿¡æ¯
        for node_type, embeddings in node_embeddings.items():
            logger.info(f"èŠ‚ç‚¹ç±»å‹ {node_type}: åµŒå…¥å½¢çŠ¶ {embeddings.shape}")

        logger.info(f"GATç¼–ç å™¨ç”ŸæˆèŠ‚ç‚¹åµŒå…¥å®Œæˆ: {list(node_embeddings.keys())}")

        # ä½¿ç”¨åŸæœ‰çš„create_enhanced_environmentå‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°ä¼šå¤„ç†æ‰€æœ‰å¿…éœ€å‚æ•°
        # ï¿½ï¿½ é‡è¦ä¿®å¤ï¼šä¼ é€’ç¼–ç å™¨å®ä¾‹è¿›è¡Œç»Ÿä¸€ç®¡ç†
        env = create_enhanced_environment(
            hetero_data=hetero_data,
            node_embeddings=node_embeddings,
            num_partitions=env_config['num_partitions'],
            config=config_copy,
            use_enhanced=True,
            gat_encoder=encoder
        )
        
        assert env is not None, "ç¯å¢ƒåˆ›å»ºå¤±è´¥ï¼Œè¿”å›äº†None"
        logger.info(f"åŸºç¡€ç¯å¢ƒåˆ›å»ºå®Œæˆ: {env.total_nodes}èŠ‚ç‚¹, {env.num_partitions}åˆ†åŒº")
        return env
    
    @staticmethod
    def create_scenario_environment(mpc: Dict, config: Dict[str, Any], 
                                  scale_generator: Optional[ScaleAwareSyntheticGenerator] = None,
                                  gat_encoder: Optional[Any] = None) -> Tuple[EnhancedPowerGridPartitioningEnv, PowerGridPartitionGymEnv]:
        """åˆ›å»ºåœºæ™¯ç”Ÿæˆç¯å¢ƒ"""
        try:
            # ç¡®ä¿è®¾å¤‡é…ç½®æ­£ç¡®ä¼ é€’
            config_copy = config.copy()
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            config_copy['system']['device'] = str(device)
            
            # ğŸ”§ é‡è¦ä¿®å¤ï¼šä¼ é€’é¢„è®­ç»ƒç¼–ç å™¨å®ä¾‹é¿å…é‡å¤åˆ›å»º
            gym_env = PowerGridPartitionGymEnv(
                base_case_data=mpc,
                config=config_copy,
                gat_encoder=gat_encoder,  # ä¼ é€’ç¼–ç å™¨å®ä¾‹
                use_scenario_generator=scale_generator is None,  # å¦‚æœæœ‰å¤šå°ºåº¦ç”Ÿæˆå™¨å°±ä¸ç”¨é»˜è®¤ç”Ÿæˆå™¨
                scenario_seed=config['system']['seed'],
                scale_generator=scale_generator
            )
            
            # é‡ç½®ç¯å¢ƒè·å–åˆå§‹çŠ¶æ€
            obs_array, info = gym_env.reset()
            env = gym_env.internal_env
            
            assert env is not None, "å†…éƒ¨ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥ï¼Œè¿”å›äº†None"
            logger.info(f"åœºæ™¯ç”Ÿæˆç¯å¢ƒåˆ›å»ºå®Œæˆ: {env.total_nodes}èŠ‚ç‚¹, {env.num_partitions}åˆ†åŒº")
            return env, gym_env
            
        except Exception as e:
            logger.error(f"åˆ›å»ºåœºæ™¯ç”Ÿæˆç¯å¢ƒå¤±è´¥: {e}")
            raise


class AgentFactory:
    """æ™ºèƒ½ä½“åˆ›å»ºå·¥å‚"""
    
    @staticmethod
    def create_agent(env: EnhancedPowerGridPartitioningEnv, config: Dict[str, Any], 
                    device: torch.device) -> Any:
        """åˆ›å»ºå¢å¼ºæ™ºèƒ½ä½“ï¼ˆåŒå¡”æ¶æ„ï¼‰"""
        agent_config = config['agent']

        assert env is not None and env.state_manager is not None
        # è°ƒè¯•ï¼šæ‰“å°çŠ¶æ€ç»´åº¦ä¿¡æ¯
        logger.info(f"ç¯å¢ƒçŠ¶æ€ç»´åº¦: {env.state_manager.embedding_dim}")
        logger.info(f"åˆ†åŒºæ•°: {env.num_partitions}")

        agent = create_enhanced_agent(
            state_dim=env.state_manager.embedding_dim,
            num_partitions=env.num_partitions,
            config=agent_config,
            device=str(device) # ç¡®ä¿ä¼ é€’çš„æ˜¯å­—ç¬¦ä¸²
        )
        
        logger.info("å¢å¼ºæ™ºèƒ½ä½“åˆ›å»ºå®Œæˆï¼ˆåŒå¡”æ¶æ„ï¼‰")
        return agent
    
    @staticmethod
    def setup_gnn_pretraining(hetero_data: Any, config: Dict[str, Any],
                            training_source: Union[List, Any] = None) -> Any:
        """è®¾ç½®GNNé¢„è®­ç»ƒå¹¶é€‰æ‹©æ€§åœ°åŠ è½½æƒé‡ï¼ˆv3.0å¢å¼ºï¼‰

        Args:
            hetero_data: å¼‚æ„å›¾æ•°æ®ï¼Œç”¨äºåˆ›å»ºGATç¼–ç å™¨
            config: é…ç½®å­—å…¸
            training_source: è®­ç»ƒæ•°æ®æºï¼ˆå¯é€‰ï¼‰

        Returns:
            GATç¼–ç å™¨ï¼Œå¯èƒ½å·²åŠ è½½é¢„è®­ç»ƒæƒé‡
        """
        # ğŸ”§ é‡è¦ä¿®å¤ï¼šæ·»åŠ é…ç½®éªŒè¯é€»è¾‘
        if not isinstance(config, dict):
            raise ValueError("configå‚æ•°å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
        
        if 'gat' not in config:
            raise ValueError("é…ç½®ä¸­ç¼ºå°‘'gat'éƒ¨åˆ†")
        
        if not hasattr(hetero_data, 'x_dict') or not hetero_data.x_dict:
            raise ValueError("hetero_dataå¿…é¡»åŒ…å«æœ‰æ•ˆçš„x_dict")
        
        pretrain_config = config.get('gnn_pretrain', {})
        
        # ç»Ÿä¸€åˆ›å»ºç¼–ç å™¨
        from code.src.gat import create_production_encoder
        gat_config = config.get('gat', {})
        
        try:
            gat_encoder = create_production_encoder(hetero_data, gat_config)
            logger.info(f"åˆ›å»ºGATç¼–ç å™¨: {type(gat_encoder).__name__}")
        except Exception as e:
            logger.error(f"GATç¼–ç å™¨åˆ›å»ºå¤±è´¥: {e}")
            raise RuntimeError(f"æ— æ³•åˆ›å»ºGATç¼–ç å™¨: {e}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œé¢„è®­ç»ƒ
        if pretrain_config.get('enabled', False) and training_source:
            try:
                # ... (æ­¤å¤„çœç•¥äº†é¢„è®­ç»ƒçš„è¯¦ç»†å®ç°ï¼Œå› ä¸ºå®ƒåœ¨ run_pretrain.py ä¸­)
                logger.info("åœ¨æ­¤å¤„å°†è°ƒç”¨ run_pretrain.pyï¼Œæ­¤å¤„åªè´Ÿè´£åŠ è½½")
            except Exception as e:
                logger.error(f"GNNé¢„è®­ç»ƒæ‰§è¡Œå¤±è´¥: {e}")
                
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åŠ è½½é¢„è®­ç»ƒæƒé‡
        if pretrain_config.get('load_pretrained_weights', True): # é»˜è®¤ä¸ºTrue
            checkpoint_dir = Path(pretrain_config.get('checkpoint_dir', 'data/latest/pretrain_checkpoints'))
            best_model_path = checkpoint_dir / 'best_model.pt'
            
            if best_model_path.exists():
                try:
                    logger.info(f"æ­£åœ¨ä» {best_model_path} åŠ è½½é¢„è®­ç»ƒæƒé‡...")
                    # ğŸ”§ ç»Ÿä¸€è®¾å¤‡å¤„ç†ï¼šç¡®ä¿æƒé‡åŠ è½½åˆ°æ­£ç¡®è®¾å¤‡
                    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                    checkpoint = torch.load(best_model_path, map_location=device)
                    
                    # å…¼å®¹æ—§æ ¼å¼å’Œæ–°æ ¼å¼çš„æ£€æŸ¥ç‚¹
                    if 'encoder_state_dict' in checkpoint:
                        weights = checkpoint['encoder_state_dict']
                    else:
                        weights = checkpoint

                    gat_encoder.load_state_dict(weights)
                    logger.info("âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒGNNæƒé‡ã€‚")
                except Exception as e:
                    logger.warning(f"âš ï¸ åŠ è½½é¢„è®­ç»ƒGNNæƒé‡å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„GNNã€‚")
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡æ–‡ä»¶: {best_model_path}ã€‚å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„GNNã€‚")
        else:
            logger.info("æœªå¯ç”¨åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„GNNã€‚")
            
        return gat_encoder


class LoggerFactory:
    """æ—¥å¿—ç³»ç»Ÿå·¥å‚"""
    
    @staticmethod
    def create_training_logger(config: Dict[str, Any], total_episodes: int) -> 'TrainingLogger':
        """åˆ›å»ºè®­ç»ƒæ—¥å¿—è®°å½•å™¨"""
        return TrainingLogger(config, total_episodes)


# =====================================================
# æ ¸å¿ƒç»„ä»¶å±‚
# =====================================================

class ScaleGeneratorManager:
    """å¤šå°ºåº¦ç”Ÿæˆå™¨ç®¡ç†å™¨"""
    
    @staticmethod
    def create_scale_generator(config: Dict[str, Any]) -> Optional[ScaleAwareSyntheticGenerator]:
        """åˆ›å»ºå¤šå°ºåº¦ç”Ÿæˆå™¨"""
        multi_scale_cfg = config.get('multi_scale_generation', {})
        if not multi_scale_cfg.get('enabled', False):
            return None
            
        try:
            topo_cfg = multi_scale_cfg.get('topology_variation', {})
            loadgen_cfg = multi_scale_cfg.get('load_gen_variation', {})
            
            gen_cfg_kwargs = {
                'topology_variation_prob': topo_cfg.get('variation_prob', 0.2),
                'node_addition_prob': topo_cfg.get('node_addition_prob', 0.1),
                'node_removal_prob': topo_cfg.get('node_removal_prob', 0.05),
                'edge_addition_prob': topo_cfg.get('edge_addition_prob', 0.1),
                'edge_removal_prob': topo_cfg.get('edge_removal_prob', 0.05),
                'load_variation_range': tuple(loadgen_cfg.get('load_range', [0.7, 1.3])),
                'gen_variation_range': tuple(loadgen_cfg.get('gen_range', [0.8, 1.2])),
                'ensure_connectivity': topo_cfg.get('ensure_connectivity', True),
                'min_degree': topo_cfg.get('min_degree', 2),
                'max_degree': topo_cfg.get('max_degree', 6)
            }
            
            grid_gen_config = GridGenerationConfig(**gen_cfg_kwargs)
            data_manager = DataManager(config)
            
            scale_generator = ScaleAwareSyntheticGenerator(
                base_case_loader=data_manager.load_power_grid_data,
                config=grid_gen_config,
                seed=config['system']['seed']
            )
            
            logger.info("å¤šå°ºåº¦ç”Ÿæˆå™¨åˆ›å»ºå®Œæˆ")
            return scale_generator
            
        except Exception as e:
            logger.warning(f"å¤šå°ºåº¦ç”Ÿæˆå™¨åˆ›å»ºå¤±è´¥: {e}")
            return None


class AdaptiveDirectorManager:
    """è‡ªé€‚åº”å¯¼æ¼”ç®¡ç†å™¨"""
    
    @staticmethod
    def create_adaptive_director(config: Dict[str, Any], base_mode: str = 'fast') -> Optional[AdaptiveDirector]:
        """åˆ›å»ºè‡ªé€‚åº”å¯¼æ¼”"""
        # å…¼å®¹æ–°æ—§é…ç½®ç»“æ„ï¼šé¦–å…ˆæ£€æŸ¥åµŒå¥—é…ç½®ï¼Œç„¶åå›é€€åˆ°é¡¶å±‚
        adaptive_mode_cfg = config.get('adaptive_mode', {})
        is_enabled_in_mode = adaptive_mode_cfg.get('adaptive_curriculum', {}).get('enabled', False)
        is_enabled_at_top = config.get('adaptive_curriculum', {}).get('enabled', False)

        if not (is_enabled_in_mode or is_enabled_at_top):
            return None
            
        try:
            director = AdaptiveDirector(config, base_mode)
            logger.info(f"è‡ªé€‚åº”å¯¼æ¼”åˆ›å»ºå®Œæˆ (åŸºç¡€æ¨¡å¼: {base_mode})")
            return director
        except Exception as e:
            logger.warning(f"è‡ªé€‚åº”å¯¼æ¼”åˆ›å»ºå¤±è´¥: {e}")
            return None


class UnifiedDirectorManager:
    """ç»Ÿä¸€å¯¼æ¼”ç®¡ç†å™¨"""
    
    @staticmethod
    def create_unified_director(config: Dict[str, Any]) -> Optional[UnifiedDirector]:
        """åˆ›å»ºç»Ÿä¸€å¯¼æ¼”"""
        # å…¼å®¹æ–°æ—§é…ç½®ç»“æ„ï¼šé¦–å…ˆæ£€æŸ¥åµŒå¥—é…ç½®ï¼Œç„¶åå›é€€åˆ°é¡¶å±‚
        unified_mode_cfg = config.get('unified_mode', {})
        is_enabled_in_mode = unified_mode_cfg.get('unified_director', {}).get('enabled', False)
        is_enabled_at_top = config.get('unified_director', {}).get('enabled', False)

        if not (is_enabled_in_mode or is_enabled_at_top):
            return None
            
        try:
            director = UnifiedDirector(config)
            director.set_total_episodes(config['training']['num_episodes'])
            logger.info("ç»Ÿä¸€å¯¼æ¼”åˆ›å»ºå®Œæˆ")
            return director
        except Exception as e:
            logger.warning(f"ç»Ÿä¸€å¯¼æ¼”åˆ›å»ºå¤±è´¥: {e}")
            return None


# =====================================================
# è®­ç»ƒæ—¥å¿—è®°å½•å™¨
# =====================================================

class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—è®°å½•å™¨"""

    def __init__(self, config: Dict[str, Any], total_episodes: int):
        self.config = config
        self.start_time = time.time()
        self.total_episodes = total_episodes

        # åˆ›å»ºæ—¶é—´æˆ³ç›®å½•
        self.timestamp_dir = self._create_timestamp_directory()

        # è®­ç»ƒæŒ‡æ ‡
        self.episode_rewards = []
        self.episode_lengths = []
        self.actor_losses = []
        self.critic_losses = []
        self.entropies = []
        self.success_rates = []
        self.best_reward = -float('inf')

        # è®¾ç½®TensorBoard
        self.use_tensorboard = config.get('logging', {}).get('use_tensorboard', False)
        self.tensorboard_writer = self._setup_tensorboard() if self.use_tensorboard else None

        # è®¾ç½®è¿›åº¦æ¡
        self.progress_bar = self._setup_progress_bar()

        # åªåœ¨verboseæ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        verbose = config.get('system', {}).get('verbose', False)
        if verbose:
            logger.info(f"è®­ç»ƒæ—¥å¿—è®°å½•å™¨åˆå§‹åŒ–å®Œæˆ: {total_episodes} episodes")
            logger.info(f"è®­ç»ƒç›®å½•: {self.timestamp_dir}")
        else:
            print_status(f"è®­ç»ƒç›®å½•: {Path(self.timestamp_dir).name}")

    def _create_timestamp_directory(self) -> str:
        """åˆ›å»ºæ—¶é—´æˆ³ç›®å½•ç»“æ„"""
        from datetime import datetime

        # åˆ›å»ºæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
        timestamp_dir = Path("data") / timestamp

        # åˆ›å»ºå®Œæ•´çš„ç›®å½•ç»“æ„
        subdirs = ['logs', 'checkpoints', 'models', 'figures', 'cache', 'output']
        for subdir in subdirs:
            (timestamp_dir / subdir).mkdir(parents=True, exist_ok=True)

        return str(timestamp_dir)

    def _setup_tensorboard(self):
        """è®¾ç½®TensorBoard"""
        try:
            from torch.utils.tensorboard.writer import SummaryWriter
            log_dir = Path(self.timestamp_dir) / "logs"
            writer = SummaryWriter(str(log_dir))
            verbose = self.config.get('system', {}).get('verbose', False)
            if verbose:
                logger.info(f"TensorBoardå·²å¯ç”¨: {log_dir}")
            else:
                print_status("TensorBoardå·²å¯ç”¨")
            return writer
        except ImportError:
            if self.config.get('system', {}).get('verbose', False):
                logger.warning("TensorBoardä¸å¯ç”¨")
            return None
        except Exception as e:
            if self.config.get('system', {}).get('verbose', False):
                logger.warning(f"TensorBoardåˆå§‹åŒ–å¤±è´¥: {e}")
            return None
    
    def _setup_progress_bar(self):
        """è®¾ç½®è¿›åº¦æ¡"""
        try:
            from tqdm import tqdm
            return tqdm(total=self.total_episodes, desc="ğŸš€ è®­ç»ƒè¿›åº¦",
                       bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')
        except ImportError:
            return None
    
    def log_episode(self, episode: int, reward: float, length: int, info: Optional[Dict] = None):
        """è®°å½•å•ä¸ªå›åˆ"""
        self.episode_rewards.append(reward)
        self.episode_lengths.append(length)
        
        if reward > self.best_reward:
            self.best_reward = reward
        
        # æ›´æ–°è¿›åº¦æ¡
        if self.progress_bar:
            avg_reward = sum(self.episode_rewards[-10:]) / min(len(self.episode_rewards), 10)
            desc = f"ğŸš€ è®­ç»ƒè¿›åº¦ | å½“å‰: {reward:.3f} | æœ€ä½³: {self.best_reward:.3f} | å¹³å‡: {avg_reward:.3f}"
            self.progress_bar.set_description(desc)
            self.progress_bar.update(1)
        
        # TensorBoardè®°å½•
        if self.use_tensorboard and self.tensorboard_writer:
            self.tensorboard_writer.add_scalar('Core/Episode_Reward', reward, episode)
            self.tensorboard_writer.add_scalar('Core/Episode_Length', length, episode)
            
            if info:
                # è®°å½•æˆåŠŸç‡
                if 'success' in info:
                    self.tensorboard_writer.add_scalar('Core/Success_Rate', float(info['success']), episode)
                
                # è®°å½•å¥–åŠ±ç»„ä»¶
                reward_components = ['balance_reward', 'decoupling_reward', 'power_reward', 'quality_reward', 'final_reward']
                for component in reward_components:
                    if component in info and isinstance(info[component], (int, float)):
                        self.tensorboard_writer.add_scalar(f'Reward/{component.capitalize()}', info[component], episode)
                
                # è®°å½•è´¨é‡æŒ‡æ ‡
                quality_metrics = ['quality_score', 'plateau_confidence']
                for metric in quality_metrics:
                    if metric in info and isinstance(info[metric], (int, float)):
                        self.tensorboard_writer.add_scalar(f'Quality/{metric.capitalize()}', info[metric], episode)

                # è®°å½•è¿é€šæ€§å’Œåˆ†åŒºæŒ‡æ ‡
                if 'metrics' in info:
                    metrics = info['metrics']
                    connectivity_metrics = ['connectivity', 'avg_connectivity']
                    for metric in connectivity_metrics:
                        if metric in metrics and isinstance(metrics[metric], (int, float)):
                            self.tensorboard_writer.add_scalar(f'Connectivity/{metric.capitalize()}', metrics[metric], episode)

                    partition_metrics = ['load_balance', 'decoupling', 'power_balance']
                    for metric in partition_metrics:
                        if metric in metrics and isinstance(metrics[metric], (int, float)):
                            self.tensorboard_writer.add_scalar(f'Partition/{metric.capitalize()}', metrics[metric], episode)

                # è®°å½•è®­ç»ƒçŠ¶æ€
                if 'training_state' in info:
                    state = info['training_state']
                    if isinstance(state, dict):
                        for key, value in state.items():
                            if isinstance(value, (int, float)):
                                self.tensorboard_writer.add_scalar(f'Training_State/{key.capitalize()}', value, episode)
            
            # å®šæœŸåˆ·æ–°
            if episode % 10 == 0:
                self.tensorboard_writer.flush()
    
    def log_training_step(self, episode: int, actor_loss: Optional[float] = None,
                         critic_loss: Optional[float] = None, entropy: Optional[float] = None):
        """è®°å½•è®­ç»ƒæ­¥éª¤"""
        if actor_loss is not None:
            self.actor_losses.append(actor_loss)
            if self.use_tensorboard and self.tensorboard_writer:
                self.tensorboard_writer.add_scalar('Training/Actor_Loss', actor_loss, episode)
        
        if critic_loss is not None:
            self.critic_losses.append(critic_loss)
            if self.use_tensorboard and self.tensorboard_writer:
                self.tensorboard_writer.add_scalar('Training/Critic_Loss', critic_loss, episode)
        
        if entropy is not None:
            self.entropies.append(entropy)
            if self.use_tensorboard and self.tensorboard_writer:
                self.tensorboard_writer.add_scalar('Training/Policy_Entropy', entropy, episode)
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        if not self.episode_rewards:
            return {}
        
        return {
            'total_episodes': len(self.episode_rewards),
            'best_reward': self.best_reward,
            'mean_reward': np.mean(self.episode_rewards),
            'std_reward': np.std(self.episode_rewards),
            'mean_length': np.mean(self.episode_lengths),
            'training_time': time.time() - self.start_time,
            'success_rate': np.mean(self.success_rates) if self.success_rates else 0.0
        }
    
    def close(self):
        """å…³é—­æ—¥å¿—è®°å½•å™¨"""
        if self.progress_bar:
            self.progress_bar.close()
        if self.tensorboard_writer:
            self.tensorboard_writer.close()


# =====================================================
# æŠ½è±¡è®­ç»ƒå™¨åŸºç±»
# =====================================================

class BaseTrainer(ABC):
    """æŠ½è±¡è®­ç»ƒå™¨åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device = self._setup_device()
        self.data_manager = DataManager(config)
        self.logger = None
        
    def _setup_device(self) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        device_config = self.config['system'].get('device', 'auto')
        if device_config == 'auto':
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            device = torch.device(device_config)
        
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
        return device
    
    @abstractmethod
    def train(self) -> Dict[str, Any]:
        """è®­ç»ƒæ¥å£ - å­ç±»å¿…é¡»å®ç°"""
        pass
    
    def evaluate(self, agent: Any, env: Any, num_episodes: int = 10) -> Dict[str, Any]:
        """è¯„ä¼°æ™ºèƒ½ä½“"""
        logger.info(f"å¼€å§‹è¯„ä¼° {num_episodes} è½®...")
        
        eval_rewards = []
        success_count = 0
        
        for episode in range(num_episodes):
            state, _ = env.reset()
            episode_reward = 0
            episode_info = {}
            
            for step in range(200):  # æœ€å¤§æ­¥æ•°
                # EnhancedPPOAgentè¿”å›4ä¸ªå€¼ï¼Œæ™®é€šPPOAgentè¿”å›3ä¸ªå€¼
                result = agent.select_action(state, training=False)
                if len(result) == 4:
                    action, _, _, _ = result  # å¿½ç•¥log_prob, value, embeddings
                else:
                    action, _, _ = result  # å¿½ç•¥log_prob, value
                
                if action is None:
                    break
                
                state, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated
                episode_reward += reward
                
                if info:
                    episode_info.update(info)
                
                if done:
                    break
            
            eval_rewards.append(episode_reward)
            
            # è¯„ä¼°æˆåŠŸæ ‡å‡†
            is_success = self._evaluate_episode_success(episode_reward, episode_info)
            if is_success:
                success_count += 1
        
        return {
            'avg_reward': np.mean(eval_rewards),
            'success_rate': success_count / num_episodes,
            'rewards': eval_rewards
        }
    
    def _evaluate_episode_success(self, episode_reward: float, episode_info: Dict[str, Any]) -> bool:
        """è¯„ä¼°å•ä¸ªepisodeæ˜¯å¦æˆåŠŸ"""
        # æˆåŠŸæ ‡å‡†1: å¥–åŠ±è¶…è¿‡åˆç†é˜ˆå€¼
        if episode_reward > -1.0:
            return True
        
        # æˆåŠŸæ ‡å‡†2: åŸºäºè´¨é‡æŒ‡æ ‡
        if episode_info and 'metrics' in episode_info:
            metrics = episode_info['metrics']
            
            # æ£€æŸ¥è´Ÿè½½å¹³è¡¡
            cv = metrics.get('cv', metrics.get('load_cv', 1.0))
            if cv < 0.5:
                return True
            
            # æ£€æŸ¥è¿é€šæ€§
            connectivity = metrics.get('connectivity', 0.0)
            if connectivity > 0.9:
                return True
            
            # æ£€æŸ¥è€¦åˆæ¯”ç‡
            coupling_ratio = metrics.get('coupling_ratio', 1.0)
            if coupling_ratio < 0.3:
                return True
        
        # æˆåŠŸæ ‡å‡†3: åŸºäºè´¨é‡åˆ†æ•°
        if episode_info and 'reward_components' in episode_info:
            components = episode_info['reward_components']
            quality_score = components.get('quality_score', 0.0)
            if quality_score > 0.4:
                return True
        
        # ç›¸å¯¹æ”¹è¿›æ ‡å‡†
        if episode_reward > -2.5:
            return True
        
        return False
    
    def _save_model(self, agent: Any, final_stats: Dict[str, Any],
                   model_suffix: str = "") -> Optional[str]:
        """ä¿å­˜è®­ç»ƒæ¨¡å‹"""
        try:
            assert self.logger is not None
            models_dir = Path(self.logger.timestamp_dir) / "models"
            models_dir.mkdir(parents=True, exist_ok=True)

            model_filename = f"agent_{model_suffix}.pth"
            model_path = models_dir / model_filename
            
            # ä¿å­˜æ¨¡å‹
            checkpoint = {
                'actor_state_dict': agent.actor.state_dict(),
                'critic_state_dict': agent.critic.state_dict(),
                'actor_optimizer_state_dict': agent.actor_optimizer.state_dict(),
                'critic_optimizer_state_dict': agent.critic_optimizer.state_dict(),
                'training_stats': final_stats,
                'timestamp': Path(self.logger.timestamp_dir).name,
                'training_directory': self.logger.timestamp_dir,
                'config': self.config,
                'model_info': {
                    'node_embedding_dim': agent.node_embedding_dim,
                    'region_embedding_dim': agent.region_embedding_dim,
                    'num_partitions': agent.num_partitions,
                    'device': str(agent.device)
                }
            }
            
            torch.save(checkpoint, model_path)
            
            logger.info(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")
            return str(model_path)
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            return None


# =====================================================
# å››ä¸ªè§£è€¦çš„è®­ç»ƒå™¨å®ç°
# =====================================================

class BasicTrainer(BaseTrainer):
    """çº¯åŸºç¡€è®­ç»ƒå™¨ - æ— è¯¾ç¨‹å­¦ä¹ ã€æ— æ‹“æ‰‘å˜åŒ–"""
    
    def train(self) -> Dict[str, Any]:
        """æ‰§è¡ŒåŸºç¡€è®­ç»ƒ"""
        logger.info("ğŸ”¥ å¼€å§‹åŸºç¡€è®­ç»ƒæ¨¡å¼ - æ— è¯¾ç¨‹å­¦ä¹ ã€æ— æ‹“æ‰‘å˜åŒ–")
        
        try:
            # 1. è®­ç»ƒé…ç½®
            training_config = self.config['training']
            num_episodes = training_config['num_episodes']
            max_steps_per_episode = training_config['max_steps_per_episode']
            update_interval = training_config['update_interval']

            # 2. åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨ï¼ˆéœ€è¦å…ˆåˆ›å»ºæ—¶é—´æˆ³ç›®å½•ï¼‰
            self.logger = LoggerFactory.create_training_logger(self.config, num_episodes)

            # 3. æ•°æ®åŠ è½½å’Œå¤„ç†
            ref_case = self.config['data'].get('reference_case', 'ieee14')
            mpc = self.data_manager.load_power_grid_data(ref_case)
            hetero_data = self.data_manager.process_data(mpc, self.config, self.device, self.logger.timestamp_dir)

            # 4. åˆ›å»ºåŸºç¡€ç¯å¢ƒï¼ˆæ— åœºæ™¯ç”Ÿæˆï¼‰
            try:
                env = EnvironmentFactory.create_basic_environment(hetero_data, self.config)
            except Exception as e:
                logger.error(f"åˆ›å»ºåŸºç¡€ç¯å¢ƒå¤±è´¥: {e}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                raise

            # 5. åˆ›å»ºæ™ºèƒ½ä½“
            try:
                agent = AgentFactory.create_agent(env, self.config, self.device)
            except Exception as e:
                logger.error(f"åˆ›å»ºæ™ºèƒ½ä½“å¤±è´¥: {e}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                raise

            # 6. GNNé¢„è®­ç»ƒå·²åœ¨ç¯å¢ƒåˆ›å»ºæ—¶å®Œæˆ
            
            # 7. åŸºç¡€è®­ç»ƒå¾ªç¯
            logger.info(f"å¼€å§‹åŸºç¡€è®­ç»ƒ: {num_episodes} episodes")

            for episode in range(num_episodes):
                try:
                    state, _ = env.reset()
                    episode_reward = 0
                    episode_length = 0
                    episode_info = {}

                    # Episodeæ‰§è¡Œ
                    for step in range(max_steps_per_episode):
                        try:
                            # EnhancedPPOAgentè¿”å›4ä¸ªå€¼ï¼Œæ™®é€šPPOAgentè¿”å›3ä¸ªå€¼
                            result = agent.select_action(state, training=True)
                            if len(result) == 4:
                                action, log_prob, value, _ = result  # å¿½ç•¥embeddings
                            else:
                                action, log_prob, value = result

                            if action is None:
                                break

                            next_state, reward, terminated, truncated, info = env.step(action)
                            done = terminated or truncated

                            agent.store_experience(state, action, reward, log_prob, value, done)
                            episode_reward += reward
                            episode_length += 1
                            state = next_state

                            if info:
                                episode_info.update(info)

                            if done:
                                break
                        except Exception as e:
                            logger.error(f"Episode {episode}, Step {step} æ‰§è¡Œå¤±è´¥: {e}")
                            import traceback
                            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                            raise
                except Exception as e:
                    logger.error(f"Episode {episode} å¤±è´¥: {e}")
                    import traceback
                    logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                    raise
                
                # è®°å½•è®­ç»ƒæ—¥å¿—
                self.logger.log_episode(episode, episode_reward, episode_length, episode_info)
                
                # æ™ºèƒ½ä½“æ›´æ–°
                if episode % update_interval == 0 and episode > 0:
                    try:
                        training_stats = agent.update()
                        if training_stats:
                            self.logger.log_training_step(
                                episode,
                                training_stats.get('actor_loss'),
                                training_stats.get('critic_loss'),
                                training_stats.get('entropy')
                            )
                    except Exception as e:
                        logger.warning(f"Episode {episode} æ™ºèƒ½ä½“æ›´æ–°å¤±è´¥: {e}")
            
            # 8. è®­ç»ƒå®Œæˆ
            final_stats = self.logger.get_statistics()
            logger.info("åŸºç¡€è®­ç»ƒå®Œæˆ")
            
            # 9. æ¨¡å‹ä¿å­˜
            model_path = self._save_model(agent, final_stats, f"basic_{ref_case}")
            
            # 10. è¯„ä¼°
            eval_stats = self.evaluate(agent, env)
            
            # 11. æ¸…ç†
            self.logger.close()
            
            return {
                'success': True,
                'mode': 'basic',
                'config': self.config,
                'training_stats': final_stats,
                'eval_stats': eval_stats,
                'model_path': model_path,
                'best_reward': self.logger.best_reward
            }
            
        except Exception as e:
            logger.error(f"åŸºç¡€è®­ç»ƒå¤±è´¥: {e}")
            if self.logger:
                self.logger.close()
            return {'success': False, 'error': str(e), 'mode': 'basic'}


class TopologyTrainer(BaseTrainer):
    """æ‹“æ‰‘å˜åŒ–è®­ç»ƒå™¨ - åªæ‹“æ‰‘å˜åŒ–ã€æ— å‚æ•°è‡ªé€‚åº”"""
    
    def train(self) -> Dict[str, Any]:
        """æ‰§è¡Œæ‹“æ‰‘å˜åŒ–è®­ç»ƒ"""
        logger.info("ğŸŒ å¼€å§‹æ‹“æ‰‘å˜åŒ–è®­ç»ƒæ¨¡å¼ - åªå¤šå°ºåº¦ç”Ÿæˆå™¨")
        
        try:
            # 1. è®­ç»ƒé…ç½®
            training_config = self.config['training']
            num_episodes = training_config['num_episodes']
            max_steps_per_episode = training_config['max_steps_per_episode']
            update_interval = training_config['update_interval']

            # 2. åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨ (æå‰)
            self.logger = LoggerFactory.create_training_logger(self.config, num_episodes)
            
            # 3. æ•°æ®åŠ è½½
            ref_case = self.config['data'].get('reference_case', 'ieee14')
            mpc = self.data_manager.load_power_grid_data(ref_case)
            
            # 4. åˆ›å»ºå¤šå°ºåº¦ç”Ÿæˆå™¨
            scale_generator = ScaleGeneratorManager.create_scale_generator(self.config)

            # 5. GNNé¢„è®­ç»ƒ/åŠ è½½ (åœ¨åˆ›å»ºç¯å¢ƒå‰å®Œæˆ)
            sample_hetero_data = self.data_manager.process_data(mpc, self.config, self.device, self.logger.timestamp_dir)
            gat_encoder = AgentFactory.setup_gnn_pretraining(sample_hetero_data, self.config, scale_generator)
            
            # 6. åˆ›å»ºåœºæ™¯ç”Ÿæˆç¯å¢ƒ
            env, gym_env = EnvironmentFactory.create_scenario_environment(
                mpc, self.config, scale_generator, gat_encoder
            )
            
            # 7. åˆ›å»ºæ™ºèƒ½ä½“
            agent = AgentFactory.create_agent(env, self.config, self.device)
            
            # 8. æ‹“æ‰‘å˜åŒ–è®­ç»ƒå¾ªç¯
            logger.info(f"å¼€å§‹æ‹“æ‰‘å˜åŒ–è®­ç»ƒ: {num_episodes} episodes")
            
            for episode in range(num_episodes):
                # é‡ç½®ç¯å¢ƒï¼ˆä¼šç”Ÿæˆæ–°çš„æ‹“æ‰‘å˜åŒ–ï¼‰
                assert gym_env is not None, "Gymç¯å¢ƒæœªåˆå§‹åŒ–"
                obs_array, info = gym_env.reset()
                env = gym_env.internal_env  # è·å–æ–°çš„å†…éƒ¨ç¯å¢ƒ
                assert env is not None, "å†…éƒ¨ç¯å¢ƒåœ¨é‡ç½®åä¸ºNone"
                state, _ = env.reset()
                
                episode_reward = 0
                episode_length = 0
                episode_info = {}
                
                # Episodeæ‰§è¡Œ
                for step in range(max_steps_per_episode):
                    action, log_prob, value, _ = agent.select_action(state, training=True)
                    
                    if action is None:
                        break
                    
                    next_state, reward, terminated, truncated, info = env.step(action)
                    done = terminated or truncated
                    
                    agent.store_experience(state, action, reward, log_prob, value, done)
                    episode_reward += reward
                    episode_length += 1
                    state = next_state
                    
                    if info:
                        episode_info.update(info)
                    
                    if done:
                        break
                
                # è®°å½•è®­ç»ƒæ—¥å¿—
                self.logger.log_episode(episode, episode_reward, episode_length, episode_info)
                
                # æ™ºèƒ½ä½“æ›´æ–°
                if episode % update_interval == 0 and episode > 0:
                    try:
                        training_stats = agent.update()
                        if training_stats:
                            self.logger.log_training_step(
                                episode,
                                training_stats.get('actor_loss'),
                                training_stats.get('critic_loss'),
                                training_stats.get('entropy')
                            )
                    except Exception as e:
                        logger.warning(f"Episode {episode} æ™ºèƒ½ä½“æ›´æ–°å¤±è´¥: {e}")
            
            # 9. è®­ç»ƒå®Œæˆ
            final_stats = self.logger.get_statistics()
            logger.info("æ‹“æ‰‘å˜åŒ–è®­ç»ƒå®Œæˆ")
            
            # 10. æ¨¡å‹ä¿å­˜
            model_path = self._save_model(agent, final_stats, f"topology_{ref_case}")
            
            # 11. è¯„ä¼°
            eval_stats = self.evaluate(agent, env)
            
            # 12. æ¸…ç†
            self.logger.close()
            
            return {
                'success': True,
                'mode': 'topology',
                'config': self.config,
                'training_stats': final_stats,
                'eval_stats': eval_stats,
                'model_path': model_path,
                'best_reward': self.logger.best_reward,
                'scale_generator_enabled': True
            }
            
        except Exception as e:
            logger.error(f"æ‹“æ‰‘å˜åŒ–è®­ç»ƒå¤±è´¥: {e}")
            if self.logger:
                self.logger.close()
            return {'success': False, 'error': str(e), 'mode': 'topology'}


class AdaptiveTrainer(BaseTrainer):
    """è‡ªé€‚åº”è®­ç»ƒå™¨ - åªå‚æ•°è‡ªé€‚åº”ã€æ— æ‹“æ‰‘å˜åŒ–"""
    
    def train(self) -> Dict[str, Any]:
        """æ‰§è¡Œè‡ªé€‚åº”è®­ç»ƒ"""
        logger.info("ğŸ§  å¼€å§‹è‡ªé€‚åº”è®­ç»ƒæ¨¡å¼ - åªå‚æ•°è‡ªé€‚åº”")
        
        try:
            # 1. è®­ç»ƒé…ç½®
            training_config = self.config['training']
            num_episodes = training_config['num_episodes']
            max_steps_per_episode = training_config['max_steps_per_episode']
            update_interval = training_config['update_interval']
            
            # 2. åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨ (æå‰)
            self.logger = LoggerFactory.create_training_logger(self.config, num_episodes)

            # 3. æ•°æ®åŠ è½½
            ref_case = self.config['data'].get('reference_case', 'ieee14')
            mpc = self.data_manager.load_power_grid_data(ref_case)
            
            # 4. åˆ›å»ºè‡ªé€‚åº”å¯¼æ¼”
            base_mode = self._detect_base_mode()
            adaptive_director = AdaptiveDirectorManager.create_adaptive_director(self.config, base_mode)
            
            if adaptive_director is None:
                logger.warning("è‡ªé€‚åº”å¯¼æ¼”æœªå¯ç”¨ï¼Œå›é€€åˆ°åŸºç¡€è®­ç»ƒ")
                basic_trainer = BasicTrainer(self.config)
                return basic_trainer.train()
            
            # 5. GNNé¢„è®­ç»ƒ/åŠ è½½
            sample_hetero_data = self.data_manager.process_data(mpc, self.config, self.device, self.logger.timestamp_dir)
            training_source = [sample_hetero_data]
            gat_encoder = AgentFactory.setup_gnn_pretraining(sample_hetero_data, self.config, training_source)
            
            # 6. åˆ›å»ºåœºæ™¯ç¯å¢ƒ (æ— æ‹“æ‰‘å˜åŒ–)
            env, gym_env = EnvironmentFactory.create_scenario_environment(mpc, self.config, None, gat_encoder)
            
            # 7. åˆ›å»ºæ™ºèƒ½ä½“
            agent = AgentFactory.create_agent(env, self.config, self.device)
            
            # 8. è‡ªé€‚åº”è®­ç»ƒå¾ªç¯
            logger.info(f"å¼€å§‹è‡ªé€‚åº”è®­ç»ƒ: {num_episodes} episodes")
            director_decisions = []
            
            for episode in range(num_episodes):
                # é‡ç½®ç¯å¢ƒ
                assert gym_env is not None, "Gymç¯å¢ƒæœªåˆå§‹åŒ–"
                if gym_env is not None:
                    obs_array, info = gym_env.reset()
                    env = gym_env.internal_env
                    assert env is not None, "å†…éƒ¨ç¯å¢ƒåœ¨é‡ç½®åä¸ºNone"
                    state, _ = env.reset()
                else:
                    # è¿™ä¸ªåˆ†æ”¯ç†è®ºä¸Šä¸åº”è¯¥æ‰§è¡Œï¼Œå› ä¸ºadaptive_directoræ£€æŸ¥ä¼šæå‰è¿”å›
                    state, _ = env.reset()
                
                episode_reward = 0
                episode_length = 0
                episode_info = {}
                
                # Episodeæ‰§è¡Œ
                for step in range(max_steps_per_episode):
                    action, log_prob, value, _ = agent.select_action(state, training=True)
                    
                    if action is None:
                        break
                    
                    next_state, reward, terminated, truncated, info = env.step(action)
                    done = terminated or truncated
                    
                    agent.store_experience(state, action, reward, log_prob, value, done)
                    episode_reward += reward
                    episode_length += 1
                    state = next_state
                    
                    if info:
                        episode_info.update(info)
                    
                    if done:
                        break
                
                # æ”¶é›†episodeä¿¡æ¯ä¾›è‡ªé€‚åº”å¯¼æ¼”å†³ç­–
                episode_summary = {
                    'episode': episode,
                    'reward': episode_reward,
                    'episode_length': episode_length,
                    'success': episode_info.get('success', False),
                    'cv': episode_info.get('cv', episode_info.get('load_cv', 1.0)),
                    'coupling_ratio': episode_info.get('coupling_ratio', 1.0),
                    'connectivity': episode_info.get('connectivity', 0.0),
                    **episode_info
                }
                
                # è‡ªé€‚åº”å¯¼æ¼”å†³ç­–
                director_decision = adaptive_director.step(episode, episode_summary)
                director_decisions.append(director_decision)
                
                # åº”ç”¨è‡ªé€‚åº”å¯¼æ¼”çš„å‚æ•°è°ƒæ•´
                self._apply_adaptive_decision(env, agent, director_decision)
                
                # è®°å½•è®­ç»ƒæ—¥å¿—
                self.logger.log_episode(episode, episode_reward, episode_length, episode_info)
                
                # æ™ºèƒ½ä½“æ›´æ–°
                if episode % update_interval == 0 and episode > 0:
                    try:
                        training_stats = agent.update()
                        if training_stats:
                            self.logger.log_training_step(
                                episode,
                                training_stats.get('actor_loss'),
                                training_stats.get('critic_loss'),
                                training_stats.get('entropy')
                            )
                    except Exception as e:
                        logger.warning(f"Episode {episode} æ™ºèƒ½ä½“æ›´æ–°å¤±è´¥: {e}")
            
            # 9. è®­ç»ƒå®Œæˆ
            final_stats = self.logger.get_statistics()
            director_summary = adaptive_director.get_status_summary()
            logger.info("è‡ªé€‚åº”è®­ç»ƒå®Œæˆ")
            
            # 10. æ¨¡å‹ä¿å­˜
            model_path = self._save_model(agent, final_stats, f"adaptive_{ref_case}")
            
            # 11. è¯„ä¼°
            eval_stats = self.evaluate(agent, env)
            
            # 12. æ¸…ç†
            self.logger.close()
            
            return {
                'success': True,
                'mode': 'adaptive',
                'config': self.config,
                'training_stats': final_stats,
                'eval_stats': eval_stats,
                'model_path': model_path,
                'best_reward': self.logger.best_reward,
                'director_decisions': director_decisions,
                'director_summary': director_summary,
                'adaptive_director_enabled': True
            }
            
        except Exception as e:
            logger.error(f"è‡ªé€‚åº”è®­ç»ƒå¤±è´¥: {e}")
            if self.logger:
                self.logger.close()
            return {'success': False, 'error': str(e), 'mode': 'adaptive'}
    
    def _detect_base_mode(self) -> str:
        """æ£€æµ‹åŸºç¡€è®­ç»ƒæ¨¡å¼"""
        num_episodes = self.config.get('training', {}).get('num_episodes', 1000)
        if num_episodes >= 3000:
            return 'ieee57'
        elif num_episodes >= 2000:
            return 'full'
        else:
            return 'fast'
    
    def _apply_adaptive_decision(self, env: Any, agent: Any, decision: Dict[str, Any]):
        """åº”ç”¨è‡ªé€‚åº”å¯¼æ¼”çš„å†³ç­–"""
        try:
            # æ›´æ–°ç¯å¢ƒå¥–åŠ±å‚æ•°
            if hasattr(env, 'reward_function') and 'reward_weights' in decision:
                reward_weights = decision['reward_weights']
                if hasattr(env.reward_function, 'update_weights'):
                    env.reward_function.update_weights(reward_weights)
            
            # æ›´æ–°è¿é€šæ€§æƒ©ç½š
            if 'connectivity_penalty' in decision:
                if hasattr(env, 'connectivity_penalty'):
                    env.connectivity_penalty = decision['connectivity_penalty']
            
            # æ›´æ–°æ™ºèƒ½ä½“å­¦ä¹ ç‡
            if 'learning_rate_factor' in decision and decision['learning_rate_factor'] != 1.0:
                if hasattr(agent, 'update_learning_rate'):
                    agent.update_learning_rate(decision['learning_rate_factor'])
                    
        except Exception as e:
            logger.warning(f"åº”ç”¨è‡ªé€‚åº”å†³ç­–å¤±è´¥: {e}")


class UnifiedTrainer(BaseTrainer):
    """ç»Ÿä¸€å¯¼æ¼”è®­ç»ƒå™¨ - æ‹“æ‰‘å˜åŒ– + å‚æ•°åè°ƒ"""
    
    def train(self) -> Dict[str, Any]:
        """æ‰§è¡Œç»Ÿä¸€å¯¼æ¼”è®­ç»ƒ"""
        logger.info("ğŸ­ å¼€å§‹ç»Ÿä¸€å¯¼æ¼”è®­ç»ƒæ¨¡å¼ - æ‹“æ‰‘å˜åŒ– + å‚æ•°åè°ƒ")
        
        try:
            # 1. è®­ç»ƒé…ç½®
            training_config = self.config['training']
            num_episodes = training_config['num_episodes']
            max_steps_per_episode = training_config['max_steps_per_episode']
            update_interval = training_config['update_interval']

            # 2. åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨ (æå‰)
            self.logger = LoggerFactory.create_training_logger(self.config, num_episodes)

            # 3. æ•°æ®åŠ è½½
            ref_case = self.config['data'].get('reference_case', 'ieee14')
            mpc = self.data_manager.load_power_grid_data(ref_case)
            
            # 4. åˆ›å»ºç»Ÿä¸€å¯¼æ¼”
            unified_director = UnifiedDirectorManager.create_unified_director(self.config)
            
            if unified_director is None:
                logger.warning("ç»Ÿä¸€å¯¼æ¼”æœªå¯ç”¨ï¼Œå›é€€åˆ°è‡ªé€‚åº”è®­ç»ƒ")
                adaptive_trainer = AdaptiveTrainer(self.config)
                return adaptive_trainer.train()
            
            # 5. åˆ›å»ºå¤šå°ºåº¦ç”Ÿæˆå™¨
            scale_generator = ScaleGeneratorManager.create_scale_generator(self.config)
            
            # 6. åˆ›å»ºè‡ªé€‚åº”å¯¼æ¼”ä½œä¸ºç»„ä»¶
            base_mode = self._detect_base_mode()
            adaptive_director = AdaptiveDirectorManager.create_adaptive_director(self.config, base_mode)
            
            # 7. é›†æˆç»„ä»¶åˆ°ç»Ÿä¸€å¯¼æ¼”
            unified_director.integrate_components(
                adaptive_director=adaptive_director,
                scale_generator=scale_generator
            )
            
            # 8. GNNé¢„è®­ç»ƒ/åŠ è½½
            sample_hetero_data = self.data_manager.process_data(mpc, self.config, self.device, self.logger.timestamp_dir)
            training_source = scale_generator if scale_generator else None
            gat_encoder = AgentFactory.setup_gnn_pretraining(sample_hetero_data, self.config, training_source)
            
            # 9. åˆ›å»ºåœºæ™¯ç”Ÿæˆç¯å¢ƒ
            env, gym_env = EnvironmentFactory.create_scenario_environment(
                mpc, self.config, scale_generator, gat_encoder
            )
            
            # 10. åˆ›å»ºæ™ºèƒ½ä½“
            agent = AgentFactory.create_agent(env, self.config, self.device)

            # ç¡®ä¿æ™ºèƒ½ä½“ä½¿ç”¨é¢„è®­ç»ƒåçš„ç¼–ç å™¨ï¼ˆå¦‚æœæ™ºèƒ½ä½“éœ€è¦çš„è¯ï¼‰
            if hasattr(agent, 'gat_encoder'):
                agent.gat_encoder = gat_encoder
            
            # 11. è®­ç»ƒå¾ªç¯
            logger.info(f"å¼€å§‹ç»Ÿä¸€å¯¼æ¼”è®­ç»ƒ: {num_episodes} episodes")
            unified_decisions = []
            
            for episode in range(num_episodes):
                # é‡ç½®ç¯å¢ƒ
                assert gym_env is not None, "Gymç¯å¢ƒæœªåˆå§‹åŒ–"
                if gym_env is not None:
                    obs_array, info = gym_env.reset()
                    env = gym_env.internal_env
                    assert env is not None, "å†…éƒ¨ç¯å¢ƒåœ¨é‡ç½®åä¸ºNone"
                    state, _ = env.reset()
                else:
                    state, _ = env.reset()
                
                episode_reward = 0
                episode_length = 0
                episode_info = {}
                
                # Episodeæ‰§è¡Œ
                for step in range(max_steps_per_episode):
                    action, log_prob, value, _ = agent.select_action(state, training=True)
                    
                    if action is None:
                        break
                    
                    next_state, reward, terminated, truncated, info = env.step(action)
                    done = terminated or truncated
                    
                    agent.store_experience(state, action, reward, log_prob, value, done)
                    episode_reward += reward
                    episode_length += 1
                    state = next_state
                    
                    if info:
                        episode_info.update(info)
                    
                    if done:
                        break
                
                # æ”¶é›†episodeä¿¡æ¯ä¾›ç»Ÿä¸€å¯¼æ¼”å†³ç­–
                episode_summary = {
                    'episode': episode,
                    'reward': episode_reward,
                    'episode_length': episode_length,
                    'success': episode_info.get('success', False),
                    'cv': episode_info.get('cv', episode_info.get('load_cv', 1.0)),
                    'coupling_ratio': episode_info.get('coupling_ratio', 1.0),
                    'connectivity': episode_info.get('connectivity', 0.0),
                    'quality_score': episode_info.get('quality_score', 0.0),
                    **episode_info
                }
                
                # ç»Ÿä¸€å¯¼æ¼”å†³ç­–
                unified_decision = unified_director.step(episode, episode_summary)
                unified_decisions.append(unified_decision)
                
                # åº”ç”¨ç»Ÿä¸€å¯¼æ¼”çš„å†³ç­–
                self._apply_unified_decision(env, agent, unified_decision)
                
                # è®°å½•è®­ç»ƒæ—¥å¿—
                self.logger.log_episode(episode, episode_reward, episode_length, episode_info)
                
                # æ™ºèƒ½ä½“æ›´æ–°
                if episode % update_interval == 0 and episode > 0:
                    try:
                        training_stats = agent.update(env=env)
                        if training_stats:
                            self.logger.log_training_step(
                                episode,
                                training_stats.get('actor_loss'),
                                training_stats.get('critic_loss'),
                                training_stats.get('entropy')
                            )
                    except Exception as e:
                        logger.warning(f"Episode {episode} æ™ºèƒ½ä½“æ›´æ–°å¤±è´¥: {e}")
            
            # 12. è®­ç»ƒå®Œæˆ
            final_stats = self.logger.get_statistics()
            director_summary = unified_director.get_status_summary()
            logger.info("ç»Ÿä¸€å¯¼æ¼”è®­ç»ƒå®Œæˆ")
            
            # 13. æ¨¡å‹ä¿å­˜
            model_path = self._save_model(agent, final_stats, f"unified_{ref_case}")
            
            # 14. è¯„ä¼°
            eval_stats = self.evaluate(agent, env)
            
            # 15. æ¸…ç†
            self.logger.close()
            
            return {
                'success': True,
                'mode': 'unified',
                'config': self.config,
                'training_stats': final_stats,
                'eval_stats': eval_stats,
                'model_path': model_path,
                'best_reward': self.logger.best_reward,
                'unified_decisions': unified_decisions,
                'director_summary': director_summary,
                'unified_director_enabled': True,
                'scale_generator_enabled': scale_generator is not None,
                'adaptive_director_enabled': adaptive_director is not None
            }
            
        except Exception as e:
            import traceback
            logger.error(f"ç»Ÿä¸€å¯¼æ¼”è®­ç»ƒå¤±è´¥: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            if self.logger:
                self.logger.close()
            return {'success': False, 'error': str(e), 'mode': 'unified'}
    
    def _detect_base_mode(self) -> str:
        """æ£€æµ‹åŸºç¡€è®­ç»ƒæ¨¡å¼"""
        num_episodes = self.config.get('training', {}).get('num_episodes', 1000)
        if num_episodes >= 3000:
            return 'ieee57'
        elif num_episodes >= 2000:
            return 'full'
        else:
            return 'fast'
    
    def _apply_unified_decision(self, env: Any, agent: Any, unified_decision: Dict[str, Any]):
        """åº”ç”¨ç»Ÿä¸€å¯¼æ¼”çš„å†³ç­–"""
        try:
            # è·å–å‚æ•°è°ƒæ•´
            parameter_adjustments = unified_decision.get('parameter_adjustments', {})
            warmup_params = parameter_adjustments.get('warmup_params', {})
            
            # åº”ç”¨é¢„çƒ­å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
            if warmup_params:
                # æ›´æ–°ç¯å¢ƒå‚æ•°
                if hasattr(env, 'reward_function') and 'reward_weights' in warmup_params:
                    reward_weights = warmup_params['reward_weights']
                    if hasattr(env.reward_function, 'update_weights'):
                        env.reward_function.update_weights(reward_weights)
                
                if 'connectivity_penalty' in warmup_params:
                    if hasattr(env, 'connectivity_penalty'):
                        env.connectivity_penalty = warmup_params['connectivity_penalty']
                
                # æ›´æ–°æ™ºèƒ½ä½“å‚æ•°
                if 'learning_rate_factor' in warmup_params:
                    if hasattr(agent, 'update_learning_rate'):
                        agent.update_learning_rate(warmup_params['learning_rate_factor'])
            
            # åº”ç”¨è‡ªé€‚åº”å¯¼æ¼”çš„å†³ç­–ï¼ˆå¦‚æœæœ‰ä¸”ä¸å†²çªï¼‰
            adaptive_decision = unified_decision.get('adaptive_decision', {})
            if adaptive_decision and not adaptive_decision.get('unified_override', False):
                self._apply_adaptive_decision(env, agent, adaptive_decision)
                
        except Exception as e:
            logger.warning(f"åº”ç”¨ç»Ÿä¸€å¯¼æ¼”å†³ç­–å¤±è´¥: {e}")
    
    def _apply_adaptive_decision(self, env: Any, agent: Any, decision: Dict[str, Any]):
        """åº”ç”¨è‡ªé€‚åº”å¯¼æ¼”å†³ç­–ï¼ˆè¾…åŠ©æ–¹æ³•ï¼‰"""
        try:
            # æ›´æ–°ç¯å¢ƒå¥–åŠ±å‚æ•°
            if hasattr(env, 'reward_function') and 'reward_weights' in decision:
                reward_weights = decision['reward_weights']
                if hasattr(env.reward_function, 'update_weights'):
                    env.reward_function.update_weights(reward_weights)
            
            # æ›´æ–°è¿é€šæ€§æƒ©ç½š
            if 'connectivity_penalty' in decision:
                if hasattr(env, 'connectivity_penalty'):
                    env.connectivity_penalty = decision['connectivity_penalty']
            
            # æ›´æ–°æ™ºèƒ½ä½“å­¦ä¹ ç‡
            if 'learning_rate_factor' in decision and decision['learning_rate_factor'] != 1.0:
                if hasattr(agent, 'update_learning_rate'):
                    agent.update_learning_rate(decision['learning_rate_factor'])
                    
        except Exception as e:
            logger.warning(f"åº”ç”¨è‡ªé€‚åº”å†³ç­–å¤±è´¥: {e}")


# =====================================================
# è®­ç»ƒç®¡ç†å™¨
# =====================================================

class TrainingManager:
    """è®­ç»ƒç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†å››ç§è®­ç»ƒæ¨¡å¼"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.mode_mapping = {
            'basic': BasicTrainer,
            'topology': TopologyTrainer, 
            'adaptive': AdaptiveTrainer,
            'unified': UnifiedTrainer
        }
        
    def select_training_mode(self, mode: Optional[str] = None) -> str:
        """é€‰æ‹©è®­ç»ƒæ¨¡å¼
        
        Args:
            mode: æŒ‡å®šçš„è®­ç»ƒæ¨¡å¼ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
            
        Returns:
            é€‰æ‹©çš„è®­ç»ƒæ¨¡å¼åç§°
        """
        if mode and mode in self.mode_mapping:
            return mode
            
        # è‡ªåŠ¨æ¨¡å¼é€‰æ‹©é€»è¾‘ - æ£€æŸ¥æ–°çš„é…ç½®ç»“æ„
        config = self.config
        
        # æ£€æŸ¥å„æ¨¡å¼æ˜¯å¦å¯ç”¨
        if config.get('unified_mode', {}).get('enabled', False):
            logger.info("æ£€æµ‹åˆ°unified_modeå¯ç”¨ï¼Œé€‰æ‹©unifiedæ¨¡å¼")
            return 'unified'
        
        if config.get('adaptive_mode', {}).get('enabled', False):
            logger.info("æ£€æµ‹åˆ°adaptive_modeå¯ç”¨ï¼Œé€‰æ‹©adaptiveæ¨¡å¼")
            return 'adaptive'
        
        if config.get('topology_mode', {}).get('enabled', False):
            logger.info("æ£€æµ‹åˆ°topology_modeå¯ç”¨ï¼Œé€‰æ‹©topologyæ¨¡å¼")
            return 'topology'
        
        if config.get('basic_mode', {}).get('enabled', False):
            logger.info("æ£€æµ‹åˆ°basic_modeå¯ç”¨ï¼Œé€‰æ‹©basicæ¨¡å¼")
            return 'basic'
        
        # å›é€€é€»è¾‘ - æ£€æŸ¥æ—§çš„é…ç½®é”®ï¼ˆå‘åå…¼å®¹ï¼‰
        if config.get('unified_director', {}).get('enabled', False):
            logger.info("æ£€æµ‹åˆ°unified_directorå¯ç”¨ï¼Œé€‰æ‹©unifiedæ¨¡å¼")
            return 'unified'
        
        if config.get('adaptive_curriculum', {}).get('enabled', False):
            logger.info("æ£€æµ‹åˆ°adaptive_curriculumå¯ç”¨ï¼Œé€‰æ‹©adaptiveæ¨¡å¼")
            return 'adaptive'
        
        if config.get('multi_scale_generation', {}).get('enabled', False):
            logger.info("æ£€æµ‹åˆ°multi_scale_generationå¯ç”¨ï¼Œé€‰æ‹©topologyæ¨¡å¼")
            return 'topology'
        
        # é»˜è®¤åŸºç¡€æ¨¡å¼
        logger.info("æœªæ£€æµ‹åˆ°è¯¾ç¨‹å­¦ä¹ ç»„ä»¶ï¼Œé€‰æ‹©basicæ¨¡å¼")
        return 'basic'
    
    def create_trainer(self, mode: str) -> BaseTrainer:
        """åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹
        
        Args:
            mode: è®­ç»ƒæ¨¡å¼
            
        Returns:
            è®­ç»ƒå™¨å®ä¾‹
        """
        if mode not in self.mode_mapping:
            raise ValueError(f"ä¸æ”¯æŒçš„è®­ç»ƒæ¨¡å¼: {mode}. æ”¯æŒçš„æ¨¡å¼: {list(self.mode_mapping.keys())}")
        
        trainer_class = self.mode_mapping[mode]
        return trainer_class(self.config)
    
    def validate_config(self, mode: str) -> Tuple[bool, List[str]]:
        """éªŒè¯é…ç½®çš„å®Œæ•´æ€§
        
        Args:
            mode: è®­ç»ƒæ¨¡å¼
            
        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯åˆ—è¡¨)
        """
        errors = []
        
        # æ£€æŸ¥åŸºç¡€é…ç½®
        required_sections = ['training', 'environment', 'agent', 'gat']
        for section in required_sections:
            if section not in self.config:
                errors.append(f"ç¼ºå°‘å¿…éœ€çš„é…ç½®èŠ‚: {section}")
        
        # æ¨¡å¼ç‰¹å®šéªŒè¯ - æ£€æŸ¥æ–°çš„é…ç½®ç»“æ„
        if mode == 'topology':
            topology_config = self.config.get('topology_mode', {})
            if topology_config.get('multi_scale_generation', {}).get('enabled', False):
                # topology_modeä¸‹æœ‰é…ç½®
                pass
            elif self.config.get('multi_scale_generation', {}).get('enabled', False):
                # å…¨å±€é…ç½®ä¸­æœ‰
                pass
            else:
                errors.append("topologyæ¨¡å¼éœ€è¦å¯ç”¨multi_scale_generationï¼ˆåœ¨topology_modeæˆ–å…¨å±€é…ç½®ä¸­ï¼‰")
        
        elif mode == 'adaptive':
            adaptive_config = self.config.get('adaptive_mode', {})
            if adaptive_config.get('adaptive_curriculum', {}).get('enabled', False):
                # adaptive_modeä¸‹æœ‰é…ç½®
                pass
            elif self.config.get('adaptive_curriculum', {}).get('enabled', False):
                # å…¨å±€é…ç½®ä¸­æœ‰
                pass
            else:
                errors.append("adaptiveæ¨¡å¼éœ€è¦å¯ç”¨adaptive_curriculumï¼ˆåœ¨adaptive_modeæˆ–å…¨å±€é…ç½®ä¸­ï¼‰")
        
        elif mode == 'unified':
            unified_config = self.config.get('unified_mode', {})
            if unified_config.get('unified_director', {}).get('enabled', False):
                # unified_modeä¸‹æœ‰é…ç½®
                pass
            elif self.config.get('unified_director', {}).get('enabled', False):
                # å…¨å±€é…ç½®ä¸­æœ‰
                pass
            else:
                errors.append("unifiedæ¨¡å¼éœ€è¦å¯ç”¨unified_directorï¼ˆåœ¨unified_modeæˆ–å…¨å±€é…ç½®ä¸­ï¼‰")
        
        # æ£€æŸ¥ä¾èµ–åº“
        if not DEPENDENCIES_OK:
            errors.append(f"ä¾èµ–åº“å¯¼å…¥å¤±è´¥: {IMPORT_ERROR}")
        
        return len(errors) == 0, errors
    
    def run_training(self, mode: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """æ‰§è¡Œè®­ç»ƒ
        
        Args:
            mode: è®­ç»ƒæ¨¡å¼
            **kwargs: é¢å¤–çš„è®­ç»ƒå‚æ•°
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        # é€‰æ‹©è®­ç»ƒæ¨¡å¼
        selected_mode = self.select_training_mode(mode)
        logger.info(f"é€‰æ‹©è®­ç»ƒæ¨¡å¼: {selected_mode}")
        
        # éªŒè¯é…ç½®
        is_valid, errors = self.validate_config(selected_mode)
        if not is_valid:
            error_msg = f"é…ç½®éªŒè¯å¤±è´¥: {'; '.join(errors)}"
            logger.error(error_msg)
            return {
                'success': False,
                'error': error_msg,
                'mode': selected_mode,
                'config_errors': errors
            }
        
        # åˆ›å»ºè®­ç»ƒå™¨
        try:
            trainer = self.create_trainer(selected_mode)
            logger.info(f"åˆ›å»º{selected_mode}è®­ç»ƒå™¨æˆåŠŸ")
        except Exception as e:
            error_msg = f"åˆ›å»ºè®­ç»ƒå™¨å¤±è´¥: {e}"
            logger.error(error_msg)
            return {
                'success': False,
                'error': error_msg,
                'mode': selected_mode
            }
        
        # æ‰§è¡Œè®­ç»ƒ
        try:
            start_time = time.time()
            logger.info(f"å¼€å§‹{selected_mode}æ¨¡å¼è®­ç»ƒ")
            
            result = trainer.train()
            
            # æ·»åŠ è¿è¡Œæ—¶ä¿¡æ¯
            result.update({
                'selected_mode': selected_mode,
                'training_time': time.time() - start_time,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            })
            
            if result.get('success', False):
                logger.info(f"{selected_mode}æ¨¡å¼è®­ç»ƒå®Œæˆ")
            else:
                logger.error(f"{selected_mode}æ¨¡å¼è®­ç»ƒå¤±è´¥: {result.get('error', 'Unknown error')}")
            
            return result
            
        except Exception as e:
            error_msg = f"{selected_mode}æ¨¡å¼è®­ç»ƒå¼‚å¸¸: {e}"
            logger.error(error_msg)
            return {
                'success': False,
                'error': error_msg,
                'mode': selected_mode,
                'exception': str(e)
            }


# =====================================================
# å‘½ä»¤è¡Œæ¥å£å’Œä¸»å…¥å£
# =====================================================

def setup_argument_parser() -> argparse.ArgumentParser:
    """è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description='ç”µåŠ›ç½‘ç»œåˆ†åŒºå¼ºåŒ–å­¦ä¹ è®­ç»ƒç³»ç»Ÿ v2.0 - å››æ¨¡å¼è§£è€¦æ¶æ„',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # æ ¸å¿ƒå‚æ•°
    parser.add_argument('--config', type=str, default='config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--mode', type=str, choices=['basic', 'topology', 'adaptive', 'unified', 'auto'],
                       default='auto', help='è®­ç»ƒæ¨¡å¼é€‰æ‹©')
    parser.add_argument('--run', action='store_true',
                       help='ç«‹å³å¼€å§‹è®­ç»ƒ')
    
    # è°ƒè¯•å‚æ•°
    parser.add_argument('--debug', action='store_true',
                       help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--verbose', action='store_true',
                       help='è¯¦ç»†æ—¥å¿—è¾“å‡º')
    parser.add_argument('--dry-run', action='store_true',
                       help='æ¨¡æ‹Ÿè¿è¡Œï¼ŒéªŒè¯é…ç½®')
    
    # è¾“å‡ºæ§åˆ¶
    parser.add_argument('--output-dir', type=str,
                       help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--no-tensorboard', action='store_true',
                       help='ç¦ç”¨TensorBoardæ—¥å¿—')
    
    return parser


def load_and_merge_config(config_path: str, args: argparse.Namespace) -> Dict[str, Any]:
    """åŠ è½½å¹¶åˆå¹¶é…ç½®
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        args: å‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        åˆå¹¶åçš„é…ç½®å­—å…¸
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    except FileNotFoundError:
        logger.error(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
        raise
    except yaml.YAMLError as e:
        logger.error(f"é…ç½®æ–‡ä»¶è§£æå¤±è´¥: {e}")
        raise
    
    # åº”ç”¨æ¨¡å¼é…ç½®
    if args.mode and args.mode != 'auto':
        mode_key = f"{args.mode}_mode"
        if mode_key in config:
            config[mode_key]['enabled'] = True
            logger.info(f"å¯ç”¨ {args.mode} æ¨¡å¼")
            
            # ç¦ç”¨å…¶ä»–æ¨¡å¼
            other_modes = ['basic', 'topology', 'adaptive', 'unified']
            for mode in other_modes:
                if mode != args.mode:
                    other_key = f"{mode}_mode"
                    if other_key in config:
                        config[other_key]['enabled'] = False
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.debug:
        config['debug']['enabled'] = True
        config['debug']['verbose_logging'] = True
        if args.verbose:
            logger.info("å·²å¯ç”¨è°ƒè¯•æ¨¡å¼")

    # ä¼ é€’verboseå‚æ•°åˆ°é…ç½®
    if 'system' not in config:
        config['system'] = {}
    config['system']['verbose'] = args.verbose or args.debug
    
    if args.verbose:
        config['debug']['verbose_logging'] = True
        config['logging']['console_log_interval'] = 1
    
    if args.no_tensorboard:
        config['logging']['use_tensorboard'] = False
        logger.info("å·²ç¦ç”¨TensorBoard")
    
    if args.output_dir:
        config['logging']['log_dir'] = args.output_dir + '/logs'
        config['logging']['checkpoint_dir'] = args.output_dir + '/checkpoints'
        config['visualization']['figures_dir'] = args.output_dir + '/figures'
        logger.info(f"è¾“å‡ºç›®å½•è®¾ç½®ä¸º: {args.output_dir}")
    
    return config


def merge_dict_recursive(base_dict: Dict, override_dict: Dict):
    """é€’å½’åˆå¹¶å­—å…¸"""
    for key, value in override_dict.items():
        if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
            merge_dict_recursive(base_dict[key], value)
        else:
            base_dict[key] = value


def print_system_info(verbose: bool = False):
    """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
    if verbose:
        logger.info("=" * 60)
        logger.info("ç”µåŠ›ç½‘ç»œåˆ†åŒºå¼ºåŒ–å­¦ä¹ è®­ç»ƒç³»ç»Ÿ v2.0")
        logger.info("=" * 60)
        logger.info(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        logger.info(f"è®¾å¤‡: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
        if torch.cuda.is_available():
            logger.info(f"GPUæ•°é‡: {torch.cuda.device_count()}")
            logger.info(f"å½“å‰GPU: {torch.cuda.get_device_name()}")
        logger.info(f"ä¾èµ–åº“çŠ¶æ€: {'âœ“ æ­£å¸¸' if DEPENDENCIES_OK else 'âœ— é”™è¯¯'}")
        logger.info("=" * 60)
    else:
        print_header("ç”µåŠ›ç½‘ç»œåˆ†åŒºå¼ºåŒ–å­¦ä¹ è®­ç»ƒç³»ç»Ÿ v2.0")
        device_info = f"CUDA ({torch.cuda.get_device_name()})" if torch.cuda.is_available() else "CPU"
        print_status(f"è®¾å¤‡: {device_info}")
        if not DEPENDENCIES_OK:
            print_status("ä¾èµ–åº“æ£€æŸ¥å¤±è´¥", "error")


def print_training_summary(result: Dict[str, Any], verbose: bool = False):
    """æ‰“å°è®­ç»ƒç»“æœæ‘˜è¦"""
    if verbose:
        logger.info("=" * 60)
        logger.info("è®­ç»ƒå®Œæˆæ‘˜è¦")
        logger.info("=" * 60)

        if result.get('success', False):
            logger.info(f"âœ“ è®­ç»ƒæˆåŠŸ")
            logger.info(f"æ¨¡å¼: {result.get('selected_mode', 'unknown')}")
            logger.info(f"è®­ç»ƒæ—¶é—´: {result.get('training_time', 0):.2f}ç§’")

            if 'best_reward' in result:
                logger.info(f"æœ€ä½³å¥–åŠ±: {result['best_reward']:.4f}")

            if 'model_path' in result:
                logger.info(f"æ¨¡å‹ä¿å­˜: {result['model_path']}")

            # è¯„ä¼°ç»“æœ
            eval_stats = result.get('eval_stats', {})
            if eval_stats:
                logger.info(f"è¯„ä¼°å¹³å‡å¥–åŠ±: {eval_stats.get('avg_reward', 0):.4f}")
                logger.info(f"è¯„ä¼°æˆåŠŸç‡: {eval_stats.get('success_rate', 0):.2%}")

        else:
            logger.error(f"âœ— è®­ç»ƒå¤±è´¥")
            logger.error(f"é”™è¯¯: {result.get('error', 'Unknown error')}")

        logger.info("=" * 60)
    else:
        # ç®€æ´è¾“å‡º
        print_header("è®­ç»ƒå®Œæˆ")

        if result.get('success', False):
            print_status(f"è®­ç»ƒæˆåŠŸ ({result.get('selected_mode', 'unknown')}æ¨¡å¼)", "success")
            print_result("è®­ç»ƒæ—¶é—´", f"{result.get('training_time', 0):.2f}ç§’")

            if 'best_reward' in result:
                print_result("æœ€ä½³å¥–åŠ±", f"{result['best_reward']:.4f}")

            if 'model_path' in result:
                print_status(f"æ¨¡å‹å·²ä¿å­˜: {Path(result['model_path']).name}")

            # è¯„ä¼°ç»“æœ
            eval_stats = result.get('eval_stats', {})
            if eval_stats:
                print_result("è¯„ä¼°å¹³å‡å¥–åŠ±", f"{eval_stats.get('avg_reward', 0):.4f}")
                print_result("è¯„ä¼°æˆåŠŸç‡", f"{eval_stats.get('success_rate', 0):.2%}")
        else:
            print_status(f"è®­ç»ƒå¤±è´¥: {result.get('error', 'Unknown error')}", "error")


def main():
    """ä¸»å…¥å£å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = setup_argument_parser()
    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    setup_logging(verbose=args.verbose, debug=args.debug)

    try:
        # æ‰“å°ç³»ç»Ÿä¿¡æ¯
        print_system_info(verbose=args.verbose)

        # æ£€æŸ¥ä¾èµ–
        if not DEPENDENCIES_OK:
            print_status(f"ä¾èµ–åº“æ£€æŸ¥å¤±è´¥: {IMPORT_ERROR}", "error")
            print_status("è¯·æ£€æŸ¥ç¯å¢ƒé…ç½®å’Œä¾èµ–åº“å®‰è£…", "error")
            return 1

        # åŠ è½½é…ç½®
        if args.verbose:
            logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
        config = load_and_merge_config(args.config, args)
        
        # æ¨¡æ‹Ÿè¿è¡Œæ¨¡å¼
        if args.dry_run:
            print_header("æ¨¡æ‹Ÿè¿è¡Œæ¨¡å¼")
            print_status("éªŒè¯é…ç½®å’Œä¾èµ–")
            training_manager = TrainingManager(config)

            # éªŒè¯æ‰€æœ‰æ¨¡å¼
            for mode in ['basic', 'topology', 'adaptive', 'unified']:
                is_valid, errors = training_manager.validate_config(mode)
                status = "success" if is_valid else "error"
                print_status(f"{mode}æ¨¡å¼é…ç½®éªŒè¯: {'é€šè¿‡' if is_valid else 'å¤±è´¥'}", status)
                if errors and args.verbose:
                    for error in errors:
                        print_status(f"  - {error}", "warning")

            print_status("æ¨¡æ‹Ÿè¿è¡Œå®Œæˆ", "success")
            return 0
        
        # åˆ›å»ºè®­ç»ƒç®¡ç†å™¨
        training_manager = TrainingManager(config)
        
        # ç¡®å®šè®­ç»ƒæ¨¡å¼
        training_mode = args.mode if args.mode != 'auto' else None
        
        # éè¿è¡Œæ¨¡å¼ - ä»…æ˜¾ç¤ºä¿¡æ¯
        if not args.run:
            selected_mode = training_manager.select_training_mode(training_mode)

            # éªŒè¯é…ç½®
            is_valid, errors = training_manager.validate_config(selected_mode)
            if is_valid:
                print_status(f"é…ç½®éªŒè¯é€šè¿‡ ({selected_mode}æ¨¡å¼)", "success")
                print_status("ä½¿ç”¨ --run å‚æ•°å¼€å§‹è®­ç»ƒ")
            else:
                print_status("é…ç½®éªŒè¯å¤±è´¥", "error")
                if args.verbose:
                    for error in errors:
                        print_status(f"  - {error}", "error")
                return 1

            return 0

        # æ‰§è¡Œè®­ç»ƒ
        print_header(f"å¼€å§‹{training_mode or 'auto'}æ¨¡å¼è®­ç»ƒ")
        result = training_manager.run_training(training_mode)

        # æ‰“å°ç»“æœæ‘˜è¦
        print_training_summary(result, verbose=args.verbose)

        # è¿”å›çŠ¶æ€ç 
        return 0 if result.get('success', False) else 1
        
    except KeyboardInterrupt:
        print_status("ç”¨æˆ·ä¸­æ–­è®­ç»ƒ", "warning")
        return 1
    except Exception as e:
        print_status(f"ç¨‹åºå¼‚å¸¸: {e}", "error")
        if args.debug:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())