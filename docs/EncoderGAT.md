# Encoder GAT

åˆ›å»ºæ—¶é—´: May 11, 2025 11:58 PM
æ ‡ç­¾: Code
çŠ¶æ€: å®Œæˆ

# **GATå›¾ç¥ç»ç½‘ç»œæ¨¡å‹ (`src/gat.py`)**

è¯¥æ–‡ä»¶å®šä¹‰äº†ä¸€ä¸ªä¸“ä¸ºç”µç½‘è®¾è®¡çš„ã€åŸºäº**ç‰©ç†å…ˆéªŒ**çš„**å¼‚æ„å›¾æ³¨æ„åŠ›ç½‘ç»œ (GATv2)** ç¼–ç å™¨ã€‚

å…¶æ ¸å¿ƒæ˜¯å°†ç”µç½‘çš„ç‰©ç†ç‰¹æ€§ï¼ˆå¦‚æ”¯è·¯é˜»æŠ—ï¼‰ç›´æ¥èå…¥åˆ°GNNçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä»è€Œè®©æ¨¡å‹èƒ½å¤Ÿæ›´æ™ºèƒ½åœ°å­¦ä¹ ç”µç½‘çš„æ‹“æ‰‘å’Œç”µæ°”å…³ç³»ã€‚

æ¨¡å‹æœ€ç»ˆè¾“å‡ºé«˜è´¨é‡çš„**èŠ‚ç‚¹åµŒå…¥**å’Œ**å›¾çº§åˆ«åµŒå…¥**ï¼Œå¯ç”¨äºä¸‹æ¸¸çš„å¼ºåŒ–å­¦ä¹ ã€‚

---

# **MODEL - `HeteroGraphEncoder`**

## æ ¸å¿ƒæ¶æ„

æ¨¡å‹é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œä¸»è¦ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼š**ç‰¹å¾é¢„å¤„ç†å±‚**ã€**GNNä¸»å¹²ç½‘ç»œ**å’Œ**è¾“å‡ºå±‚**ã€‚

### 1. **ç‰¹å¾é¢„å¤„ç†å±‚ (`node_projectors`)**

- **ç›®çš„**ï¼šç»Ÿä¸€ä¸åŒèŠ‚ç‚¹ç±»å‹ï¼ˆ`bus_pq`, `bus_pv`, `bus_slack`ï¼‰çš„ç‰¹å¾ç»´åº¦ã€‚
- **å®ç°**ï¼šä¸ºæ¯ç§èŠ‚ç‚¹ç±»å‹åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„çº¿æ€§æŠ•å½±å±‚ (`nn.Linear`)ï¼Œå°†å®ƒä»¬çš„ç‰¹å¾ç»´åº¦æ˜ å°„åˆ°ä¸€ä¸ªç»Ÿä¸€çš„ `max_node_dim`ã€‚è¿™ä½¿å¾—ä¸‹æ¸¸çš„åŒæ„GNNä¸»å¹²ç½‘ç»œå¯ä»¥å¤„ç†å®ƒä»¬ã€‚

### 2. **GNNä¸»å¹²ç½‘ç»œ (`hetero_encoder`)**

è¿™æ˜¯æ¨¡å‹çš„æ ¸å¿ƒï¼Œå®ƒé¦–å…ˆå®šä¹‰ä¸€ä¸ª**åŒæ„**çš„ `GNNEncoder`ï¼Œç„¶åä½¿ç”¨PyGçš„ `to_hetero` å·¥å…·å°†å…¶åŠ¨æ€è½¬æ¢ä¸ºä¸€ä¸ª**å¼‚æ„**æ¨¡å‹ã€‚

### 2.1 **`GNNEncoder` - åŒæ„ä¸»å¹²**

- **ç»“æ„**ï¼šä¸€ä¸ªç”±å¤šä¸ª `PhysicsGATv2Conv` å±‚å †å è€Œæˆçš„ç½‘ç»œã€‚
- **ç‰¹æ€§**ï¼š
    - **æ®‹å·®è¿æ¥**ï¼š`x = x + residual_proj(residual)`ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚
    - **å±‚å½’ä¸€åŒ–**ï¼š`LayerNorm`ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚
    - **Dropout**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

### 2.2 **`PhysicsGATv2Conv` - æ ¸å¿ƒåˆ›æ–°ï¼šç‰©ç†å¼•å¯¼çš„æ³¨æ„åŠ›å±‚**

è¿™æ˜¯æ ‡å‡†çš„ `GATv2Conv` çš„ä¸€ä¸ª**å¢å¼ºå­ç±»**ï¼Œå®ƒåœ¨è®¡ç®—æ³¨æ„åŠ›æƒé‡æ—¶èå…¥äº†ç‰©ç†å…ˆéªŒçŸ¥è¯†ã€‚

**æ•°å­¦åŸç†**:
æ ‡å‡†GATv2çš„æ³¨æ„åŠ›åˆ†æ•° `e_ij` è®¡ç®—å¦‚ä¸‹ï¼š

$$
e_{ij} = \vec{a}^T \cdot \text{LeakyReLU}(\mathbf{W}_l[ \mathbf{W}_r \vec{h}_i || \mathbf{W}r \vec{h}j ] + \mathbf{W}{edge} \vec{e}{ij})
$$

**ç‰©ç†GATv2çš„åˆ›æ–°**åœ¨äºï¼Œå®ƒé€šè¿‡å¢å¼ºè¾¹ç‰¹å¾ `edge_attr` æ¥å¼•å…¥ç‰©ç†å…ˆéªŒï¼š

$$
\text{enhanced\_edge\_attr} = \text{original\_edge\_attr} + \text{physics\_prior}

$$

å…¶ä¸­ï¼Œç‰©ç†å…ˆéªŒ physics_prior ä¸æ”¯è·¯é˜»æŠ— $Z_{ij}$ çš„å€’æ•°æˆæ­£æ¯”ï¼š

$$
\text{physics\_prior} = \frac{\tau \cdot w\_\text{phys}}{|Z_{ij}|}
$$

- **`|Z_ij|`**ï¼šæ”¯è·¯ `i-j` çš„é˜»æŠ—æ¨¡é•¿ã€‚**é˜»æŠ—è¶Šå°ï¼Œå¯¼çº³è¶Šå¤§ï¼Œç‰©ç†å…ˆéªŒæƒé‡è¶Šé«˜**ã€‚
- **`Ï„` (temperature)**ï¼šä¸€ä¸ª**å¯å­¦ä¹ çš„**æ¸©åº¦å‚æ•° `nn.Parameter`ï¼Œç”¨äºè‡ªåŠ¨è°ƒæ•´ç‰©ç†å…ˆéªŒçš„å½±å“å¼ºåº¦ã€‚
- **`w_phys` (physics_weight)**ï¼šä¸€ä¸ªå›ºå®šçš„æƒé‡è¶…å‚æ•°ã€‚

**âœ… æ ¸å¿ƒä¼˜åŠ¿**: æ¨¡å‹åœ¨å­¦ä¹ èŠ‚ç‚¹é—´å…³ç³»æ—¶ï¼Œä¼šå¤©ç„¶åœ°æ›´å…³æ³¨é‚£äº›**ç”µæ°”è¿æ¥æ›´ç´§å¯†ï¼ˆé˜»æŠ—æ›´ä½ï¼‰**çš„æ”¯è·¯ï¼Œè¿™å®Œå…¨ç¬¦åˆç”µç½‘çš„ç‰©ç†è§„å¾‹ã€‚

### 2.3 **`to_hetero` - åŠ¨æ€è½¬æ¢ä¸ºå¼‚æ„æ¨¡å‹**

- **åŠŸèƒ½**ï¼šå°†è®¾è®¡å¥½çš„ `GNNEncoder` åŒ…è£…æˆä¸€ä¸ªå¯ä»¥å¤„ç† `HeteroData` çš„å¼‚æ„æ¨¡å‹ã€‚
- **å®ç°**ï¼š`self.hetero_encoder = to_hetero(gnn_encoder, metadata, aggr='sum')`
- **ä¼˜ç‚¹**ï¼šä»£ç ç®€æ´ï¼Œæ— éœ€ä¸ºæ¯ç§å…³ç³»ç±»å‹æ‰‹åŠ¨ç¼–å†™ä¸åŒçš„GNNå±‚ã€‚æ¨¡å‹å¯ä»¥è‡ªåŠ¨å¤„ç†ä¸åŒç±»å‹çš„èŠ‚ç‚¹å’Œè¾¹ç‰¹å¾ã€‚

### 3. **è¾“å‡ºå±‚ (`output_projector` & `graph_pooling`)**

- **èŠ‚ç‚¹åµŒå…¥**ï¼š
    - GNNä¸»å¹²ç½‘ç»œè¾“å‡ºçš„èŠ‚ç‚¹åµŒå…¥ä¼šç»è¿‡ä¸€ä¸ªå¯é€‰çš„çº¿æ€§æŠ•å½±å±‚ (`output_projector`)ï¼Œä»¥è·å¾—æœ€ç»ˆæ‰€éœ€çš„è¾“å‡ºç»´åº¦ `output_dim`ã€‚
- **å›¾åµŒå…¥**ï¼š
    - **èšåˆ (Pooling)**ï¼šä½¿ç”¨ `global_mean_pool` å°†æ‰€æœ‰èŠ‚ç‚¹çš„åµŒå…¥èšåˆä¸ºä¸€ä¸ªå•ä¸€çš„å›¾çº§åˆ«è¡¨ç¤ºã€‚
    - **æŠ•å½± (Projection)**ï¼šå°†èšåˆåçš„å›¾è¡¨ç¤ºé€šè¿‡ä¸€ä¸ªç®€å•çš„MLP (`graph_projector`) è¿›è¡Œå˜æ¢ï¼Œä»¥å¢å¼ºå…¶è¡¨è¾¾èƒ½åŠ›ã€‚

---

# **INPUT - PyTorch Geometric `HeteroData`**

`HeteroGraphEncoder` çš„è¾“å…¥æ˜¯ä¸€ä¸ªæ ‡å‡†çš„PyG `HeteroData` å¯¹è±¡ï¼Œå…¶ç»“æ„ä¸ `data_processing.py` çš„è¾“å‡ºå®Œå…¨ä¸€è‡´ã€‚

### **å›¾ G=(N,E) - å¼‚æ„å›¾ç»“æ„**

### N èŠ‚ç‚¹é›† - æŒ‰ç‰©ç†ç±»å‹åˆ†ç»„ (`data.node_types`)

| èŠ‚ç‚¹ç±»å‹ | æè¿° |
| --- | --- |
| **`bus_pq`** | PQè´Ÿè·èŠ‚ç‚¹ |
| **`bus_pv`** | PVå‘ç”µèŠ‚ç‚¹ |
| **`bus_slack`** | Slackå¹³è¡¡èŠ‚ç‚¹ |

**èŠ‚ç‚¹ç‰¹å¾ (`data[node_type].x`)**: æ¯ç§ç±»å‹ `(num_nodes_of_type, 14)`

### E è¾¹é›† - æŒ‰è¿æ¥å…³ç³»åˆ†ç»„ (`data.edge_types`)

| è¾¹ç±»å‹ | æè¿° |
| --- | --- |
| **`connects_line`** | è¾“ç”µçº¿è·¯è¿æ¥ |
| **`connects_transformer`** | å˜å‹å™¨è¿æ¥ |

**è¾¹ç‰¹å¾ (`data[edge_type].edge_attr`)**: **âœ… å®Œæ•´çš„9ç»´ç‰¹å¾**

---

# **OUTPUT - åµŒå…¥è¡¨ç¤º & æ³¨æ„åŠ›æƒé‡**

`HeteroGraphEncoder` çš„ `forward` æ–¹æ³•å¯ä»¥æ ¹æ®å‚æ•°è¿”å›ä¸åŒçš„è¾“å‡ºç»„åˆã€‚

### 1. **èŠ‚ç‚¹åµŒå…¥ (`node_embeddings`)**

- **ç±»å‹**: `Dict[str, torch.Tensor]`
- **ç»“æ„**: ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯èŠ‚ç‚¹ç±»å‹å­—ç¬¦ä¸²ï¼Œå€¼æ˜¯è¯¥ç±»å‹æ‰€æœ‰èŠ‚ç‚¹çš„åµŒå…¥å¼ é‡ã€‚
- **å½¢çŠ¶**: æ¯ä¸ªå¼ é‡çš„å½¢çŠ¶ä¸º `[num_nodes_of_type, output_dim]`ã€‚

### 2. **å›¾çº§åˆ«åµŒå…¥ (`graph_embedding`)**

- **ç±»å‹**: `torch.Tensor`
- **ç»“æ„**: ä¸€ä¸ªå•ä¸€çš„å‘é‡ï¼Œä»£è¡¨æ•´ä¸ªç”µç½‘çš„å…¨å±€çŠ¶æ€ã€‚
- **å½¢çŠ¶**: `[1, output_dim]`

### 3. **æ³¨æ„åŠ›æƒé‡ (`attention_weights`)**

- **ç±»å‹**: `List[torch.Tensor]`
- **ç»“æ„**: ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†æ¨¡å‹ä¸­æ‰€æœ‰ `PhysicsGATv2Conv` å±‚åœ¨æ¯æ¬¡å‰å‘ä¼ æ’­ä¸­è®¡ç®—å‡ºçš„æ³¨æ„åŠ›æƒé‡ `alpha`ã€‚
- **ç”¨é€”**: **å¯è§£é‡Šæ€§åˆ†æ**ã€‚å¯ä»¥ç”¨æ¥å¯è§†åŒ–å“ªäº›æ”¯è·¯åœ¨æ¨¡å‹çš„å†³ç­–ä¸­è¢«è®¤ä¸ºæ›´é‡è¦ã€‚

---

# **API & ä½¿ç”¨æ–¹æ³•**

### **åˆ›å»ºæ¨¡å‹**

ä½¿ç”¨ä¾¿æ·çš„å·¥å‚å‡½æ•° `create_hetero_graph_encoder`ã€‚

```python
from src.gat import create_hetero_graph_encoder

# dataæ˜¯HeteroDataå¯¹è±¡
encoder = create_hetero_graph_encoder(
    data,
    hidden_channels=64,
    gnn_layers=3,
    heads=4,
    output_dim=128
)

```

### **è·å–åµŒå…¥å’Œæƒé‡**

```python
# 1. åªè·å–èŠ‚ç‚¹åµŒå…¥ (æœ€é«˜æ•ˆ)
node_embs = encoder.encode_nodes(data)

# 2. åªè·å–å›¾åµŒå…¥
graph_emb = encoder.encode_graph(data)

# 3. è·å–æ‰€æœ‰è¾“å‡º
node_embs, attention_weights, graph_emb = encoder(
    data,
    return_attention_weights=True,
    return_graph_embedding=True
)

# 4. å•ç‹¬è·å–æ³¨æ„åŠ›æƒé‡ (åœ¨å‰å‘ä¼ æ’­åè°ƒç”¨)
att_weights = encoder.get_attention_weights()

```

---

# **Exampleï¼š**

### 1. **æ„å»ºè¾“å…¥ (`HeteroData` å¯¹è±¡)**

é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªä»£è¡¨è¿™ä¸ªå¾®å‹ç”µç½‘çš„ `HeteroData` å¯¹è±¡ã€‚

```python
import torch
from torch_geometric.data import HeteroData
from src.gat import create_hetero_graph_encoder

# --- æ¨¡æ‹Ÿè¾“å…¥æ•°æ® ---
data = HeteroData()

# èŠ‚ç‚¹ç±»å‹1: 'bus_pq' (2ä¸ªPQèŠ‚ç‚¹)
# 14ç»´ç‰¹å¾ (è¿™é‡Œç”¨éšæœºæ•°ä»£æ›¿)
data['bus_pq'].x = torch.randn(2, 14)
data['bus_pq'].global_ids = torch.tensor([2, 3]) # åŸå§‹æ¯çº¿ID

# èŠ‚ç‚¹ç±»å‹2: 'bus_pv' (1ä¸ªPVèŠ‚ç‚¹)
data['bus_pv'].x = torch.randn(1, 14)
data['bus_pv'].global_ids = torch.tensor([1])

# èŠ‚ç‚¹ç±»å‹3: 'bus_slack' (1ä¸ªSlackèŠ‚ç‚¹)
data['bus_slack'].x = torch.randn(1, 14)
data['bus_slack'].global_ids = torch.tensor([0])

# è¾¹å…³ç³»1: slack(0) å’Œ pv(0) ä¹‹é—´æœ‰ä¸€æ¡çº¿è·¯è¿æ¥
# (æ³¨æ„ï¼šç´¢å¼•æ˜¯ç›¸å¯¹äºå„è‡ªç±»å‹çš„å±€éƒ¨ç´¢å¼•)
data['bus_slack', 'connects_line', 'bus_pv'].edge_index = torch.tensor([[0], [0]], dtype=torch.long)
# 9ç»´è¾¹ç‰¹å¾ (è¿™é‡Œç”¨éšæœºæ•°ä»£æ›¿)
data['bus_slack', 'connects_line', 'bus_pv'].edge_attr = torch.randn(1, 9)

# è¾¹å…³ç³»2: pv(0) å’Œ pq(0) ä¹‹é—´æœ‰ä¸€æ¡å˜å‹å™¨è¿æ¥
data['bus_pv', 'connects_transformer', 'bus_pq'].edge_index = torch.tensor([[0], [0]], dtype=torch.long)
data['bus_pv', 'connects_transformer', 'bus_pq'].edge_attr = torch.randn(1, 9)

# è¾¹å…³ç³»3: pq(0) å’Œ pq(1) ä¹‹é—´æœ‰ä¸€æ¡çº¿è·¯è¿æ¥
data['bus_pq', 'connects_line', 'bus_pq'].edge_index = torch.tensor([[0], [1]], dtype=torch.long)
data['bus_pq', 'connects_line', 'bus_pq'].edge_attr = torch.randn(1, 9)

print("--- INPUT: HeteroData Object ---")
print(data)
print("--------------------------------\\n")

```

### 2. **åˆå§‹åŒ–æ¨¡å‹å¹¶æ‰§è¡Œå‰å‘ä¼ æ’­**

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ›å»ºç¼–ç å™¨ï¼Œå¹¶å°†ä¸Šé¢æ„å»ºçš„ `data` å¯¹è±¡ä½œä¸ºè¾“å…¥ä¼ é€’ç»™å®ƒã€‚

```python
# --- åˆå§‹åŒ–æ¨¡å‹ ---
# output_dim=32 è¡¨ç¤ºæˆ‘ä»¬å¸Œæœ›æœ€ç»ˆçš„åµŒå…¥æ˜¯32ç»´çš„
encoder = create_hetero_graph_encoder(
    data,
    hidden_channels=16,
    gnn_layers=2, # 2å±‚GNN
    heads=4,      # 4ä¸ªæ³¨æ„åŠ›å¤´
    output_dim=32
)

# --- æ‰§è¡Œå‰å‘ä¼ æ’­ï¼Œè·å–æ‰€æœ‰è¾“å‡º ---
# å°†dataå¯¹è±¡å’Œæ¨¡å‹éƒ½ç§»åŠ¨åˆ°åŒä¸€ä¸ªè®¾å¤‡ (ä¾‹å¦‚CPU)
device = torch.device('cpu')
data = data.to(device)
encoder = encoder.to(device)

# ä¸è®¡ç®—æ¢¯åº¦ï¼Œä»¥åŠ å¿«é€Ÿåº¦
with torch.no_grad():
    node_embeddings, attention_weights, graph_embedding = encoder(
        data,
        return_attention_weights=True,
        return_graph_embedding=True
    )

```

### 3. **æŸ¥çœ‹è¾“å‡ºç»“æœ**

ä¸‹é¢æ˜¯ä¸Šè¿°ä»£ç è¿è¡Œåï¼Œå„ä¸ªè¾“å‡ºå˜é‡çš„å†…å®¹å’Œå½¢çŠ¶ã€‚

```python
# --- OUTPUT: ç»“æœå±•ç¤º ---

print("--- OUTPUT 1: Node Embeddings (Dict[str, Tensor]) ---")
for node_type, embeddings in node_embeddings.items():
    print(f"Node type '{node_type}':")
    print(f"  - Shape: {embeddings.shape}")
    # print(f"  - Embeddings:\\n{embeddings}") # å–æ¶ˆæ³¨é‡Šä»¥æŸ¥çœ‹å…·ä½“æ•°å€¼
print("-----------------------------------------------------\\n")

print("--- OUTPUT 2: Graph Embedding (Tensor) ---")
print(f"Shape: {graph_embedding.shape}")
# print(f"Embedding:\\n{graph_embedding}")
print("-------------------------------------------\\n")

print("--- OUTPUT 3: Attention Weights (List[Tensor]) ---")
# å› ä¸ºæœ‰2å±‚GNNï¼Œæ¯å±‚æœ‰3ç§å…³ç³»ï¼Œæ‰€ä»¥ç†è®ºä¸Šä¼šæœ‰ 2*3=6 ä¸ªæƒé‡å¼ é‡
# (æ³¨æ„ï¼šå¦‚æœæŸå…³ç³»åœ¨æŸå±‚æ²¡æœ‰è¾¹ï¼Œå¯èƒ½ä¸ä¼šäº§ç”Ÿæƒé‡)
print(f"Number of attention tensors found: {len(attention_weights)}")
for i, weights in enumerate(attention_weights):
    # å½¢çŠ¶æ˜¯ [è¾¹çš„æ•°é‡, æ³¨æ„åŠ›å¤´çš„æ•°é‡]
    print(f"Tensor {i}:")
    print(f"  - Shape: {weights.shape}")
    # print(f"  - Weights (softmax scores):\\n{weights}")
print("--------------------------------------------------\\n")

```

### **è¾“å‡ºç»“æœç¤ºä¾‹**

è¿è¡Œä¸Šè¿°ä»£ç ï¼Œä½ ä¼šå¾—åˆ°ç±»ä¼¼ä¸‹é¢çš„è¾“å‡ºï¼ˆç”±äºéšæœºåˆå§‹åŒ–ï¼Œå…·ä½“æ•°å€¼ä¼šä¸åŒï¼‰ï¼š

```
--- INPUT: HeteroData Object ---
HeteroData(
  bus_pq={
    x[2, 14],
    global_ids[2]
  },
  bus_pv={
    x[1, 14],
    global_ids[1]
  },
  bus_slack={
    x[1, 14],
    global_ids[1]
  },
  ('bus_slack', 'connects_line', 'bus_pv')={
    edge_index[2, 1],
    edge_attr[1, 9]
  },
  ('bus_pv', 'connects_transformer', 'bus_pq')={
    edge_index[2, 1],
    edge_attr[1, 9]
  },
  ('bus_pq', 'connects_line', 'bus_pq')={
    edge_index[2, 1],
    edge_attr[1, 9]
  }
)
--------------------------------

--- OUTPUT 1: Node Embeddings (Dict[str, Tensor]) ---
Node type 'bus_pq':
  - Shape: torch.Size([2, 32])
Node type 'bus_pv':
  - Shape: torch.Size([1, 32])
Node type 'bus_slack':
  - Shape: torch.Size([1, 32])
-----------------------------------------------------

--- OUTPUT 2: Graph Embedding (Tensor) ---
Shape: torch.Size([1, 32])
-------------------------------------------

--- OUTPUT 3: Attention Weights (List[Tensor]) ---
Number of attention tensors found: 6
Tensor 0:
  - Shape: torch.Size([1, 4])
Tensor 1:
  - Shape: torch.Size([1, 4])
Tensor 2:
  - Shape: torch.Size([1, 4])
Tensor 3:
  - Shape: torch.Size([1, 4])
Tensor 4:
  - Shape: torch.Size([1, 4])
Tensor 5:
  - Shape: torch.Size([1, 4])
--------------------------------------------------

```

è¿™ä¸ªä¾‹å­æ¸…æ™°åœ°å±•ç¤ºäº†ï¼š

- **è¾“å…¥**æ˜¯ä¸€ä¸ªç»“æ„åŒ–çš„ `HeteroData` å¯¹è±¡ã€‚
- **è¾“å‡º**æ˜¯ä¸‰ä¸ªéƒ¨åˆ†ï¼šä¸€ä¸ªåŒ…å«å„ç±»èŠ‚ç‚¹åµŒå…¥çš„**å­—å…¸**ï¼Œä¸€ä¸ªä»£è¡¨å…¨å›¾çš„**å¼ é‡**ï¼Œä»¥åŠä¸€ä¸ªåŒ…å«å„å±‚æ³¨æ„åŠ›åˆ†æ•°çš„**åˆ—è¡¨**ã€‚

---

# Code

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATv2Conv, to_hetero, LayerNorm, global_mean_pool
from torch_geometric.data import HeteroData
from torch_geometric.utils import softmax
from typing import Optional, List, Dict, Tuple, Union
import numpy as np
import warnings

# å­˜å‚¨æ•è·åˆ°çš„æ³¨æ„åŠ›æƒé‡çš„å…¨å±€åˆ—è¡¨ï¼ˆæˆ–ç±»å±æ€§ï¼‰
captured_attention_weights = []

def attention_hook(module, input, output):
    """ä¸€ä¸ªforward hookï¼Œç”¨äºæ•è·æ³¨æ„åŠ›æƒé‡"""
    # GATv2Convåœ¨return_attention_weights=Trueæ—¶ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªå…ƒç»„(out, attention_details)
    # attention_details æ˜¯ (edge_index, alpha)
    if isinstance(output, tuple) and len(output) == 2:
        # æˆ‘ä»¬åªå…³å¿ƒ alpha å¼ é‡
        if isinstance(output[1], tuple) and len(output[1]) == 2:
            captured_attention_weights.append(output[1][1])

class PhysicsGATv2Conv(GATv2Conv):
    """
    ç‰©ç†å¼•å¯¼çš„GAT
    
    æ ¸å¿ƒåˆ›æ–°ï¼š
    1. åŸºäºæœ€æ–°çš„GATv2æ¶æ„ï¼Œå…·æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›
    2. åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­èå…¥ç”µæ°”é˜»æŠ—å…ˆéªŒ
    3. é˜»æŠ—è¶Šå°çš„è¿æ¥è·å¾—è¶Šé«˜çš„æ³¨æ„åŠ›æƒé‡
    4. å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°æ§åˆ¶ç‰©ç†å…ˆéªŒçš„å½±å“ç¨‹åº¦
    
    æ•°å­¦åŸç†ï¼š
    æ ‡å‡†GATv2: Î±_ij = softmax(W_a^T LeakyReLU(W_l[W_r h_i || W_r h_j] + b))
    ç‰©ç†GATv2: Î±_ij = softmax(W_a^T LeakyReLU(W_l[W_r h_i || W_r h_j] + b) + Ï„/|Z_ij|)
    """
    
    def __init__(self, in_channels: int, out_channels: int, heads: int = 8,
                 concat: bool = True, dropout: float = 0.6, 
                 edge_dim: Optional[int] = None, temperature: float = 1.0,
                 z_index: int = 3, physics_weight: float = 1.0, **kwargs):
        
        super().__init__(in_channels, out_channels, heads=heads, concat=concat,
                        dropout=dropout, edge_dim=edge_dim, **kwargs)
        
        # ç‰©ç†å¼•å¯¼å‚æ•°
        self.temperature = nn.Parameter(torch.tensor(temperature))
        self.physics_weight = physics_weight
        self.z_index = z_index  # é˜»æŠ—æ¨¡é•¿åœ¨è¾¹ç‰¹å¾ä¸­çš„ç´¢å¼•
        
        # è¾¹ç‰¹å¾å¤„ç†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if edge_dim is not None:
            self.edge_processor = nn.Linear(edge_dim, heads)
        else:
            self.edge_processor = None
    
    def forward(self, x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], 
                edge_index: torch.Tensor,
                edge_attr: Optional[torch.Tensor] = None,
                return_attention_weights: Optional[bool] = None):
        """
        å‰å‘ä¼ æ’­ï¼Œåœ¨æ ‡å‡†GATv2åŸºç¡€ä¸Šèå…¥ç‰©ç†å…ˆéªŒå¹¶ç¼“å­˜æ³¨æ„åŠ›æƒé‡
        """
        # å¤„ç†ç‰©ç†å…ˆéªŒå¢å¼ºçš„è¾¹ç‰¹å¾
        if edge_attr is not None:
            enhanced_edge_attr = self.physics_enhanced_edge_attr(edge_attr)
        else:
            enhanced_edge_attr = edge_attr
        
        # å§‹ç»ˆè¯·æ±‚æ³¨æ„åŠ›æƒé‡
        result = super().forward(
            x, edge_index, enhanced_edge_attr, return_attention_weights=True
        )
        
        out, attention_details = result
        
        # å°†æ³¨æ„åŠ›åˆ†æ•°å­˜å‚¨åœ¨å®ä¾‹å±æ€§ä¸­ï¼Œä»¥ä¾¿åç»­æå–
        self._alpha = attention_details[1]

        # æ ¹æ®è°ƒç”¨è€…çš„è¦æ±‚å†³å®šè¿”å›å€¼
        if return_attention_weights:
            return out, attention_details
        return out
    
    def physics_enhanced_edge_attr(self, edge_attr: torch.Tensor) -> torch.Tensor:
        """
        å¢å¼ºè¾¹ç‰¹å¾ï¼Œèå…¥ç‰©ç†å…ˆéªŒ
        """
        if edge_attr is None or edge_attr.shape[1] <= self.z_index:
            return edge_attr
        
        # æå–é˜»æŠ—æ¨¡é•¿
        z_magnitude = edge_attr[:, self.z_index].clamp(min=1e-6)
        
        # è®¡ç®—ç‰©ç†æƒé‡ï¼š1/|Z| (å¯¼çº³)
        physics_prior = self.physics_weight / z_magnitude
        
        # å°†ç‰©ç†å…ˆéªŒæ·»åŠ åˆ°è¾¹ç‰¹å¾ä¸­
        enhanced_edge_attr = edge_attr.clone()
        if enhanced_edge_attr.shape[1] < self.z_index + 2:
            # å¦‚æœç©ºé—´ä¸å¤Ÿï¼Œæ‰©å±•ç‰¹å¾ç»´åº¦
            padding = torch.zeros(
                enhanced_edge_attr.shape[0], 
                self.z_index + 2 - enhanced_edge_attr.shape[1],
                device=enhanced_edge_attr.device
            )
            enhanced_edge_attr = torch.cat([enhanced_edge_attr, padding], dim=1)
        
        # å°†ç‰©ç†å…ˆéªŒå­˜å‚¨åˆ°è¾¹ç‰¹å¾çš„æœ€åä¸€åˆ—
        enhanced_edge_attr[:, -1] = physics_prior * self.temperature
        
        return enhanced_edge_attr

class GNNEncoder(nn.Module):
    """
    åŒæ„GNNç¼–ç å™¨
    
    ç‰¹ç‚¹ï¼š
    1. å¤šå±‚PhysicsGATv2Convå †å 
    2. å±‚å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥
    3. è‡ªé€‚åº”dropout
    4. æ”¯æŒä¸åŒçš„èšåˆæ–¹å¼
    """
    
    def __init__(self, in_channels: int, hidden_channels: int, 
                 num_layers: int = 3, heads: int = 4, dropout: float = 0.3,
                 edge_dim: Optional[int] = None, activation: str = 'elu'):
        super().__init__()
        
        self.num_layers = num_layers
        self.activation = getattr(F, activation)
        
        # æ„å»ºå¤šå±‚ç½‘ç»œ
        self.convs = nn.ModuleList()
        self.norms = nn.ModuleList()
        self.residual_projs = nn.ModuleList()
        
        current_dim = in_channels
        
        for i in range(num_layers):
            # æœ€åä¸€å±‚ä¸ä½¿ç”¨å¤šå¤´concat
            is_last_layer = (i == num_layers - 1)
            concat = not is_last_layer
            
            # GATv2å·ç§¯å±‚
            conv = PhysicsGATv2Conv(
                current_dim, 
                hidden_channels,
                heads=heads,
                concat=concat,
                dropout=dropout,
                edge_dim=edge_dim,
                add_self_loops=False
            )
            self.convs.append(conv)
            
            # è®¡ç®—å®é™…è¾“å‡ºç»´åº¦
            actual_out_dim = hidden_channels * heads if concat else hidden_channels
            
            # å±‚å½’ä¸€åŒ–
            self.norms.append(LayerNorm(actual_out_dim))
            
            # æ®‹å·®æŠ•å½±
            if current_dim != actual_out_dim:
                self.residual_projs.append(nn.Linear(current_dim, actual_out_dim))
            else:
                self.residual_projs.append(nn.Identity())
            
            current_dim = actual_out_dim
        
        self.final_dim = current_dim
    
    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, 
                edge_attr: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        """
        for i, (conv, norm, residual_proj) in enumerate(
            zip(self.convs, self.norms, self.residual_projs)
        ):
            # ä¿å­˜æ®‹å·®
            residual = x
            
            # å·ç§¯
            x = conv(x, edge_index, edge_attr)
            
            # ç¡®ä¿xæ˜¯tensorè€Œä¸æ˜¯tuple
            if isinstance(x, tuple):
                x = x[0]
            
            # å±‚å½’ä¸€åŒ–
            x = norm(x)
            
            # æ®‹å·®è¿æ¥
            x = x + residual_proj(residual)
            
            # æ¿€æ´»å‡½æ•°ï¼ˆæœ€åä¸€å±‚é™¤å¤–ï¼‰
            if i < self.num_layers - 1:
                x = self.activation(x)
        
        return x

class HeteroGraphEncoder(nn.Module):
    """
    å¼‚æ„å›¾ç¼–ç å™¨ - ä¸“æ³¨äºå›¾è¡¨ç¤ºå­¦ä¹ 
    
    èŒè´£ï¼š
    1. å¤„ç†å¼‚æ„å›¾æ•°æ®ï¼ˆå¤šç§èŠ‚ç‚¹ç±»å‹å’Œè¾¹ç±»å‹ï¼‰
    2. æå–é«˜è´¨é‡çš„èŠ‚ç‚¹åµŒå…¥è¡¨ç¤º
    3. èå…¥ç”µç½‘ç‰©ç†å…ˆéªŒçŸ¥è¯†
    4. æ”¯æŒä¸‹æ¸¸ä»»åŠ¡çš„ç‰¹å¾æå–
    """
    
    def __init__(self, 
                 node_feature_dims: Dict[str, int],
                 edge_feature_dims: Dict[str, int],
                 metadata: Tuple[List[str], List[Tuple[str, str, str]]],
                 hidden_channels: int = 64,
                 gnn_layers: int = 3,
                 heads: int = 4,
                 dropout: float = 0.3,
                 output_dim: Optional[int] = None):
        super().__init__()
        
        self.node_types = metadata[0]
        self.edge_types = metadata[1]
        self.hidden_channels = hidden_channels
        self.output_dim = output_dim or hidden_channels
        
        # --- 1. ç‰¹å¾é¢„å¤„ç†å±‚ ---
        # ç»Ÿä¸€æ‰€æœ‰èŠ‚ç‚¹ç±»å‹çš„ç‰¹å¾ç»´åº¦
        max_node_dim = max(node_feature_dims.values())
        max_edge_dim = max(edge_feature_dims.values()) if edge_feature_dims else None
        
        # ä¸ºä¸åŒç±»å‹çš„èŠ‚ç‚¹åˆ›å»ºç‰¹å¾æŠ•å½±å±‚
        self.node_projectors = nn.ModuleDict()
        for node_type, feature_dim in node_feature_dims.items():
            if feature_dim != max_node_dim:
                self.node_projectors[node_type] = nn.Linear(feature_dim, max_node_dim)
            else:
                self.node_projectors[node_type] = nn.Identity()
        
        # --- 2. ä¸»å¹²ç½‘ç»œ (Backbone) ---
        # åˆ›å»ºåŒæ„GNNç¼–ç å™¨
        gnn_encoder = GNNEncoder(
            in_channels=max_node_dim,
            hidden_channels=hidden_channels,
            num_layers=gnn_layers,
            heads=heads,
            dropout=dropout,
            edge_dim=max_edge_dim
        )
        
        # è½¬æ¢ä¸ºå¼‚æ„æ¨¡å‹
        # åœ¨å®šä¹‰äº†ç¡®åˆ‡çš„ä¾èµ–å…³ç³»åï¼Œæˆ‘ä»¬ç¡®ä¿¡ to_hetero ä¼šç¨³å®šå·¥ä½œ
        self.hetero_encoder = to_hetero(gnn_encoder, metadata, aggr='sum')
        print("âœ… ä½¿ç”¨ to_hetero è½¬æ¢çš„å¼‚æ„ç¼–ç å™¨")
        
        self.final_dim = gnn_encoder.final_dim
        
        # --- 3. è¾“å‡ºæŠ•å½±å±‚ ---
        encoder_output_dim = gnn_encoder.final_dim
        if self.output_dim != encoder_output_dim:
            self.output_projector = nn.Linear(encoder_output_dim, self.output_dim)
        else:
            self.output_projector = nn.Identity()
        
        # --- 4. å›¾çº§åˆ«è¡¨ç¤ºèšåˆå™¨ ---
        self.graph_pooling = global_mean_pool
        self.graph_projector = nn.Sequential(
            nn.Linear(self.output_dim, hidden_channels),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels, self.output_dim)
        )
    
    def preprocess_node_features(self, x_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        é¢„å¤„ç†èŠ‚ç‚¹ç‰¹å¾ï¼Œç¡®ä¿æ‰€æœ‰ç±»å‹å…·æœ‰ç›¸åŒç»´åº¦
        """
        processed_x_dict = {}
        for node_type, x in x_dict.items():
            processed_x_dict[node_type] = self.node_projectors[node_type](x)
        return processed_x_dict
    
    def forward(self, data: HeteroData, 
                return_attention_weights: bool = False,
                return_graph_embedding: bool = False) -> Union[Dict[str, torch.Tensor], 
                                                               Tuple[Dict[str, torch.Tensor], ...]]:
        """
        å‰å‘ä¼ æ’­ - æå–èŠ‚ç‚¹å’Œå›¾çº§åˆ«çš„åµŒå…¥è¡¨ç¤º
        
        å‚æ•°:
            data: å¼‚æ„å›¾æ•°æ®
            return_attention_weights: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
            return_graph_embedding: æ˜¯å¦è¿”å›å›¾çº§åˆ«åµŒå…¥
        
        è¿”å›:
            node_embeddings: å„ç±»å‹èŠ‚ç‚¹çš„åµŒå…¥è¡¨ç¤º {node_type: embeddings}
            attention_weights: æ³¨æ„åŠ›æƒé‡ (å¯é€‰)
            graph_embedding: å›¾çº§åˆ«åµŒå…¥ (å¯é€‰)
        """
        # --- 1. é¢„å¤„ç†èŠ‚ç‚¹ç‰¹å¾ ---
        processed_x_dict = self.preprocess_node_features(data.x_dict)
        
        # --- 2. å¼‚æ„GNNç¼–ç  ---
        raw_embeddings = self.hetero_encoder(
            processed_x_dict,
            data.edge_index_dict,
            data.edge_attr_dict
        )
        # åº”ç”¨è¾“å‡ºæŠ•å½±
        node_embeddings = {
            node_type: self.output_projector(embeddings)
            for node_type, embeddings in raw_embeddings.items()
        }
        
        # --- 3. å‡†å¤‡è¿”å›å€¼ ---
        results = [node_embeddings]
        
        # --- 4. æ³¨æ„åŠ›æƒé‡ï¼ˆå¯é€‰ï¼‰---
        if return_attention_weights:
            attention_weights = self.get_attention_weights()
            results.append(attention_weights)
        
        # --- 5. å›¾çº§åˆ«åµŒå…¥ï¼ˆå¯é€‰ï¼‰---
        if return_graph_embedding:
            # èšåˆæ‰€æœ‰èŠ‚ç‚¹åµŒå…¥ä¸ºå›¾çº§åˆ«è¡¨ç¤º
            all_embeddings = torch.cat(list(node_embeddings.values()), dim=0)
            if all_embeddings.size(0) > 0:
                batch = torch.zeros(all_embeddings.size(0), 
                                  dtype=torch.long, 
                                  device=all_embeddings.device)
                graph_embedding = self.graph_pooling(all_embeddings, batch)
                graph_embedding = self.graph_projector(graph_embedding)
            else:
                graph_embedding = torch.zeros(1, self.output_dim, device=data.x_dict[list(data.x_dict.keys())[0]].device)
            results.append(graph_embedding)
        
        # æ ¹æ®è¿”å›å‚æ•°å†³å®šè¿”å›æ ¼å¼
        if len(results) == 1:
            return results[0]
        else:
            return tuple(results)
    
    def encode_nodes(self, data: HeteroData) -> Dict[str, torch.Tensor]:
        """
        ä¾¿æ·æ–¹æ³•ï¼šä»…æå–èŠ‚ç‚¹åµŒå…¥
        """
        return self.forward(data, return_attention_weights=False, return_graph_embedding=False)
    
    def encode_graph(self, data: HeteroData) -> torch.Tensor:
        """
        ä¾¿æ·æ–¹æ³•ï¼šä»…æå–å›¾çº§åˆ«åµŒå…¥
        """
        _, graph_embedding = self.forward(data, return_attention_weights=False, return_graph_embedding=True)
        return graph_embedding
    
    def get_attention_weights(self) -> List[torch.Tensor]:
        """
        è·å–æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ–åˆ†æ
        
        æ³¨æ„ï¼što_heteroè½¬æ¢åï¼Œæ¨¡å‹ç»“æ„ä¼šå‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦é€’å½’æœç´¢æ‰€æœ‰æ¨¡å—
        """
        attention_weights = []
        
        def collect_attention_weights(module):
            """é€’å½’æ”¶é›†æ³¨æ„åŠ›æƒé‡"""
            # æ£€æŸ¥å½“å‰æ¨¡å—æ˜¯å¦æ˜¯PhysicsGATv2Conv
            if isinstance(module, PhysicsGATv2Conv):
                if hasattr(module, '_alpha') and module._alpha is not None:
                    attention_weights.append(module._alpha)
            
            # é€’å½’æ£€æŸ¥å­æ¨¡å—
            for child in module.children():
                collect_attention_weights(child)
        
        # ä»hetero_encoderå¼€å§‹é€’å½’æœç´¢
        if hasattr(self, 'hetero_encoder'):
            collect_attention_weights(self.hetero_encoder)
        
        return attention_weights

    def get_embedding_dim(self) -> int:
        """
        è·å–åµŒå…¥ç»´åº¦
        """
        return self.output_dim

def create_hetero_graph_encoder(data: HeteroData, 
                               hidden_channels: int = 64,
                               gnn_layers: int = 3,
                               heads: int = 4,
                               dropout: float = 0.3,
                               output_dim: Optional[int] = None) -> HeteroGraphEncoder:
    """
    åˆ›å»ºå¼‚æ„å›¾ç¼–ç å™¨çš„ä¾¿æ·å‡½æ•°
    
    å‚æ•°:
        data: å¼‚æ„å›¾æ•°æ®ç¤ºä¾‹
        hidden_channels: éšè—å±‚ç»´åº¦
        gnn_layers: GNNå±‚æ•°
        heads: æ³¨æ„åŠ›å¤´æ•°
        dropout: dropoutæ¦‚ç‡
        output_dim: è¾“å‡ºåµŒå…¥ç»´åº¦
    
    è¿”å›:
        é…ç½®å¥½çš„HeteroGraphEncoderæ¨¡å‹
    """
    # æå–èŠ‚ç‚¹å’Œè¾¹ç‰¹å¾ç»´åº¦
    node_feature_dims = {node_type: x.shape[1] for node_type, x in data.x_dict.items()}
    edge_feature_dims = {edge_type: attr.shape[1] for edge_type, attr in data.edge_attr_dict.items()}
    
    # åˆ›å»ºç¼–ç å™¨
    encoder = HeteroGraphEncoder(
        node_feature_dims=node_feature_dims,
        edge_feature_dims=edge_feature_dims,
        metadata=data.metadata(),
        hidden_channels=hidden_channels,
        gnn_layers=gnn_layers,
        heads=heads,
        dropout=dropout,
        output_dim=output_dim
    )
    
    return encoder

def test_hetero_graph_encoder(data: HeteroData, device: torch.device):
    """
    æµ‹è¯•å¼‚æ„å›¾ç¼–ç å™¨
    """
    print("\nğŸ§  æµ‹è¯•å¼‚æ„Physics-Guided GATv2ç¼–ç å™¨...")
    
    # åˆ›å»ºç¼–ç å™¨
    encoder = create_hetero_graph_encoder(
        data, 
        hidden_channels=32, 
        gnn_layers=2, 
        heads=4, 
        output_dim=64
    )
    encoder = encoder.to(device)
    data = data.to(device)
    
    # æµ‹è¯•ä¸åŒçš„å‰å‘ä¼ æ’­æ¨¡å¼
    with torch.no_grad():
        # 1. ä»…æå–èŠ‚ç‚¹åµŒå…¥
        node_embeddings = encoder.encode_nodes(data)
        
        # 2. æå–å›¾çº§åˆ«åµŒå…¥
        graph_embedding = encoder.encode_graph(data)
        
        # 3. å®Œæ•´å‰å‘ä¼ æ’­ï¼ˆåŒ…å«æ³¨æ„åŠ›æƒé‡ï¼‰
        node_emb, attention_weights, graph_emb = encoder(
            data, 
            return_attention_weights=True, 
            return_graph_embedding=True
        )
    
    print(f"âœ… å¼‚æ„å›¾ç¼–ç å™¨æµ‹è¯•æˆåŠŸï¼")
    print(f"ğŸ“Š ç¼–ç å™¨å‚æ•°é‡: {sum(p.numel() for p in encoder.parameters()):,}")
    print(f"ğŸ“Š è¾“å‡ºåµŒå…¥ç»´åº¦: {encoder.get_embedding_dim()}")
    print(f"ğŸ“Š èŠ‚ç‚¹ç±»å‹æ•°é‡: {len(node_embeddings)}")
    
    for node_type, embeddings in node_embeddings.items():
        print(f"   - {node_type}: {embeddings.shape}")
    
    print(f"ğŸ“Š å›¾çº§åˆ«åµŒå…¥å½¢çŠ¶: {graph_embedding.shape}")
    print(f"ğŸ“Š æ³¨æ„åŠ›æƒé‡æ•°é‡: {len(attention_weights)}")
    
    return encoder

if __name__ == "__main__":
    print("ğŸ”¥ å¼‚æ„Physics-Guided GATv2å›¾ç¼–ç å™¨ - ä¸“æ³¨äºè¡¨ç¤ºå­¦ä¹ ï¼")
    print("ğŸ“– ä½¿ç”¨è¯´æ˜ï¼š")
    print("1. ä½¿ç”¨ create_hetero_graph_encoder() åˆ›å»ºç¼–ç å™¨")
    print("2. ä½¿ç”¨ encoder.encode_nodes() æå–èŠ‚ç‚¹åµŒå…¥")
    print("3. ä½¿ç”¨ encoder.encode_graph() æå–å›¾çº§åˆ«åµŒå…¥")
    print("4. ç¼–ç å™¨ä¸“æ³¨äºç‰¹å¾æå–ï¼Œä¸åŒ…å«å†³ç­–é€»è¾‘")

```