# ç”µåŠ›ç½‘ç»œåˆ†åŒºå¼ºåŒ–å­¦ä¹ ç³»ç»ŸæŠ€æœ¯æ–‡æ¡£

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»ç³»ç»Ÿå„ä¸ªæ¨¡å—çš„æŠ€æœ¯å®ç°å’Œä½¿ç”¨æ–¹æ³•ã€‚

## ğŸ“ ç³»ç»Ÿæ¶æ„

```
src/
â”œâ”€â”€ data_processing.py     # å¼‚æ„å›¾æ•°æ®å¤„ç†
â”œâ”€â”€ gat.py                # ç‰©ç†å¢å¼ºGATç¼–ç å™¨
â”œâ”€â”€ visualization.py      # å¯è§†åŒ–ç®¡ç†å™¨
â””â”€â”€ rl/                   # å¼ºåŒ–å­¦ä¹ æ¨¡å—
    â”œâ”€â”€ environment.py    # åˆ†åŒºç¯å¢ƒ
    â”œâ”€â”€ agent.py          # PPOæ™ºèƒ½ä½“
    â”œâ”€â”€ state.py          # çŠ¶æ€ç®¡ç†
    â”œâ”€â”€ action_space.py   # åŠ¨ä½œç©ºé—´
    â”œâ”€â”€ reward.py         # å¥–åŠ±å‡½æ•°
    â”œâ”€â”€ utils.py          # å·¥å…·å‡½æ•°
    â”œâ”€â”€ scenario_generator.py  # åœºæ™¯ç”Ÿæˆå™¨
    â””â”€â”€ gym_wrapper.py    # OpenAI GymåŒ…è£…å™¨
```

## ğŸ”§ æ ¸å¿ƒæ¨¡å—è¯¦è§£

### 1. æ•°æ®å¤„ç†æ¨¡å— (`src/data_processing.py`)

**åŠŸèƒ½**: å°†MATPOWERæ ¼å¼çš„ç”µåŠ›ç³»ç»Ÿæ•°æ®è½¬æ¢ä¸ºPyTorch Geometricå¼‚æ„å›¾

**æ ¸å¿ƒç±»**: `PowerGridDataProcessor`

**ä¸»è¦ç‰¹æ€§**:
- **å¼‚æ„å›¾æ„å»º** - åŒºåˆ†ä¸åŒç±»å‹çš„èŠ‚ç‚¹ï¼ˆPQ/PV/Slackï¼‰å’Œè¾¹ï¼ˆçº¿è·¯/å˜å‹å™¨ï¼‰
- **ç‰©ç†ç‰¹å¾æå–** - 14ç»´èŠ‚ç‚¹ç‰¹å¾ï¼Œ9ç»´è¾¹ç‰¹å¾
- **æ™ºèƒ½ç¼“å­˜** - MD5å“ˆå¸Œç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤è®¡ç®—
- **æ ‡å‡†åŒ–å¤„ç†** - RobustScalerç»Ÿä¸€ç‰¹å¾å°ºåº¦

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from src.data_processing import PowerGridDataProcessor

# åˆ›å»ºå¤„ç†å™¨
processor = PowerGridDataProcessor(normalize=True, cache_dir='cache')

# å¤„ç†MATPOWERæ•°æ®
hetero_data = processor.graph_from_mpc(mpc_data)

# è·å–èŠ‚ç‚¹å’Œè¾¹ç‰¹å¾
node_features = hetero_data.x_dict
edge_features = hetero_data.edge_attr_dict
```

**æŠ€æœ¯ç»†èŠ‚**:
- æ”¯æŒIEEEæ ‡å‡†æµ‹è¯•ç³»ç»Ÿï¼ˆ14/30/57/118èŠ‚ç‚¹ï¼‰
- è‡ªåŠ¨å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸æ•°æ®
- ç”Ÿæˆæ— å‘å¼‚æ„å›¾ç»“æ„
- ä¿æŒå…¨å±€ç´¢å¼•åˆ°å±€éƒ¨ç´¢å¼•çš„æ˜ å°„å…³ç³»

### 2. GATç¼–ç å™¨æ¨¡å— (`src/gat.py`)

**åŠŸèƒ½**: åŸºäºç‰©ç†çº¦æŸçš„å¼‚æ„å›¾æ³¨æ„åŠ›ç½‘ç»œç¼–ç å™¨

**æ ¸å¿ƒç±»**: 
- `PhysicsGATv2Conv` - ç‰©ç†å¢å¼ºçš„GATå·ç§¯å±‚
- `HeteroGraphEncoder` - å¼‚æ„å›¾ç¼–ç å™¨

**ä¸»è¦ç‰¹æ€§**:
- **ç‰©ç†å¢å¼ºæ³¨æ„åŠ›** - ç”µæ°”é˜»æŠ—æŒ‡å¯¼çš„æ³¨æ„åŠ›æœºåˆ¶
- **å¼‚æ„å›¾æ”¯æŒ** - è‡ªåŠ¨å¤„ç†ä¸åŒç±»å‹çš„èŠ‚ç‚¹å’Œè¾¹
- **å¤šå±‚æ¶æ„** - 3-4å±‚GNNæ•è·é•¿ç¨‹ä¾èµ–
- **æ³¨æ„åŠ›æƒé‡æå–** - æ”¯æŒå¯è§£é‡Šæ€§åˆ†æ

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from src.gat import create_hetero_graph_encoder

# åˆ›å»ºç¼–ç å™¨
encoder = create_hetero_graph_encoder(
    data=hetero_data,
    hidden_channels=64,
    gnn_layers=3,
    heads=4,
    output_dim=128
)

# ç¼–ç èŠ‚ç‚¹
node_embeddings = encoder.encode_nodes(hetero_data)

# è·å–æ³¨æ„åŠ›æƒé‡
node_emb, attention_weights = encoder.encode_nodes_with_attention(hetero_data)
```

**æŠ€æœ¯ç»†èŠ‚**:
- åŸºäºGATv2æ¶æ„ï¼Œå…·æœ‰æ›´å¼ºè¡¨è¾¾èƒ½åŠ›
- èå…¥ç”µæ°”é˜»æŠ—å…ˆéªŒï¼šÎ±_ij = softmax(attention + Ï„/|Z_ij|)
- æ”¯æŒæ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
- å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°æ§åˆ¶ç‰©ç†å…ˆéªŒå½±å“

### 3. å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ (`src/rl/environment.py`)

**åŠŸèƒ½**: ç”µåŠ›ç½‘ç»œåˆ†åŒºçš„MDPç¯å¢ƒå®ç°

**æ ¸å¿ƒç±»**: `PowerGridPartitioningEnv`

**ä¸»è¦ç‰¹æ€§**:
- **å®Œæ•´MDPå»ºæ¨¡** - çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€è½¬ç§»
- **çº¦æŸå¤„ç†** - è¿é€šæ€§ã€æ‹“æ‰‘çº¦æŸè‡ªåŠ¨æ»¡è¶³
- **çŠ¶æ€ç®¡ç†** - é«˜æ•ˆçš„å¢é‡çŠ¶æ€æ›´æ–°
- **ç»ˆæ­¢æ¡ä»¶** - æ”¶æ•›æ£€æµ‹å’Œæœ€å¤§æ­¥æ•°é™åˆ¶

**çŠ¶æ€ç©ºé—´**:
- èŠ‚ç‚¹ç‰¹å¾åµŒå…¥ (H): ä»GATç¼–ç å™¨è·å¾—
- èŠ‚ç‚¹åˆ†é…æ ‡ç­¾ (z_t): åŠ¨æ€åˆ†åŒºåˆ†é…
- è¾¹ç•ŒèŠ‚ç‚¹ (Bdry_t): ä¸ä¸åŒåˆ†åŒºç›¸é‚»çš„èŠ‚ç‚¹
- åŒºåŸŸåµŒå…¥: æ¯ä¸ªåˆ†åŒºçš„èšåˆåµŒå…¥

**åŠ¨ä½œç©ºé—´**:
- ä¸¤é˜¶æ®µå†³ç­–: èŠ‚ç‚¹é€‰æ‹© â†’ åˆ†åŒºé€‰æ‹©
- åŠ¨ä½œæ©ç : åªæœ‰è¾¹ç•ŒèŠ‚ç‚¹å¯ç§»åŠ¨
- çº¦æŸæ»¡è¶³: è¿é€šæ€§å’Œé‚»æ¥çº¦æŸ

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from src.rl.environment import PowerGridPartitioningEnv

# åˆ›å»ºç¯å¢ƒ
env = PowerGridPartitioningEnv(
    hetero_data=hetero_data,
    node_embeddings=node_embeddings,
    num_partitions=3,
    max_steps=200
)

# é‡ç½®ç¯å¢ƒ
state, info = env.reset()

# æ‰§è¡ŒåŠ¨ä½œ
action = (node_idx, target_partition)
next_state, reward, done, truncated, info = env.step(action)
```

### 4. PPOæ™ºèƒ½ä½“ (`src/rl/agent.py`)

**åŠŸèƒ½**: åŸºäºPPOç®—æ³•çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“

**æ ¸å¿ƒç±»**: 
- `ActorNetwork` - ç­–ç•¥ç½‘ç»œ
- `CriticNetwork` - ä»·å€¼ç½‘ç»œ
- `PPOAgent` - PPOæ™ºèƒ½ä½“

**ä¸»è¦ç‰¹æ€§**:
- **Actor-Criticæ¶æ„** - åˆ†ç¦»çš„ç­–ç•¥å’Œä»·å€¼ç½‘ç»œ
- **åŠ¨ä½œæ©ç æ”¯æŒ** - çº¦æŸæ»¡è¶³çš„æœ‰æ•ˆåŠ¨ä½œç”Ÿæˆ
- **ç»éªŒå›æ”¾** - é«˜æ•ˆçš„æ‰¹é‡æ›´æ–°æœºåˆ¶
- **è‡ªé€‚åº”å­¦ä¹ ** - åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from src.rl.agent import PPOAgent

# åˆ›å»ºæ™ºèƒ½ä½“
agent = PPOAgent(
    node_embedding_dim=128,
    region_embedding_dim=256,
    num_partitions=3,
    lr_actor=3e-4,
    lr_critic=1e-3
)

# é€‰æ‹©åŠ¨ä½œ
action, log_prob, value = agent.select_action(state, training=True)

# å­˜å‚¨ç»éªŒ
agent.store_experience(state, action, reward, log_prob, value, done)

# æ›´æ–°ç½‘ç»œ
losses = agent.update()
```

### 5. çŠ¶æ€ç®¡ç† (`src/rl/state.py`)

**åŠŸèƒ½**: ç®¡ç†MDPçŠ¶æ€çš„è¡¨ç¤ºå’Œæ›´æ–°

**æ ¸å¿ƒç±»**: `StateManager`

**ä¸»è¦ç‰¹æ€§**:
- **å¼‚æ„å›¾çŠ¶æ€** - æ”¯æŒå¤šç§èŠ‚ç‚¹å’Œè¾¹ç±»å‹
- **å¢é‡æ›´æ–°** - é«˜æ•ˆçš„çŠ¶æ€è½¬ç§»
- **è¾¹ç•ŒèŠ‚ç‚¹è·Ÿè¸ª** - åŠ¨æ€è®¡ç®—å¯ç§»åŠ¨èŠ‚ç‚¹
- **åŒºåŸŸåµŒå…¥** - åˆ†åŒºçº§åˆ«çš„ç‰¹å¾èšåˆ

**æŠ€æœ¯ç»†èŠ‚**:
- ç»´æŠ¤å…¨å±€åˆ°å±€éƒ¨ç´¢å¼•æ˜ å°„
- ä½¿ç”¨é‚»æ¥åˆ—è¡¨åŠ é€Ÿè¾¹ç•ŒèŠ‚ç‚¹è®¡ç®—
- æ”¯æŒmeanå’Œmax poolingçš„åŒºåŸŸèšåˆ
- æä¾›å®Œæ•´çš„è§‚æµ‹ç©ºé—´æ¥å£

### 6. åŠ¨ä½œç©ºé—´ç®¡ç† (`src/rl/action_space.py`)

**åŠŸèƒ½**: ç®¡ç†ä¸¤é˜¶æ®µåŠ¨ä½œç©ºé—´å’Œçº¦æŸ

**æ ¸å¿ƒç±»**: 
- `ActionSpace` - åŠ¨ä½œç©ºé—´ç®¡ç†
- `ActionMask` - çº¦æŸæ©ç å¤„ç†

**ä¸»è¦ç‰¹æ€§**:
- **ä¸¤é˜¶æ®µåŠ¨ä½œ** - èŠ‚ç‚¹é€‰æ‹© + åˆ†åŒºé€‰æ‹©
- **çº¦æŸå¤„ç†** - æ‹“æ‰‘ã€è¿é€šæ€§ã€å¤§å°å¹³è¡¡
- **æ©ç ç”Ÿæˆ** - é«˜æ•ˆçš„æœ‰æ•ˆåŠ¨ä½œæšä¸¾
- **çº¦æŸéªŒè¯** - åŠ¨ä½œæœ‰æ•ˆæ€§æ£€æŸ¥

**çº¦æŸç±»å‹**:
- è¾¹ç•ŒèŠ‚ç‚¹çº¦æŸ: åªæœ‰è¾¹ç•ŒèŠ‚ç‚¹å¯ç§»åŠ¨
- é‚»æ¥çº¦æŸ: åªèƒ½ç§»åŠ¨åˆ°ç›¸é‚»åˆ†åŒº
- è¿é€šæ€§çº¦æŸ: ä¿æŒåˆ†åŒºå†…éƒ¨è¿é€š
- å¤§å°å¹³è¡¡çº¦æŸ: é¿å…åˆ†åŒºè¿‡å¤§æˆ–è¿‡å°

### 7. å¥–åŠ±å‡½æ•° (`src/rl/reward.py`)

**åŠŸèƒ½**: å¤šç›®æ ‡å¤åˆå¥–åŠ±å‡½æ•°

**æ ¸å¿ƒç±»**: `RewardFunction`

**å¥–åŠ±ç»„ä»¶**:
1. **è´Ÿè½½å¹³è¡¡** (R_balance): -Var(Lâ‚, ..., Lâ‚–)
2. **ç”µæ°”è§£è€¦** (R_decoupling): -Î£|Y_uv| (è·¨åˆ†åŒºè¾¹)
3. **åŠŸç‡å¹³è¡¡** (R_internal): -Î£(P_gen - P_load)Â²

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from src.rl.reward import RewardFunction

# åˆ›å»ºå¥–åŠ±å‡½æ•°
reward_fn = RewardFunction(
    hetero_data=hetero_data,
    reward_weights={
        'load_balance': 0.4,
        'electrical_decoupling': 0.4,
        'power_balance': 0.2
    }
)

# è®¡ç®—å¥–åŠ±
reward = reward_fn.compute_reward(current_partition, boundary_nodes, action)
```

### 8. åœºæ™¯ç”Ÿæˆå™¨ (`src/rl/scenario_generator.py`)

**åŠŸèƒ½**: ç”Ÿæˆå¤šæ ·åŒ–çš„è®­ç»ƒåœºæ™¯

**æ ¸å¿ƒç±»**: `ScenarioGenerator`

**åœºæ™¯ç±»å‹**:
- **N-1æ•…éšœ**: éšæœºæ”¯è·¯æ•…éšœ
- **è´Ÿè·æ³¢åŠ¨**: è´Ÿè·å’Œå‘ç”µå˜åŒ–
- **ç»„åˆåœºæ™¯**: å¤šç§æ‰°åŠ¨åŒæ—¶åº”ç”¨

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from src.rl.scenario_generator import ScenarioGenerator

# åˆ›å»ºç”Ÿæˆå™¨
generator = ScenarioGenerator(base_case_data)

# ç”Ÿæˆéšæœºåœºæ™¯
perturbed_case = generator.generate_random_scene()

# æ‰¹é‡ç”Ÿæˆ
scenarios = generator.generate_batch_scenarios(num_scenarios=100)

# ç‰¹å®šæ•…éšœ
case_n1 = generator.apply_specific_contingency(base_case_data, branch_idx=10)
```

### 9. GymåŒ…è£…å™¨ (`src/rl/gym_wrapper.py`)

**åŠŸèƒ½**: OpenAI Gymå…¼å®¹çš„ç¯å¢ƒåŒ…è£…å™¨

**æ ¸å¿ƒç±»**: `PowerGridPartitionGymEnv`

**ä¸»è¦ç‰¹æ€§**:
- **Gymæ¥å£å…¼å®¹** - æ ‡å‡†çš„reset/stepæ¥å£
- **å¹¶è¡Œè®­ç»ƒæ”¯æŒ** - å¤šè¿›ç¨‹ç¯å¢ƒ
- **Stable-Baselines3é›†æˆ** - æ”¯æŒSB3ç®—æ³•
- **åœºæ™¯ç”Ÿæˆé›†æˆ** - è‡ªåŠ¨åœºæ™¯å¤šæ ·åŒ–

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from src.rl.gym_wrapper import make_parallel_env

# åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
parallel_env = make_parallel_env(
    base_case_data=case_data,
    config=config,
    num_envs=12,
    use_scenario_generator=True
)

# ä½¿ç”¨Stable-Baselines3è®­ç»ƒ
from stable_baselines3 import PPO
model = PPO("MlpPolicy", parallel_env)
model.learn(total_timesteps=1000000)
```

### 10. å·¥å…·å‡½æ•° (`src/rl/utils.py`)

**åŠŸèƒ½**: åˆ†åŒºåˆå§‹åŒ–å’Œè¯„ä¼°å·¥å…·

**æ ¸å¿ƒç±»**:
- `MetisInitializer` - METISåˆ†åŒºåˆå§‹åŒ–
- `PartitionEvaluator` - åˆ†åŒºè´¨é‡è¯„ä¼°

**ä¸»è¦åŠŸèƒ½**:
- METISå›¾åˆ†åŒºï¼ˆå¸¦è°±èšç±»å¤‡é€‰ï¼‰
- åˆ†åŒºè¿é€šæ€§æ£€æŸ¥
- è´Ÿè½½å¹³è¡¡è¯„ä¼°
- ç”µæ°”è§£è€¦è®¡ç®—
- æ¨¡å—åº¦è®¡ç®—

### 11. å¯è§†åŒ–æ¨¡å— (`src/visualization.py`)

**åŠŸèƒ½**: è®­ç»ƒè¿‡ç¨‹å’Œç»“æœå¯è§†åŒ–

**æ ¸å¿ƒç±»**: `VisualizationManager`

**å¯è§†åŒ–ç±»å‹**:
- **åˆ†åŒºå¯è§†åŒ–** - ç½‘ç»œæ‹“æ‰‘å’Œåˆ†åŒºè¾¹ç•Œ
- **è®­ç»ƒæ›²çº¿** - å¥–åŠ±ã€æŸå¤±ã€æŒ‡æ ‡è¶‹åŠ¿
- **äº¤äº’å¼ä»ªè¡¨æ¿** - PlotlyåŠ¨æ€å¯è§†åŒ–
- **å¯¹æ¯”åˆ†æ** - å¤šæ–¹æ³•æ€§èƒ½å¯¹æ¯”

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from src.visualization import VisualizationManager

# åˆ›å»ºå¯è§†åŒ–ç®¡ç†å™¨
viz = VisualizationManager(config)

# å¯è§†åŒ–åˆ†åŒºç»“æœ
viz.visualize_partition(env, title="Final Partition", save_path="partition.png")

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
viz.plot_training_curves(history, save_path="training_curves.png")

# åˆ›å»ºäº¤äº’å¼å¯è§†åŒ–
fig = viz.create_interactive_visualization(env, comparison_df)
```

## ğŸ”„ æ¨¡å—é—´äº¤äº’

```
æ•°æ®å¤„ç† â†’ GATç¼–ç å™¨ â†’ RLç¯å¢ƒ â†’ PPOæ™ºèƒ½ä½“
    â†“           â†“          â†“         â†“
  å¼‚æ„å›¾    â†’ èŠ‚ç‚¹åµŒå…¥  â†’ MDPçŠ¶æ€  â†’ ç­–ç•¥ç½‘ç»œ
    â†“           â†“          â†“         â†“
  ç¼“å­˜æ•°æ®   â†’ æ³¨æ„åŠ›æƒé‡ â†’ å¥–åŠ±ä¿¡å· â†’ åŠ¨ä½œé€‰æ‹©
```

## ğŸ› ï¸ æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°çš„å¥–åŠ±ç»„ä»¶
```python
# åœ¨ reward.py ä¸­æ·»åŠ æ–°æ–¹æ³•
def _compute_custom_reward(self, current_partition):
    # å®ç°è‡ªå®šä¹‰å¥–åŠ±é€»è¾‘
    return custom_reward

# åœ¨ compute_reward ä¸­é›†æˆ
total_reward += self.weights['custom'] * custom_reward
```

### æ·»åŠ æ–°çš„çº¦æŸ
```python
# åœ¨ action_space.py ä¸­æ·»åŠ çº¦æŸæ–¹æ³•
def apply_custom_constraint(self, action_mask, current_partition):
    # å®ç°çº¦æŸé€»è¾‘
    return updated_mask
```

### è‡ªå®šä¹‰GATå±‚
```python
# ç»§æ‰¿ PhysicsGATv2Conv
class CustomGATConv(PhysicsGATv2Conv):
    def physics_enhanced_edge_attr(self, edge_attr):
        # å®ç°è‡ªå®šä¹‰ç‰©ç†å¢å¼º
        return enhanced_edge_attr
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **GPUåŠ é€Ÿ**: åœ¨é…ç½®ä¸­è®¾ç½® `device: cuda`
2. **æ‰¹é‡å¤„ç†**: å¢åŠ  `batch_size` å’Œ `update_interval`
3. **å†…å­˜ä¼˜åŒ–**: ä½¿ç”¨è¾ƒå°çš„åµŒå…¥ç»´åº¦
4. **å¹¶è¡Œè®­ç»ƒ**: å¯ç”¨ `parallel_training.enabled: true`
5. **ç¼“å­˜ä¼˜åŒ–**: ç¡®ä¿ç¼“å­˜ç›®å½•å¯å†™ä¸”æœ‰è¶³å¤Ÿç©ºé—´

## ğŸ” è°ƒè¯•æŒ‡å—

### å¯ç”¨è¯¦ç»†æ—¥å¿—
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

### æ£€æŸ¥æ•°æ®æµ
```python
# æ£€æŸ¥å¼‚æ„å›¾æ•°æ®
print(f"èŠ‚ç‚¹ç±»å‹: {hetero_data.node_types}")
print(f"è¾¹ç±»å‹: {hetero_data.edge_types}")

# æ£€æŸ¥åµŒå…¥ç»´åº¦
for node_type, embeddings in node_embeddings.items():
    print(f"{node_type}: {embeddings.shape}")
```

### éªŒè¯ç¯å¢ƒ
```python
# æµ‹è¯•ç¯å¢ƒé‡ç½®
state, info = env.reset()
print(f"åˆå§‹çŠ¶æ€: {state.keys()}")

# æµ‹è¯•åŠ¨ä½œç©ºé—´
valid_actions = env.get_valid_actions()
print(f"æœ‰æ•ˆåŠ¨ä½œæ•°: {len(valid_actions)}")
```

## ğŸ“‹ å¸¸è§é—®é¢˜

**Q: è®­ç»ƒä¸æ”¶æ•›æ€ä¹ˆåŠï¼Ÿ**
A: é™ä½å­¦ä¹ ç‡ï¼Œå¢åŠ è®­ç»ƒå›åˆæ•°ï¼Œæ£€æŸ¥å¥–åŠ±å‡½æ•°æƒé‡

**Q: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ**
A: å‡å°‘æ‰¹å¤§å°ï¼Œé™ä½åµŒå…¥ç»´åº¦ï¼Œä½¿ç”¨CPUè®­ç»ƒ

**Q: å¦‚ä½•æ·»åŠ æ–°çš„ç”µç½‘æ¡ˆä¾‹ï¼Ÿ**
A: å‡†å¤‡MATPOWERæ ¼å¼æ–‡ä»¶ï¼Œæ”¾å…¥dataç›®å½•ï¼Œåœ¨é…ç½®ä¸­æŒ‡å®šè·¯å¾„

**Q: å¦‚ä½•è‡ªå®šä¹‰è®­ç»ƒæ¨¡å¼ï¼Ÿ**
A: åœ¨config_unified.yamlä¸­æ·»åŠ æ–°çš„é¢„è®¾é…ç½®ï¼Œåœ¨train_unified.pyä¸­æ³¨å†Œæ–°æ¨¡å¼ 