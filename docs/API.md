# APIå‚è€ƒæ–‡æ¡£

## ğŸ¯ æ ¸å¿ƒæ¥å£æ¦‚è§ˆ

æœ¬æ–‡æ¡£æä¾›ç³»ç»Ÿå„æ¨¡å—çš„å…³é”®APIæ¥å£è¯´æ˜ï¼ŒåŒ…å«ä½¿ç”¨ç¤ºä¾‹å’Œå‚æ•°è¯¦è§£ã€‚

## ğŸ”§ æ•°æ®ç‰¹å¾æå–API

### HeteroGATEncoder

**ä¸»è¦æ¥å£:**

```python
# åˆå§‹åŒ–ç¼–ç å™¨
encoder = HeteroGATEncoder(
    node_types=['bus_pv', 'bus_slack', 'gen'],
    edge_types=[('bus_pv', 'line', 'bus_slack'), ('gen', 'connects', 'bus_pv')],
    hidden_channels=128,
    output_dim=64,
    num_layers=3,
    heads=8
)

# ç¼–ç èŠ‚ç‚¹ç‰¹å¾
node_embeddings = encoder.encode_nodes(hetero_data)
# è¿”å›: {'bus_pv': tensor, 'bus_slack': tensor, 'gen': tensor}

# è·å–æ³¨æ„åŠ›æƒé‡
attention_weights = encoder.get_attention_weights()
# è¿”å›: {'edge_type_1': tensor, 'edge_type_2': tensor, ...}

# å®Œæ•´å‰å‘ä¼ æ’­
node_emb, attn_weights, graph_emb = encoder.forward(
    hetero_data, 
    return_attention_weights=True,
    return_graph_embedding=True
)
```

**å‚æ•°è¯´æ˜:**
- `node_types`: èŠ‚ç‚¹ç±»å‹åˆ—è¡¨
- `edge_types`: è¾¹ç±»å‹ä¸‰å…ƒç»„åˆ—è¡¨ (æºèŠ‚ç‚¹, å…³ç³», ç›®æ ‡èŠ‚ç‚¹)
- `hidden_channels`: éšè—å±‚ç»´åº¦
- `output_dim`: è¾“å‡ºåµŒå…¥ç»´åº¦
- `heads`: æ³¨æ„åŠ›å¤´æ•°

## ğŸ—ï¸ å¼ºåŒ–å­¦ä¹ ç¯å¢ƒAPI

### PowerGridPartitioningEnv

**æ ‡å‡†RLæ¥å£:**

```python
# ç¯å¢ƒåˆå§‹åŒ–
env = PowerGridPartitioningEnv(
    hetero_data=hetero_data,
    node_embeddings=node_embeddings,
    num_partitions=4,
    max_steps=50,
    reward_weights={'balance': 0.4, 'decoupling': 0.4, 'power': 0.2}
)

# é‡ç½®ç¯å¢ƒ
observation = env.reset()
# è¿”å›: Dict[str, torch.Tensor] - çŠ¶æ€è§‚å¯Ÿ

# æ‰§è¡ŒåŠ¨ä½œ
obs, reward, terminated, truncated, info = env.step(action)
# action: Tuple[int, int] - (èŠ‚ç‚¹ç´¢å¼•, ç›®æ ‡åˆ†åŒº)
# è¿”å›: (è§‚å¯Ÿ, å¥–åŠ±, æ˜¯å¦ç»ˆæ­¢, æ˜¯å¦æˆªæ–­, é™„åŠ ä¿¡æ¯)

# è·å–æœ‰æ•ˆåŠ¨ä½œ
valid_actions = env.get_valid_actions()
# è¿”å›: List[Tuple[int, int]] - æœ‰æ•ˆåŠ¨ä½œåˆ—è¡¨

# æ¸²æŸ“å½“å‰çŠ¶æ€
env.render(mode='human')  # å¯è§†åŒ–æ˜¾ç¤º
env.render(mode='rgb_array')  # è¿”å›å›¾åƒæ•°ç»„
```

**é…ç½®å‚æ•°:**
```python
config = {
    'max_steps': 50,
    'reward_weights': {
        'balance': 0.4,
        'decoupling': 0.4, 
        'power': 0.2
    },
    'dynamic_constraints': {
        'constraint_mode': 'soft',  # 'hard' or 'soft'
        'connectivity_penalty': 1.0,
        'enable_adaptive': True
    }
}
```

## ğŸ¯ å¥–åŠ±ç³»ç»ŸAPI

### RewardFunction

**æ ¸å¿ƒæ–¹æ³•:**

```python
# åˆå§‹åŒ–å¥–åŠ±å‡½æ•°
reward_func = RewardFunction(
    hetero_data=hetero_data,
    config=reward_config,
    device=device
)

# è®¡ç®—å³æ—¶å¥–åŠ±
reward, plateau_result = reward_func.compute_incremental_reward(
    current_partition=partition_tensor,
    action=(node_idx, target_partition),
    scenario_context=scenario_context  # å¯é€‰
)

# è®¡ç®—ç»ˆå±€å¥–åŠ±
final_reward, components = reward_func.compute_final_reward(
    final_partition=final_partition,
    termination_type='natural'  # 'natural', 'timeout', 'stuck'
)

# è´¨é‡åˆ†æ•°è®¡ç®—
quality_score = reward_func.compute_quality_score(partition_tensor)

# å¹³å°æœŸæ£€æµ‹
is_plateau = reward_func.should_early_stop(partition_tensor)
```

**å¥–åŠ±ç»„ä»¶è¯¦æƒ…:**
```python
# componentså­—å…¸åŒ…å«:
{
    'balance_reward': float,      # å¹³è¡¡æ€§å¥–åŠ±
    'decoupling_reward': float,   # è§£è€¦å¥–åŠ±
    'power_reward': float,        # åŠŸç‡å¹³è¡¡å¥–åŠ±
    'quality_reward': float,      # ç»¼åˆè´¨é‡å¥–åŠ±
    'threshold_bonus': float,     # é˜ˆå€¼å¥–åŠ±
    'termination_discount': float, # ç»ˆæ­¢æŠ˜æ‰£
    'final_reward': float,        # æœ€ç»ˆå¥–åŠ±
    'metrics': dict              # è¯¦ç»†æŒ‡æ ‡
}
```

## ğŸ§  æ™ºèƒ½ä½“API

### PPOAgent

**è®­ç»ƒæ¥å£:**

```python
# åˆå§‹åŒ–æ™ºèƒ½ä½“
agent = PPOAgent(
    observation_space=env.observation_space,
    action_space=env.action_space,
    config=ppo_config
)

# é€‰æ‹©åŠ¨ä½œ
action, log_prob, value = agent.select_action(observation)

# æ‰¹é‡è®­ç»ƒ
train_stats = agent.train_batch(
    observations=obs_batch,
    actions=action_batch,
    rewards=reward_batch,
    dones=done_batch,
    log_probs=log_prob_batch,
    values=value_batch
)

# ä¿å­˜/åŠ è½½æ¨¡å‹
agent.save_model('path/to/model.pth')
agent.load_model('path/to/model.pth')
```

**é…ç½®å‚æ•°:**
```python
ppo_config = {
    'learning_rate': 3e-4,
    'gamma': 0.99,
    'gae_lambda': 0.95,
    'clip_epsilon': 0.2,
    'entropy_coef': 0.01,
    'value_loss_coef': 0.5,
    'max_grad_norm': 0.5,
    'update_epochs': 4,
    'batch_size': 64
}
```

## ğŸ“š è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ API

### AdaptiveCurriculumDirector

**æ ¸å¿ƒæ¥å£:**

```python
# åˆå§‹åŒ–å¯¼æ¼”ç³»ç»Ÿ
director = AdaptiveCurriculumDirector(
    total_episodes=5000,
    config=curriculum_config
)

# æ›´æ–°è®­ç»ƒå‚æ•°
updated_params = director.update_training_parameters(
    episode_num=current_episode,
    performance_metrics={
        'average_reward': avg_reward,
        'episode_length': avg_length,
        'quality_score': avg_quality,
        'connectivity_rate': conn_rate
    }
)

# è·å–å½“å‰é˜¶æ®µä¿¡æ¯
stage_info = director.get_current_stage_info()
# è¿”å›: {'stage': str, 'progress': float, 'parameters': dict}

# æ£€æŸ¥æ˜¯å¦éœ€è¦é˜¶æ®µè½¬æ¢
should_advance = director.should_advance_stage(recent_performance)
```

**è¯¾ç¨‹é…ç½®:**
```python
curriculum_config = {
    'stages': {
        'exploration': {'episodes_ratio': 0.25, 'connectivity_penalty': 0.05},
        'transition': {'episodes_ratio': 0.25, 'connectivity_penalty': 0.5},
        'refinement': {'episodes_ratio': 0.3, 'connectivity_penalty': 1.0},
        'fine_tuning': {'episodes_ratio': 0.2, 'connectivity_penalty': 1.5}
    },
    'transition_criteria': {
        'min_stability': 0.8,
        'min_improvement': 0.01,
        'confidence_threshold': 0.8
    }
}
```

## ğŸ” æ•°æ®å¤„ç†API

### DataProcessor

**æ•°æ®åŠ è½½ä¸é¢„å¤„ç†:**

```python
# åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
processor = DataProcessor(config=data_config)

# åŠ è½½MATPOWERæ•°æ®
hetero_data = processor.load_matpower_case('ieee30.mat')

# æ•°æ®é¢„å¤„ç†
processed_data = processor.preprocess_network_data(
    hetero_data,
    normalize_features=True,
    add_virtual_nodes=False
)

# åœºæ™¯ç”Ÿæˆ
scenario_data = processor.generate_scenario(
    base_data=hetero_data,
    scenario_type='n_minus_1',  # 'normal', 'n_minus_1', 'load_variation'
    severity=0.5
)
```

## ğŸ¨ å¯è§†åŒ–API

### è®­ç»ƒç›‘æ§

```python
# TensorBoardæ—¥å¿—è®°å½•
from code.src.utils.logger import TensorBoardLogger

logger = TensorBoardLogger(log_dir='data/logs')

# è®°å½•æ ‡é‡æŒ‡æ ‡
logger.log_scalar('train/reward', reward, step)
logger.log_scalar('train/episode_length', length, step)

# è®°å½•åˆ†åŒºå¯è§†åŒ–
logger.log_partition_visualization(
    partition=final_partition,
    hetero_data=hetero_data,
    step=episode
)

# è®°å½•æ³¨æ„åŠ›æƒé‡
logger.log_attention_weights(
    attention_weights=attn_weights,
    step=episode
)
```

### ç»“æœåˆ†æ

```python
# æ€§èƒ½åˆ†æ
from code.src.utils.analyzer import PerformanceAnalyzer

analyzer = PerformanceAnalyzer()

# åˆ†æè®­ç»ƒç»“æœ
analysis_report = analyzer.analyze_training_results(
    log_dir='data/logs',
    metrics=['reward', 'quality', 'episode_length']
)

# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
comparison = analyzer.compare_experiments(
    experiment_dirs=['exp1', 'exp2', 'exp3'],
    metrics=['final_quality', 'convergence_speed']
)
```

## ğŸ› ï¸ å®ç”¨å·¥å…·API

### é…ç½®ç®¡ç†

```python
# é…ç½®åŠ è½½
from code.src.utils.config import load_config, save_config

config = load_config('config.yaml')
save_config(config, 'modified_config.yaml')

# é…ç½®éªŒè¯
from code.src.utils.config import validate_config

is_valid, errors = validate_config(config)
```

### æ¨¡å‹è¯„ä¼°

```python
# æ¨¡å‹è¯„ä¼°å™¨
from code.src.utils.evaluator import ModelEvaluator

evaluator = ModelEvaluator()

# è·¨ç½‘ç»œè¯„ä¼°
results = evaluator.evaluate_cross_network(
    model_path='models/best_model.pth',
    test_cases=['ieee14', 'ieee30', 'ieee57'],
    num_episodes=10
)

# ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
report = evaluator.generate_evaluation_report(results)
```

## ğŸš€ å¿«é€Ÿå¼€å§‹ç¤ºä¾‹

### å®Œæ•´è®­ç»ƒæµç¨‹

```python
# 1. æ•°æ®å‡†å¤‡
processor = DataProcessor()
hetero_data = processor.load_matpower_case('ieee30.mat')

# 2. ç‰¹å¾æå–
encoder = HeteroGATEncoder(...)
node_embeddings = encoder.encode_nodes(hetero_data)

# 3. ç¯å¢ƒåˆå§‹åŒ–
env = PowerGridPartitioningEnv(hetero_data, node_embeddings, num_partitions=4)

# 4. æ™ºèƒ½ä½“åˆå§‹åŒ–
agent = PPOAgent(env.observation_space, env.action_space)

# 5. è®­ç»ƒå¾ªç¯
for episode in range(num_episodes):
    obs = env.reset()
    done = False
    
    while not done:
        action, log_prob, value = agent.select_action(obs)
        next_obs, reward, terminated, truncated, info = env.step(action)
        
        # å­˜å‚¨ç»éªŒå¹¶è®­ç»ƒ
        agent.store_transition(obs, action, reward, next_obs, done, log_prob, value)
        obs = next_obs
        done = terminated or truncated
    
    # æ‰¹é‡æ›´æ–°
    if episode % update_interval == 0:
        agent.train_batch()
```

è¿™äº›APIæ¥å£æä¾›äº†å®Œæ•´çš„ç³»ç»ŸåŠŸèƒ½è®¿é—®ï¼Œæ”¯æŒçµæ´»çš„å®šåˆ¶å’Œæ‰©å±•ã€‚
